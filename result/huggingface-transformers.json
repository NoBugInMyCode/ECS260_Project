{
    "https://api.github.com/repos/huggingface/transformers": {
        "forks": 27635,
        "watchers": 137851,
        "stars": 137851,
        "languages": {
            "Python": 63052263,
            "Cuda": 327808,
            "Dockerfile": 36853,
            "Shell": 30374,
            "C++": 25815,
            "C": 7703,
            "Makefile": 4201,
            "Cython": 3635,
            "Jsonnet": 937
        },
        "commits": [
            "2025-01-21T20:16:18Z",
            "2025-01-21T17:56:43Z",
            "2025-01-21T17:23:36Z",
            "2025-01-21T16:53:30Z",
            "2025-01-21T16:49:05Z",
            "2025-01-21T16:32:09Z",
            "2025-01-21T16:29:58Z",
            "2025-01-21T16:01:28Z",
            "2025-01-21T15:08:31Z",
            "2025-01-21T14:42:44Z",
            "2025-01-21T14:35:54Z",
            "2025-01-21T13:34:45Z",
            "2025-01-21T13:17:49Z",
            "2025-01-21T13:09:29Z",
            "2025-01-21T12:32:39Z",
            "2025-01-21T12:11:33Z",
            "2025-01-21T11:47:04Z",
            "2025-01-21T11:11:23Z",
            "2025-01-21T09:23:40Z",
            "2025-01-21T08:38:48Z",
            "2025-01-21T07:06:44Z",
            "2025-01-20T21:17:01Z",
            "2025-01-20T18:15:34Z",
            "2025-01-20T16:20:51Z",
            "2025-01-20T16:19:31Z",
            "2025-01-20T16:18:07Z",
            "2025-01-20T16:10:35Z",
            "2025-01-20T15:19:29Z",
            "2025-01-20T15:00:46Z",
            "2025-01-20T14:21:45Z"
        ],
        "creation_date": "2018-10-29T13:56:00Z",
        "contributors": 30,
        "topics": [
            "bert",
            "deep-learning",
            "flax",
            "hacktoberfest",
            "jax",
            "language-model",
            "language-models",
            "machine-learning",
            "model-hub",
            "natural-language-processing",
            "nlp",
            "nlp-library",
            "pretrained-models",
            "python",
            "pytorch",
            "pytorch-transformers",
            "seq2seq",
            "speech-recognition",
            "tensorflow",
            "transformer"
        ],
        "subscribers": 1129,
        "readme": "<!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://circleci.com/gh/huggingface/transformers\"><img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\"></a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\"><img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\"></a>\n    <a href=\"https://huggingface.co/docs/transformers/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\"></a>\n    <a href=\"https://github.com/huggingface/transformers/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\"></a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\"><img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\"></a>\n    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a>\n</p>\n\n<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md\">\u7b80\u4f53\u4e2d\u6587</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md\">\u7e41\u9ad4\u4e2d\u6587</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md\">\ud55c\uad6d\uc5b4</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_es.md\">Espa\u00f1ol</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md\">\u65e5\u672c\u8a9e</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md\">\u0939\u093f\u0928\u094d\u0926\u0940</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md\">\u0420\u0443\u0441\u0441\u043a\u0438\u0439</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md\">\u0420ortugu\u00eas</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_te.md\">\u0c24\u0c46\u0c32\u0c41\u0c17\u0c41</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md\">Fran\u00e7ais</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_de.md\">Deutsch</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md\">Ti\u1ebfng Vi\u1ec7t</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md\">\u0627\u0644\u0639\u0631\u0628\u064a\u0629</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md\">\u0627\u0631\u062f\u0648</a> |\n    </p>\n</h4>\n\n<h3 align=\"center\">\n    <p>State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow</p>\n</h3>\n\n<h3 align=\"center\">\n    <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a>\n</h3>\n\n\ud83e\udd17 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.\n\nThese models can be applied on:\n\n* \ud83d\udcdd Text, for tasks like text classification, information extraction, question answering, summarization, translation, and text generation, in over 100 languages.\n* \ud83d\uddbc\ufe0f Images, for tasks like image classification, object detection, and segmentation.\n* \ud83d\udde3\ufe0f Audio, for tasks like speech recognition and audio classification.\n\nTransformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n\n\ud83e\udd17 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.\n\n\ud83e\udd17 Transformers is backed by the three most popular deep learning libraries \u2014 [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) \u2014 with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.\n\n## Online demos\n\nYou can test most of our models directly on their pages from the [model hub](https://huggingface.co/models). We also offer [private model hosting, versioning, & an inference API](https://huggingface.co/pricing) for public and private models.\n\nHere are a few examples:\n\nIn Natural Language Processing:\n- [Masked word completion with BERT](https://huggingface.co/google-bert/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)\n- [Named Entity Recognition with Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)\n- [Text generation with Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n- [Natural Language Inference with RoBERTa](https://huggingface.co/FacebookAI/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)\n- [Summarization with BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)\n- [Question answering with DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)\n- [Translation with T5](https://huggingface.co/google-t5/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)\n\nIn Computer Vision:\n- [Image classification with ViT](https://huggingface.co/google/vit-base-patch16-224)\n- [Object Detection with DETR](https://huggingface.co/facebook/detr-resnet-50)\n- [Semantic Segmentation with SegFormer](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)\n- [Panoptic Segmentation with Mask2Former](https://huggingface.co/facebook/mask2former-swin-large-coco-panoptic)\n- [Depth Estimation with Depth Anything](https://huggingface.co/docs/transformers/main/model_doc/depth_anything)\n- [Video Classification with VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)\n- [Universal Segmentation with OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_dinat_large)\n\nIn Audio:\n- [Automatic Speech Recognition with Whisper](https://huggingface.co/openai/whisper-large-v3)\n- [Keyword Spotting with Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\n- [Audio Classification with Audio Spectrogram Transformer](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)\n\nIn Multimodal tasks:\n- [Table Question Answering with TAPAS](https://huggingface.co/google/tapas-base-finetuned-wtq)\n- [Visual Question Answering with ViLT](https://huggingface.co/dandelin/vilt-b32-finetuned-vqa)\n- [Image captioning with LLaVa](https://huggingface.co/llava-hf/llava-1.5-7b-hf)\n- [Zero-shot Image Classification with SigLIP](https://huggingface.co/google/siglip-so400m-patch14-384)\n- [Document Question Answering with LayoutLM](https://huggingface.co/impira/layoutlm-document-qa)\n- [Zero-shot Video Classification with X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)\n- [Zero-shot Object Detection with OWLv2](https://huggingface.co/docs/transformers/en/model_doc/owlv2)\n- [Zero-shot Image Segmentation with CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg)\n- [Automatic Mask Generation with SAM](https://huggingface.co/docs/transformers/model_doc/sam)\n\n\n## 100 projects using Transformers\n\nTransformers is more than a toolkit to use pretrained models: it's a community of projects built around it and the\nHugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyone\nelse to build their dream projects.\n\nIn order to celebrate the 100,000 stars of transformers, we have decided to put the spotlight on the\ncommunity, and we have created the [awesome-transformers](./awesome-transformers.md) page which lists 100\nincredible projects built in the vicinity of transformers.\n\nIf you own or use a project that you believe should be part of the list, please open a PR to add it!\n\n## Serious about AI in your organisation? Build faster with the Hugging Face Enterprise Hub.\n\n<a target=\"_blank\" href=\"https://huggingface.co/enterprise\">\n    <img alt=\"Hugging Face Enterprise Hub\" src=\"https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925\">\n</a><br>\n\n## Quick tour\n\nTo immediately use a model on a given input (text, image, audio, ...), we provide the `pipeline` API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts:\n\n```python\n>>> from transformers import pipeline\n\n# Allocate a pipeline for sentiment-analysis\n>>> classifier = pipeline('sentiment-analysis')\n>>> classifier('We are very happy to introduce pipeline to the transformers repository.')\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n```\n\nThe second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text. Here, the answer is \"positive\" with a confidence of 99.97%.\n\nMany tasks have a pre-trained `pipeline` ready to go, in NLP but also in computer vision and speech. For example, we can easily extract detected objects in an image:\n\n``` python\n>>> import requests\n>>> from PIL import Image\n>>> from transformers import pipeline\n\n# Download an image with cute cats\n>>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"\n>>> image_data = requests.get(url, stream=True).raw\n>>> image = Image.open(image_data)\n\n# Allocate a pipeline for object detection\n>>> object_detector = pipeline('object-detection')\n>>> object_detector(image)\n[{'score': 0.9982201457023621,\n  'label': 'remote',\n  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},\n {'score': 0.9960021376609802,\n  'label': 'remote',\n  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},\n {'score': 0.9954745173454285,\n  'label': 'couch',\n  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},\n {'score': 0.9988006353378296,\n  'label': 'cat',\n  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},\n {'score': 0.9986783862113953,\n  'label': 'cat',\n  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]\n```\n\nHere, we get a list of objects detected in the image, with a box surrounding the object and a confidence score. Here is the original image on the left, with the predictions displayed on the right:\n\n<h3 align=\"center\">\n    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\" width=\"400\"></a>\n    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample_post_processed.png\" width=\"400\"></a>\n</h3>\n\nYou can learn more about the tasks supported by the `pipeline` API in [this tutorial](https://huggingface.co/docs/transformers/task_summary).\n\nIn addition to `pipeline`, to download and use any of the pretrained models on your given task, all it takes is three lines of code. Here is the PyTorch version:\n```python\n>>> from transformers import AutoTokenizer, AutoModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n>>> model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n\n>>> inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n```\n\nAnd here is the equivalent code for TensorFlow:\n```python\n>>> from transformers import AutoTokenizer, TFAutoModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n>>> model = TFAutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n\n>>> inputs = tokenizer(\"Hello world!\", return_tensors=\"tf\")\n>>> outputs = model(**inputs)\n```\n\nThe tokenizer is responsible for all the preprocessing the pretrained model expects and can be called directly on a single string (as in the above examples) or a list. It will output a dictionary that you can use in downstream code or simply directly pass to your model using the ** argument unpacking operator.\n\nThe model itself is a regular [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) or a [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) (depending on your backend) which you can use as usual. [This tutorial](https://huggingface.co/docs/transformers/training) explains how to integrate such a model into a classic PyTorch or TensorFlow training loop, or how to use our `Trainer` API to quickly fine-tune on a new dataset.\n\n## Why should I use transformers?\n\n1. Easy-to-use state-of-the-art models:\n    - High performance on natural language understanding & generation, computer vision, and audio tasks.\n    - Low barrier to entry for educators and practitioners.\n    - Few user-facing abstractions with just three classes to learn.\n    - A unified API for using all our pretrained models.\n\n1. Lower compute costs, smaller carbon footprint:\n    - Researchers can share trained models instead of always retraining.\n    - Practitioners can reduce compute time and production costs.\n    - Dozens of architectures with over 400,000 pretrained models across all modalities.\n\n1. Choose the right framework for every part of a model's lifetime:\n    - Train state-of-the-art models in 3 lines of code.\n    - Move a single model between TF2.0/PyTorch/JAX frameworks at will.\n    - Seamlessly pick the right framework for training, evaluation, and production.\n\n1. Easily customize a model or an example to your needs:\n    - We provide examples for each architecture to reproduce the results published by its original authors.\n    - Model internals are exposed as consistently as possible.\n    - Model files can be used independently of the library for quick experiments.\n\n## Why shouldn't I use transformers?\n\n- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.\n- The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops, you should use another library (possibly, [Accelerate](https://huggingface.co/docs/accelerate)).\n- While we strive to present as many use cases as possible, the scripts in our [examples folder](https://github.com/huggingface/transformers/tree/main/examples) are just that: examples. It is expected that they won't work out-of-the-box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs.\n\n## Installation\n\n### With pip\n\nThis repository is tested on Python 3.9+, Flax 0.4.1+, PyTorch 2.0+, and TensorFlow 2.6+.\n\nYou should install \ud83e\udd17 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\n\nFirst, create a virtual environment with the version of Python you're going to use and activate it.\n\n**macOS/Linux**\n\n```python -m venv env\nsource env/bin/activate\n```\n\n**Windows**\n\n``` python -m venv env\nenv\\Scripts\\activate\n```\n\nTo use \ud83e\udd17 Transformers, you must install at least one of Flax, PyTorch, or TensorFlow. Refer to the official installation guides for platform-specific commands:\n\n[TensorFlow installation page](https://www.tensorflow.org/install/), \n[PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) and/or [Flax](https://github.com/google/flax#quick-install) and [Jax](https://github.com/google/jax#installation) \n\nWhen one of those backends has been installed, \ud83e\udd17 Transformers can be installed using pip as follows:\n\n```\npip install transformers\n```\n\nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must [install the library from source](https://huggingface.co/docs/transformers/installation#installing-from-source).\n\n```\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install\n```\n\n### With conda\n\n\ud83e\udd17 Transformers can be installed using conda as follows:\n\n```shell script\nconda install conda-forge::transformers\n```\n\n> **_NOTE:_** Installing `transformers` from the `huggingface` channel is deprecated.\n\nFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda.\n\n> **_NOTE:_**  On Windows, you may be prompted to activate Developer Mode in order to benefit from caching. If this is not an option for you, please let us know in [this issue](https://github.com/huggingface/huggingface_hub/issues/1062).\n\n## Model architectures\n\n**[All the model checkpoints](https://huggingface.co/models)** provided by \ud83e\udd17 Transformers are seamlessly integrated from the huggingface.co [model hub](https://huggingface.co/models), where they are uploaded directly by [users](https://huggingface.co/users) and [organizations](https://huggingface.co/organizations).\n\nCurrent number of checkpoints: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)\n\n\ud83e\udd17 Transformers currently provides the following architectures: see [here](https://huggingface.co/docs/transformers/model_summary) for a high-level summary of each them.\n\nTo check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated tokenizer backed by the \ud83e\udd17 Tokenizers library, refer to [this table](https://huggingface.co/docs/transformers/index#supported-frameworks).\n\nThese implementations have been tested on several datasets (see the example scripts) and should match the performance of the original implementations. You can find more details on performance in the Examples section of the [documentation](https://github.com/huggingface/transformers/tree/main/examples).\n\n\n## Learn more\n\n| Section | Description |\n|-|-|\n| [Documentation](https://huggingface.co/docs/transformers/) | Full API documentation and tutorials |\n| [Task summary](https://huggingface.co/docs/transformers/task_summary) | Tasks supported by \ud83e\udd17 Transformers |\n| [Preprocessing tutorial](https://huggingface.co/docs/transformers/preprocessing) | Using the `Tokenizer` class to prepare data for the models |\n| [Training and fine-tuning](https://huggingface.co/docs/transformers/training) | Using the models provided by \ud83e\udd17 Transformers in a PyTorch/TensorFlow training loop and the `Trainer` API |\n| [Quick tour: Fine-tuning/usage scripts](https://github.com/huggingface/transformers/tree/main/examples) | Example scripts for fine-tuning models on a wide range of tasks |\n| [Model sharing and uploading](https://huggingface.co/docs/transformers/model_sharing) | Upload and share your fine-tuned models with the community |\n\n## Citation\n\nWe now have a [paper](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) you can cite for the \ud83e\udd17 Transformers library:\n```bibtex\n@inproceedings{wolf-etal-2020-transformers,\n    title = \"Transformers: State-of-the-Art Natural Language Processing\",\n    author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R\u00e9mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = oct,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",\n    pages = \"38--45\"\n}\n```\n",
        "releases": [
            {
                "name": "Patch release v4.48.1",
                "date": "2025-01-20T16:36:05Z"
            },
            {
                "name": "v4.48.0: ModernBERT, Aria, TimmWrapper, ColPali, Falcon3, Bamba, VitPose, DinoV2 w/ Registers, Emu3, Cohere v2, TextNet, DiffLlama, PixtralLarge, Moonshine",
                "date": "2025-01-10T12:14:21Z"
            },
            {
                "name": "v4.47.1",
                "date": "2024-12-17T15:42:54Z"
            },
            {
                "name": " v4.47.0: PaliGemma-2, I-JEPA, OLMo-2, LayerSkip, Tensor Parallel",
                "date": "2024-12-05T17:45:28Z"
            },
            {
                "name": "Patch release v4.46.3",
                "date": "2024-11-18T22:13:14Z"
            },
            {
                "name": "Patch release v4.46.2",
                "date": "2024-11-05T18:21:54Z"
            },
            {
                "name": "Patch release v4.46.1",
                "date": "2024-10-29T15:50:03Z"
            },
            {
                "name": "Release v4.46.0 ",
                "date": "2024-10-24T08:15:48Z"
            },
            {
                "name": "Release v4.45.2",
                "date": "2024-10-07T17:42:52Z"
            },
            {
                "name": "Patch Release v4.45.1",
                "date": "2024-09-26T18:07:17Z"
            },
            {
                "name": "Llama 3.2, mllama, Qwen2-Audio, Qwen2-VL, OLMoE, Llava Onevision, Pixtral, FalconMamba, Modular Transformers",
                "date": "2024-09-25T18:11:59Z"
            },
            {
                "name": "Release v4.44.2",
                "date": "2024-08-22T16:56:32Z"
            },
            {
                "name": "Patch release v4.44.1",
                "date": "2024-08-20T17:51:51Z"
            },
            {
                "name": "Release v4.44.0",
                "date": "2024-08-06T18:39:34Z"
            },
            {
                "name": "v4.43.4 Patch Release",
                "date": "2024-08-05T10:57:24Z"
            },
            {
                "name": "v4.43.3 Patch deepspeed",
                "date": "2024-07-26T15:30:58Z"
            },
            {
                "name": "v4.43.2: Patch release",
                "date": "2024-07-24T15:50:18Z"
            },
            {
                "name": "v4.43.1: Patch release",
                "date": "2024-07-23T15:55:56Z"
            },
            {
                "name": "v4.43.0: Llama 3.1, Chameleon, ZoeDepth, Hiera",
                "date": "2024-07-23T15:09:04Z"
            },
            {
                "name": "Patch release v4.42.4",
                "date": "2024-07-11T17:09:38Z"
            },
            {
                "name": "Patch release v4.42.3",
                "date": "2024-06-28T15:35:36Z"
            },
            {
                "name": "Patch release v4.42.2",
                "date": "2024-06-28T06:42:26Z"
            },
            {
                "name": "v4.42.1: Patch release",
                "date": "2024-06-27T17:47:01Z"
            },
            {
                "name": "v4.42.0: Gemma 2, RTDETR, InstructBLIP, LLAVa Next, New Model Adder",
                "date": "2024-06-27T15:49:01Z"
            },
            {
                "name": "Release v4.41.2",
                "date": "2024-05-30T17:59:26Z"
            },
            {
                "name": "Release v4.41.1 Fix PaliGemma finetuning, and some small bugs",
                "date": "2024-05-22T20:36:44Z"
            },
            {
                "name": "v4.41.0: Phi3, JetMoE, PaliGemma, VideoLlava, Falcon2, FalconVLM & GGUF support",
                "date": "2024-05-17T16:28:45Z"
            },
            {
                "name": "v4.40.2",
                "date": "2024-05-06T16:09:45Z"
            },
            {
                "name": "v4.40.1: fix `EosTokenCriteria` for `Llama3` on `mps` ",
                "date": "2024-04-23T22:01:20Z"
            },
            {
                "name": "v4.40.0: Llama 3, Idefics 2, Recurrent Gemma, Jamba, DBRX, OLMo, Qwen2MoE, Grounding Dino",
                "date": "2024-04-18T14:01:56Z"
            },
            {
                "name": "Release v4.39.3",
                "date": "2024-04-02T09:36:10Z"
            },
            {
                "name": "Patch release v4.39.2",
                "date": "2024-03-28T17:36:27Z"
            },
            {
                "name": "Patch release v4.39.1",
                "date": "2024-03-22T17:01:14Z"
            },
            {
                "name": "Release v4.39.0",
                "date": "2024-03-21T01:18:35Z"
            },
            {
                "name": "v4.38.2",
                "date": "2024-03-01T03:24:28Z"
            },
            {
                "name": "v4.38.1",
                "date": "2024-02-22T00:24:34Z"
            },
            {
                "name": "v4.38: Gemma, Depth Anything, Stable LM; Static Cache, HF Quantizer, AQLM",
                "date": "2024-02-21T13:40:42Z"
            },
            {
                "name": "Patch release v4.37.2",
                "date": "2024-01-29T16:11:49Z"
            },
            {
                "name": "Patch release: v4.37.1",
                "date": "2024-01-24T16:15:23Z"
            },
            {
                "name": "v4.37 Qwen2, Phi-2, SigLIP, ViP-LLaVA, Fast2SpeechConformer, 4-bit serialization, Whisper longform generation",
                "date": "2024-01-22T11:20:12Z"
            },
            {
                "name": "Patch release: v4.36.2",
                "date": "2023-12-18T18:44:02Z"
            },
            {
                "name": "Patch release: v4.36.1",
                "date": "2023-12-14T06:57:27Z"
            },
            {
                "name": "v4.36: Mixtral, Llava/BakLlava, SeamlessM4T v2, AMD ROCm, F.sdpa wide-spread support",
                "date": "2023-12-11T12:12:34Z"
            },
            {
                "name": "Patch release: v4.35.2",
                "date": "2023-11-15T16:39:42Z"
            },
            {
                "name": "Patch release: v4.35.1",
                "date": "2023-11-14T14:59:15Z"
            },
            {
                "name": "Safetensors serialization by default, DistilWhisper, Fuyu, Kosmos-2, SeamlessM4T, Owl-v2",
                "date": "2023-11-02T17:00:05Z"
            },
            {
                "name": "Patch release: v4.34.1",
                "date": "2023-10-18T21:15:29Z"
            },
            {
                "name": "v4.34: Mistral, Persimmon, Prompt templating, Flash Attention 2, Tokenizer refactor",
                "date": "2023-10-03T15:00:52Z"
            },
            {
                "name": "Patch release: v4.33.3",
                "date": "2023-09-27T15:09:05Z"
            },
            {
                "name": "Patch release: v4.33.2",
                "date": "2023-09-15T20:24:23Z"
            },
            {
                "name": "Falcon, Code Llama, ViTDet, DINO v2, VITS",
                "date": "2023-09-06T21:14:16Z"
            },
            {
                "name": "Patch release: v4.32.1",
                "date": "2023-08-28T12:48:30Z"
            },
            {
                "name": "IDEFICS, GPTQ Quantization",
                "date": "2023-08-22T13:11:09Z"
            },
            {
                "name": "v4.31.0: Llama v2, MusicGen, Bark, MMS, EnCodec, InstructBLIP, Umt5, MRa, vIvIt",
                "date": "2023-07-18T20:16:04Z"
            },
            {
                "name": "v4.30.2: Patch release",
                "date": "2023-06-13T19:29:33Z"
            },
            {
                "name": "v4.30.1 Patch release",
                "date": "2023-06-09T15:58:10Z"
            },
            {
                "name": "v4.30.0: 100k, Agents improvements, Safetensors core dependency, Swiftformer, Autoformer, MobileViTv2, timm-as-a-backbone",
                "date": "2023-06-08T18:07:10Z"
            },
            {
                "name": "v4.29.2: Patch release",
                "date": "2023-05-16T19:47:56Z"
            },
            {
                "name": "V4.29.1: Patch release",
                "date": "2023-05-11T20:45:04Z"
            },
            {
                "name": "v4.29.0: Transformers Agents, SAM, RWKV, FocalNet, OpenLLaMa",
                "date": "2023-05-10T21:55:11Z"
            },
            {
                "name": "v4.28.1: Patch release",
                "date": "2023-04-14T16:57:36Z"
            },
            {
                "name": "v4.28.0: LLaMa, Pix2Struct, MatCha, DePlot, MEGA, NLLB-MoE, GPTBigCode",
                "date": "2023-04-13T18:40:02Z"
            },
            {
                "name": "v4.27.4: Patch release",
                "date": "2023-03-29T17:08:29Z"
            },
            {
                "name": "v4.27.3: Patch release",
                "date": "2023-03-23T19:01:13Z"
            },
            {
                "name": "v4.27.2: Patch release",
                "date": "2023-03-20T17:42:11Z"
            },
            {
                "name": "v4.27.1: Patch release",
                "date": "2023-03-15T19:50:18Z"
            },
            {
                "name": "BridgeTower, Whisper speedup, DETA, SpeechT5, BLIP-2, CLAP, ALIGN, API updates",
                "date": "2023-03-15T16:25:12Z"
            },
            {
                "name": "V4.26.1: Patch release",
                "date": "2023-02-09T19:30:35Z"
            },
            {
                "name": "v4.26.0: Generation configs, image processors, backbones and plenty of new models!",
                "date": "2023-01-25T18:32:44Z"
            },
            {
                "name": "PyTorch 2.0 support, Audio Spectogram Transformer, Jukebox, Switch Transformers and more",
                "date": "2022-12-02T16:03:31Z"
            },
            {
                "name": "v4.24.0: ESM-2/ESMFold, LiLT, Flan-T5, Table Transformer and Contrastive search decoding",
                "date": "2022-11-01T15:45:13Z"
            },
            {
                "name": "v4.23.1 Patch release",
                "date": "2022-10-11T12:02:53Z"
            },
            {
                "name": "v4.23.0: Whisper, Deformable DETR, Conditional DETR, MarkupLM, MSN, `safetensors`",
                "date": "2022-10-10T21:26:22Z"
            },
            {
                "name": "# v4.22.2 Patch release",
                "date": "2022-09-27T14:53:10Z"
            },
            {
                "name": "v4.22.1: Patch release:",
                "date": "2022-09-16T22:02:46Z"
            },
            {
                "name": "v4.22.0: Swin Transformer v2, VideoMAE, Donut, Pegasus-X, X-CLIP, ERNIE",
                "date": "2022-09-14T18:57:24Z"
            },
            {
                "name": "v4.21.3: Patch release",
                "date": "2022-09-05T10:17:00Z"
            },
            {
                "name": "v4.21.2: Patch release",
                "date": "2022-08-24T15:28:51Z"
            },
            {
                "name": "# v4.21.1: Patch release",
                "date": "2022-08-04T14:06:01Z"
            },
            {
                "name": "v4.21.0: TF XLA text generation - Custom Pipelines - OwlViT, NLLB, MobileViT, Nezha, GroupViT, MVP, CodeGen, UL2",
                "date": "2022-07-27T13:38:27Z"
            },
            {
                "name": "# v4.20.1 Patch release",
                "date": "2022-06-21T19:30:11Z"
            },
            {
                "name": "v4.20.0 Big Model inference, BLOOM, CvT, GPT Neo-X, LayoutLMv3, LeViT, LongT5, M-CTC-T, Trajectory Transformer and Wav2Vec2-Conformer",
                "date": "2022-06-16T16:20:16Z"
            },
            {
                "name": "# v4.19.4 Patch release",
                "date": "2022-06-10T19:42:31Z"
            },
            {
                "name": "# v4.19.3 Patch release",
                "date": "2022-06-09T17:13:15Z"
            },
            {
                "name": "v4.19.2: Patch release",
                "date": "2022-05-16T18:57:10Z"
            },
            {
                "name": "v4.19.1 Patch release",
                "date": "2022-05-13T18:23:46Z"
            },
            {
                "name": "v4.19.0: OPT, FLAVA, YOLOS, RegNet, TAPEX, Data2Vec vision, FSDP integration",
                "date": "2022-05-12T14:57:06Z"
            },
            {
                "name": "v4.18.0: Checkpoint sharding, vision models",
                "date": "2022-04-07T18:08:23Z"
            },
            {
                "name": "v4.17.0: XGLM, ConvNext, PoolFormer, PLBart, Data2Vec, MaskFormer and code in the Hub",
                "date": "2022-03-03T15:19:06Z"
            },
            {
                "name": "v4.16.2: Patch release",
                "date": "2022-01-31T16:54:26Z"
            },
            {
                "name": "# V4.16.1: Patch Release",
                "date": "2022-01-28T17:27:51Z"
            },
            {
                "name": "v4.16.0: Nystr\u00f6mformer, REALM, ViTMAE, ViLT, Swin Transformer, YOSO, ...",
                "date": "2022-01-27T18:14:30Z"
            },
            {
                "name": "v4.15.0",
                "date": "2021-12-22T19:34:56Z"
            },
            {
                "name": "v4.14.1: Patch release",
                "date": "2021-12-15T19:02:04Z"
            },
            {
                "name": "v4.14.0: Perceiver, Keras model cards",
                "date": "2021-12-15T17:27:37Z"
            },
            {
                "name": "v4.13.0: Perceiver, ImageGPT, mLUKE, Vision-Text dual encoders, QDQBert, new documentation frontend",
                "date": "2021-12-09T16:07:24Z"
            },
            {
                "name": "v4.12.5: Patch release",
                "date": "2021-11-17T16:38:32Z"
            },
            {
                "name": "v4.12.4: Patch release",
                "date": "2021-11-16T22:31:27Z"
            },
            {
                "name": "v4.12.3: Patch release",
                "date": "2021-11-03T13:05:08Z"
            },
            {
                "name": "v4.12.2: Patch release",
                "date": "2021-10-29T18:52:07Z"
            },
            {
                "name": "v4.12.1: Patch release",
                "date": "2021-10-29T18:43:39Z"
            },
            {
                "name": "v4.12.0: TrOCR, SEW & SEW-D, Unispeech & Unispeech-SAT, BARTPho",
                "date": "2021-10-28T16:57:28Z"
            },
            {
                "name": "v4.11.3: Patch release",
                "date": "2021-10-06T17:00:51Z"
            },
            {
                "name": "v4.11.2: Patch release",
                "date": "2021-09-30T15:55:59Z"
            },
            {
                "name": "v4.11.1: Patch release",
                "date": "2021-09-29T16:06:53Z"
            },
            {
                "name": "v4.11.0: GPT-J, Speech2Text2, FNet, Pipeline GPU utilization, dynamic model code loading",
                "date": "2021-09-27T18:20:26Z"
            },
            {
                "name": "v4.10.3: Patch release",
                "date": "2021-09-22T20:00:36Z"
            },
            {
                "name": "v4.10.2: Patch release",
                "date": "2021-09-10T16:37:01Z"
            },
            {
                "name": "v4.10.1: Patch release",
                "date": "2021-09-10T14:40:45Z"
            },
            {
                "name": "v4.10.0: LayoutLM-v2, LayoutXLM, BEiT",
                "date": "2021-08-31T14:02:59Z"
            },
            {
                "name": "v4.9.2: Patch release",
                "date": "2021-08-09T14:27:29Z"
            },
            {
                "name": "v4.9.1: Patch release",
                "date": "2021-07-26T14:24:25Z"
            },
            {
                "name": "v4.9.0: TensorFlow examples, CANINE, tokenizer training, ONNX rework",
                "date": "2021-07-22T10:53:51Z"
            },
            {
                "name": "Patch release: v4.8.2",
                "date": "2021-06-30T12:42:38Z"
            },
            {
                "name": "v4.8.1: Patch release",
                "date": "2021-06-24T14:15:37Z"
            },
            {
                "name": "v4.8.0 Integration with the Hub and Flax/JAX support",
                "date": "2021-06-23T17:28:58Z"
            },
            {
                "name": "v4.7.0: DETR, RoFormer, ByT5, HuBERT, support for torch 1.9.0",
                "date": "2021-06-17T16:20:52Z"
            },
            {
                "name": "v4.6.1: Patch release",
                "date": "2021-05-20T14:57:12Z"
            },
            {
                "name": "v4.6.0: ViT, DeiT, CLIP, LUKE, BigBirdPegasus, MegatronBERT",
                "date": "2021-05-12T15:07:35Z"
            },
            {
                "name": "v4.5.1: Patch release",
                "date": "2021-04-13T15:25:42Z"
            },
            {
                "name": "v4.5.0: BigBird, GPT Neo, Examples, Flax support",
                "date": "2021-04-06T16:50:00Z"
            },
            {
                "name": "Patch release V4.4.2",
                "date": "2021-03-18T19:17:38Z"
            },
            {
                "name": "v4.4.0: S2T, M2M100, I-BERT, mBART-50, DeBERTa-v2, XLSR-Wav2Vec2",
                "date": "2021-03-16T15:39:49Z"
            },
            {
                "name": "v4.3.3: Patch release",
                "date": "2021-02-24T20:16:39Z"
            },
            {
                "name": "V4.3.2: Patch release",
                "date": "2021-02-09T19:13:49Z"
            },
            {
                "name": "v4.3.1: Patch release",
                "date": "2021-02-09T09:01:11Z"
            },
            {
                "name": "v4.3.0: Wav2Vec2, ConvBERT, BORT, Amazon SageMaker",
                "date": "2021-02-08T17:45:23Z"
            },
            {
                "name": "v4.3.0.rc1: Wav2Vec2, ConvBERT, BORT, Amazon SageMaker",
                "date": "2021-02-04T21:20:26Z"
            },
            {
                "name": "v4.2.2: Patch release",
                "date": "2021-01-21T08:14:39Z"
            },
            {
                "name": "v4.2.1 Patch release",
                "date": "2021-01-14T13:22:31Z"
            },
            {
                "name": "v4.2.0: LED from AllenAI, Generation Scores, TensorFlow 2x speedup, faster import",
                "date": "2021-01-13T15:13:40Z"
            },
            {
                "name": "v4.1.1: TAPAS, MPNet, model parallelization, Sharded DDP, conda, multi-part downloads.",
                "date": "2020-12-17T17:09:51Z"
            },
            {
                "name": "Patch release: better error message & invalid trainer attribute",
                "date": "2020-12-09T17:07:39Z"
            },
            {
                "name": "Transformers v4.0.0: Fast tokenizers, model outputs, file reorganization",
                "date": "2020-11-30T17:01:56Z"
            },
            {
                "name": "Transformers v4.0.0-rc-1: Fast tokenizers, model outputs, file reorganization",
                "date": "2020-11-19T17:12:01Z"
            },
            {
                "name": "v3.5.1",
                "date": "2020-11-13T15:28:08Z"
            },
            {
                "name": "v3.5.0: Model versioning, TensorFlow encoder-decoder models, new scripts, refactor of the `generate` method",
                "date": "2020-11-10T13:57:07Z"
            },
            {
                "name": "ProphetNet, Blenderbot, SqueezeBERT, DeBERTa",
                "date": "2020-10-20T14:30:39Z"
            },
            {
                "name": "",
                "date": "2020-09-29T18:30:01Z"
            },
            {
                "name": "RAG",
                "date": "2020-09-28T14:32:36Z"
            },
            {
                "name": "Bert Seq2Seq models, FSMT, LayoutLM, Funnel Transformer, LXMERT",
                "date": "2020-09-22T15:58:15Z"
            },
            {
                "name": "Pegasus, DPR, self-documented outputs, new pipelines and MT support",
                "date": "2020-09-01T12:36:22Z"
            },
            {
                "name": "Tokenizer fixes",
                "date": "2020-07-06T22:54:07Z"
            },
            {
                "name": "Patch v3.0.1: Better backward compatibility for tokenizers",
                "date": "2020-07-03T15:37:17Z"
            },
            {
                "name": "New tokenizer API, TensorFlow improvements, enhanced documentation & tutorials",
                "date": "2020-06-29T15:11:47Z"
            },
            {
                "name": "Longformer",
                "date": "2020-06-02T14:26:04Z"
            },
            {
                "name": "Reformer, ElectraForSequenceClassification, ONNX conversion script",
                "date": "2020-05-22T14:57:08Z"
            },
            {
                "name": "Marian",
                "date": "2020-05-14T13:15:29Z"
            },
            {
                "name": "Trainer, TFTrainer, Multilingual BART, Encoder-decoder improvements, Generation Pipeline",
                "date": "2020-05-07T18:33:22Z"
            },
            {
                "name": "ELECTRA, Bad word filters, bugfixes & improvements",
                "date": "2020-04-06T14:15:38Z"
            },
            {
                "name": "T5 Model, BART summarization example and reduced memory, translation pipeline",
                "date": "2020-03-30T13:01:22Z"
            },
            {
                "name": "BART, organizations, community notebooks, lightning examples, dropping Python 3.5",
                "date": "2020-03-24T18:13:04Z"
            },
            {
                "name": "Patch v2.5.1: AutoTokenizer slow by default, bug fixes",
                "date": "2020-02-24T23:53:05Z"
            },
            {
                "name": "Rust Tokenizers, DistilBERT base cased, Model cards",
                "date": "2020-02-19T16:54:06Z"
            },
            {
                "name": "Patch v2.4.1: FlauBERT for AutoModel and AutoTokenizer",
                "date": "2020-01-31T19:58:13Z"
            },
            {
                "name": "FlauBERT, MMBT, UmBERTo, Dutch model, improved documentation, training from scratch, clean Python code",
                "date": "2020-01-31T14:55:51Z"
            },
            {
                "name": "Downstream NLP task API (feature extraction, text classification, NER, QA), Command-Line Interface and Serving \u2013 models: T5 \u2013 community-added models: Japanese & Finnish BERT, PPLM, XLM-R",
                "date": "2019-12-20T21:40:31Z"
            },
            {
                "name": "Bug fixes",
                "date": "2019-12-20T14:53:32Z"
            },
            {
                "name": "Bug fixes related to input shape in TensorFlow and tokenization messages",
                "date": "2019-12-03T16:23:44Z"
            },
            {
                "name": "ALBERT, CamemBERT, DistilRoberta, GPT-2 XL, and Encoder-Decoder architectures",
                "date": "2019-11-26T19:26:21Z"
            },
            {
                "name": "CTRL, DistilGPT-2, Pytorch TPU, tokenizer enhancements, guideline requirements",
                "date": "2019-10-11T14:50:36Z"
            },
            {
                "name": "Superseded by v2.1.1",
                "date": "2019-10-11T14:47:03Z"
            },
            {
                "name": "v2.0.0 - TF 2.0/PyTorch interoperability, improved tokenizers, improved torchscript support",
                "date": "2019-09-26T11:48:21Z"
            },
            {
                "name": "DistilBERT, GPT-2 Large, XLM multilingual models, torch.hub, bug fixes",
                "date": "2019-09-04T12:18:38Z"
            },
            {
                "name": "New model: RoBERTa, tokenizer sequence pair handling for sequence classification models.",
                "date": "2019-08-15T15:31:42Z"
            },
            {
                "name": "v1.0.0 - Name change, new models (XLNet, XLM), unified API for models and tokenizer, access to models internals, torchscript",
                "date": "2019-07-16T14:50:22Z"
            },
            {
                "name": "Better model/tokenizer serialization, relax network connection requirements, new scripts and bug fixes",
                "date": "2019-04-25T19:47:08Z"
            },
            {
                "name": "v0.6.1 - Small install tweak release",
                "date": "2019-02-18T11:01:17Z"
            },
            {
                "name": "v0.6.0 - Adding OpenAI small GPT-2 pretrained model",
                "date": "2019-02-18T10:40:11Z"
            },
            {
                "name": "Bug fix update to load the pretrained `TransfoXLModel` from s3, added fallback for OpenAIGPTTokenizer when SpaCy is not installed",
                "date": "2019-02-13T09:21:03Z"
            },
            {
                "name": "Adding OpenAI GPT and Transformer-XL pretrained models, python2 support, pre-training script for BERT, SQuAD 2.0 example",
                "date": "2019-02-11T13:52:14Z"
            },
            {
                "name": "4x speed-up using NVIDIA apex, new multi-choice classifier and example for SWAG-like dataset, pytorch v1.0, improved model loading, improved examples...",
                "date": "2018-12-14T14:21:57Z"
            },
            {
                "name": "Added two pre-trained models and one new fine-tuning class",
                "date": "2018-11-30T22:15:06Z"
            },
            {
                "name": "Small improvements and a few bug fixes.",
                "date": "2018-11-26T09:57:37Z"
            },
            {
                "name": "First release",
                "date": "2018-11-17T11:23:16Z"
            }
        ]
    }
}