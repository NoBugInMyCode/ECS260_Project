{
    "https://api.github.com/repos/openai/chatgpt-retrieval-plugin": {
        "forks": 3684,
        "watchers": 21100,
        "stars": 21100,
        "languages": {
            "Python": 331134,
            "Dockerfile": 584,
            "Makefile": 357
        },
        "commits": [
            "2024-04-24T17:48:16Z",
            "2024-02-08T02:47:05Z",
            "2023-12-04T02:55:19Z",
            "2023-11-16T22:47:34Z",
            "2023-10-17T17:05:45Z",
            "2023-10-17T17:02:44Z",
            "2023-10-17T17:02:17Z",
            "2023-10-17T17:01:28Z",
            "2023-10-17T17:00:52Z",
            "2023-10-17T17:00:21Z",
            "2023-07-28T18:37:10Z",
            "2023-06-23T23:37:46Z",
            "2023-06-01T20:59:34Z",
            "2023-05-31T21:13:12Z",
            "2023-05-30T16:20:19Z",
            "2023-05-30T16:19:34Z",
            "2023-05-30T16:08:09Z",
            "2023-05-19T04:54:30Z",
            "2023-05-18T03:55:25Z",
            "2023-05-16T16:48:02Z",
            "2023-05-16T16:33:24Z",
            "2023-05-15T23:31:34Z",
            "2023-05-15T22:44:22Z",
            "2023-05-15T22:00:37Z",
            "2023-05-15T21:55:54Z",
            "2023-05-15T20:46:47Z",
            "2023-05-15T19:59:48Z",
            "2023-05-15T19:57:16Z",
            "2023-05-15T18:51:19Z",
            "2023-05-15T15:40:41Z"
        ],
        "creation_date": "2023-03-23T06:06:22Z",
        "contributors": 30,
        "topics": [
            "chatgpt",
            "chatgpt-plugins"
        ],
        "subscribers": 320,
        "readme": "# ChatGPT Retrieval Plugin\n\nBuild Custom GPTs with a Retrieval Plugin backend to give ChatGPT access to personal documents.\n![Example Custom GPT Screenshot](/assets/example.png)\n\n## Introduction\n\nThe ChatGPT Retrieval Plugin repository provides a flexible solution for semantic search and retrieval of personal or organizational documents using natural language queries. It is a standalone retrieval backend, and can be used with [ChatGPT custom GPTs](https://chat.openai.com/gpts/discovery), [function calling](https://platform.openai.com/docs/guides/function-calling) with the [chat completions](https://platform.openai.com/docs/guides/text-generation) or [assistants APIs](https://platform.openai.com/docs/assistants/overview), or with the [ChatGPT plugins model (deprecated)](https://chat.openai.com/?model=gpt-4-plugins). ChatGPT and the Assistants API both natively support retrieval from uploaded files, so you should use the Retrieval Plugin as a backend only if you want more granular control of your retrieval system (e.g. document text chunk length, embedding model / size, etc.).\n\nThe repository is organized into several directories:\n\n| Directory                       | Description                                                                                                                |\n| ------------------------------- | -------------------------------------------------------------------------------------------------------------------------- |\n| [`datastore`](/datastore)       | Contains the core logic for storing and querying document embeddings using various vector database providers.              |\n| [`docs`](/docs)                 | Includes documentation for setting up and using each vector database provider, webhooks, and removing unused dependencies. |\n| [`examples`](/examples)         | Provides example configurations, authentication methods, and provider-specific examples.                                   |\n| [`local_server`](/local_server) | Contains an implementation of the Retrieval Plugin configured for localhost testing.                                       |\n| [`models`](/models)             | Contains the data models used by the plugin, such as document and metadata models.                                         |\n| [`scripts`](/scripts)           | Offers scripts for processing and uploading documents from different data sources.                                         |\n| [`server`](/server)             | Houses the main FastAPI server implementation.                                                                             |\n| [`services`](/services)         | Contains utility services for tasks like chunking, metadata extraction, and PII detection.                                 |\n| [`tests`](/tests)               | Includes integration tests for various vector database providers.                                                          |\n| [`.well-known`](/.well-known)   | Stores the plugin manifest file and OpenAPI schema, which define the plugin configuration and API specification.           |\n\nThis README provides detailed information on how to set up, develop, and deploy the ChatGPT Retrieval Plugin (stand-alone retrieval backend).\n\n## Table of Contents\n\n- [Quickstart](#quickstart)\n- [About](#about)\n  - [Retrieval Plugin](#retrieval-plugin)\n  - [Retrieval Plugin with custom GPTs](#retrieval-plugin-with-custom-gpts)\n  - [Retrieval Plugin with function calling](#retrieval-plugin-with-function-calling)\n  - [Retrieval Plugin with the plugins model (deprecated)](#chatgpt-plugins-model)\n  - [API Endpoints](#api-endpoints)\n  - [Memory Feature](#memory-feature)\n  - [Security](#security)\n  - [Choosing an Embeddings Model](#choosing-an-embeddings-model)\n- [Development](#development)\n  - [Setup](#setup)\n    - [General Environment Variables](#general-environment-variables)\n  - [Choosing a Vector Database](#choosing-a-vector-database)\n    - [Pinecone](#pinecone)\n    - [Elasticsearch](#elasticsearch)\n    - [MongoDB Atlas](#mongodb-atlas)\n    - [Weaviate](#weaviate)\n    - [Zilliz](#zilliz)\n    - [Milvus](#milvus)\n    - [Qdrant](#qdrant)\n    - [Redis](#redis)\n    - [Llama Index](#llamaindex)\n    - [Chroma](#chroma)\n    - [Azure Cognitive Search](#azure-cognitive-search)\n    - [Azure CosmosDB Mongo vCore](#azure-cosmosdb-mongo-vcore)\n    - [Supabase](#supabase)\n    - [Postgres](#postgres)\n    - [AnalyticDB](#analyticdb)\n  - [Running the API Locally](#running-the-api-locally)\n  - [Personalization](#personalization)\n  - [Authentication Methods](#authentication-methods)\n- [Deployment](#deployment)\n- [Webhooks](#webhooks)\n- [Scripts](#scripts)\n- [Limitations](#limitations)\n- [Contributors](#contributors)\n- [Future Directions](#future-directions)\n\n## Quickstart\n\nFollow these steps to quickly set up and run the ChatGPT Retrieval Plugin:\n\n1. Install Python 3.10, if not already installed.\n2. Clone the repository: `git clone https://github.com/openai/chatgpt-retrieval-plugin.git`\n3. Navigate to the cloned repository directory: `cd /path/to/chatgpt-retrieval-plugin`\n4. Install poetry: `pip install poetry`\n5. Create a new virtual environment with Python 3.10: `poetry env use python3.10`\n6. Activate the virtual environment: `poetry shell`\n7. Install app dependencies: `poetry install`\n8. Create a [bearer token](#general-environment-variables)\n9. Set the required environment variables:\n\n   ```\n   export DATASTORE=<your_datastore>\n   export BEARER_TOKEN=<your_bearer_token>\n   export OPENAI_API_KEY=<your_openai_api_key>\n   export EMBEDDING_DIMENSION=256 # edit this value based on the dimension of the embeddings you want to use\n   export EMBEDDING_MODEL=text-embedding-3-large # edit this based on your model preference, e.g. text-embedding-3-small, text-embedding-ada-002\n\n   # Optional environment variables used when running Azure OpenAI\n   export OPENAI_API_BASE=https://<AzureOpenAIName>.openai.azure.com/\n   export OPENAI_API_TYPE=azure\n   export OPENAI_EMBEDDINGMODEL_DEPLOYMENTID=<Name of embedding model deployment>\n   export OPENAI_METADATA_EXTRACTIONMODEL_DEPLOYMENTID=<Name of deployment of model for metatdata>\n   export OPENAI_COMPLETIONMODEL_DEPLOYMENTID=<Name of general model deployment used for completion>\n   export OPENAI_EMBEDDING_BATCH_SIZE=<Batch size of embedding, for AzureOAI, this value need to be set as 1>\n\n   # Add the environment variables for your chosen vector DB.\n   # Some of these are optional; read the provider's setup docs in /docs/providers for more information.\n\n   # Pinecone\n   export PINECONE_API_KEY=<your_pinecone_api_key>\n   export PINECONE_ENVIRONMENT=<your_pinecone_environment>\n   export PINECONE_INDEX=<your_pinecone_index>\n\n   # Weaviate\n   export WEAVIATE_URL=<your_weaviate_instance_url>\n   export WEAVIATE_API_KEY=<your_api_key_for_WCS>\n   export WEAVIATE_CLASS=<your_optional_weaviate_class>\n\n   # Zilliz\n   export ZILLIZ_COLLECTION=<your_zilliz_collection>\n   export ZILLIZ_URI=<your_zilliz_uri>\n   export ZILLIZ_USER=<your_zilliz_username>\n   export ZILLIZ_PASSWORD=<your_zilliz_password>\n\n   # Milvus\n   export MILVUS_COLLECTION=<your_milvus_collection>\n   export MILVUS_HOST=<your_milvus_host>\n   export MILVUS_PORT=<your_milvus_port>\n   export MILVUS_USER=<your_milvus_username>\n   export MILVUS_PASSWORD=<your_milvus_password>\n\n   # Qdrant\n   export QDRANT_URL=<your_qdrant_url>\n   export QDRANT_PORT=<your_qdrant_port>\n   export QDRANT_GRPC_PORT=<your_qdrant_grpc_port>\n   export QDRANT_API_KEY=<your_qdrant_api_key>\n   export QDRANT_COLLECTION=<your_qdrant_collection>\n\n   # AnalyticDB\n   export PG_HOST=<your_analyticdb_host>\n   export PG_PORT=<your_analyticdb_port>\n   export PG_USER=<your_analyticdb_username>\n   export PG_PASSWORD=<your_analyticdb_password>\n   export PG_DATABASE=<your_analyticdb_database>\n   export PG_COLLECTION=<your_analyticdb_collection>\n\n\n   # Redis\n   export REDIS_HOST=<your_redis_host>\n   export REDIS_PORT=<your_redis_port>\n   export REDIS_PASSWORD=<your_redis_password>\n   export REDIS_INDEX_NAME=<your_redis_index_name>\n   export REDIS_DOC_PREFIX=<your_redis_doc_prefix>\n   export REDIS_DISTANCE_METRIC=<your_redis_distance_metric>\n   export REDIS_INDEX_TYPE=<your_redis_index_type>\n\n   # Llama\n   export LLAMA_INDEX_TYPE=<gpt_vector_index_type>\n   export LLAMA_INDEX_JSON_PATH=<path_to_saved_index_json_file>\n   export LLAMA_QUERY_KWARGS_JSON_PATH=<path_to_saved_query_kwargs_json_file>\n   export LLAMA_RESPONSE_MODE=<response_mode_for_query>\n\n   # Chroma\n   export CHROMA_COLLECTION=<your_chroma_collection>\n   export CHROMA_IN_MEMORY=<true_or_false>\n   export CHROMA_PERSISTENCE_DIR=<your_chroma_persistence_directory>\n   export CHROMA_HOST=<your_chroma_host>\n   export CHROMA_PORT=<your_chroma_port>\n\n   # Azure Cognitive Search\n   export AZURESEARCH_SERVICE=<your_search_service_name>\n   export AZURESEARCH_INDEX=<your_search_index_name>\n   export AZURESEARCH_API_KEY=<your_api_key> (optional, uses key-free managed identity if not set)\n\n   # Azure CosmosDB Mongo vCore\n   export AZCOSMOS_API = <your azure cosmos db api, for now it only supports mongo>\n   export AZCOSMOS_CONNSTR = <your azure cosmos db mongo vcore connection string>\n   export AZCOSMOS_DATABASE_NAME = <your mongo database name>\n   export AZCOSMOS_CONTAINER_NAME = <your mongo container name>\n\n   # Supabase\n   export SUPABASE_URL=<supabase_project_url>\n   export SUPABASE_ANON_KEY=<supabase_project_api_anon_key>\n\n   # Postgres\n   export PG_HOST=<postgres_host>\n   export PG_PORT=<postgres_port>\n   export PG_USER=<postgres_user>\n   export PG_PASSWORD=<postgres_password>\n   export PG_DB=<postgres_database>\n\n   # Elasticsearch\n   export ELASTICSEARCH_URL=<elasticsearch_host_and_port> (either specify host or cloud_id)\n   export ELASTICSEARCH_CLOUD_ID=<elasticsearch_cloud_id>\n\n   export ELASTICSEARCH_USERNAME=<elasticsearch_username>\n   export ELASTICSEARCH_PASSWORD=<elasticsearch_password>\n   export ELASTICSEARCH_API_KEY=<elasticsearch_api_key>\n\n   export ELASTICSEARCH_INDEX=<elasticsearch_index_name>\n   export ELASTICSEARCH_REPLICAS=<elasticsearch_replicas>\n   export ELASTICSEARCH_SHARDS=<elasticsearch_shards>\n\n   # MongoDB Atlas\n   export MONGODB_URI=<mongodb_uri>\n   export MONGODB_DATABASE=<mongodb_database>\n   export MONGODB_COLLECTION=<mongodb_collection>\n   export MONGODB_INDEX=<mongodb_index>\n   ```\n\n10. Run the API locally: `poetry run start`\n11. Access the API documentation at `http://0.0.0.0:8000/docs` and test the API endpoints (make sure to add your bearer token).\n\n## About\n\n### Retrieval Plugin\n\nThis is a standalone retrieval backend that can be used with [ChatGPT custom GPTs](https://chat.openai.com/gpts/discovery), [function calling](https://platform.openai.com/docs/guides/function-calling) with the [chat completions](https://platform.openai.com/docs/guides/text-generation) or [assistants APIs](https://platform.openai.com/docs/assistants/overview), or with the [ChatGPT plugins model (deprecated)](https://chat.openai.com/?model=gpt-4-plugins).\n\nIt enables a model to carry out semantic search and retrieval of personal or organizational documents, and write answers informed by relevent retrieved context (sometimes referred to as \"Retrieval-Augmented Generation\" or \"RAG\"). It allows users to obtain the most relevant document snippets from their data sources, such as files, notes, or emails, by asking questions or expressing needs in natural language. Enterprises can make their internal documents available to their employees through ChatGPT using this plugin.\n\nThe plugin uses OpenAI's embeddings model (`text-embedding-3-large` 256 dimension embeddings by default) to generate embeddings of document chunks, and then stores and queries them using a vector database on the backend. As an open-source and self-hosted solution, developers can deploy their own Retrieval Plugin and register it with ChatGPT. The Retrieval Plugin supports several vector database providers, allowing developers to choose their preferred one from a list.\n\nA FastAPI server exposes the plugin's endpoints for upserting, querying, and deleting documents. Users can refine their search results by using metadata filters by source, date, author, or other criteria. The plugin can be hosted on any cloud platform that supports Docker containers, such as Fly.io, Heroku, Render, or Azure Container Apps. To keep the vector database updated with the latest documents, the plugin can process and store documents from various data sources continuously, using incoming webhooks to the upsert and delete endpoints. Tools like [Zapier](https://zapier.com) or [Make](https://www.make.com) can help configure the webhooks based on events or schedules.\n\n### Retrieval Plugin with Custom GPTs\n\nTo create a custom GPT that can use your Retrieval Plugin for semantic search and retrieval of your documents, and even store new information back to the database, you first need to have deployed a Retrieval Plugin. For detailed instructions on how to do this, please refer to the [Deployment section](#deployment). Once you have your app URL (e.g., `https://your-app-url.com`), take the following steps:\n\n1. Navigate to the create GPT page at `https://chat.openai.com/gpts/editor`.\n2. Follow the standard creation flow to set up your GPT.\n3. Navigate to the \"Configure\" tab. Here, you can manually fill in fields such as name, description, and instructions, or use the smart creator for assistance.\n4. Under the \"Actions\" section, click on \"Create new action\".\n5. Choose an authentication method. The Retrieval Plugin supports None, API key (Basic or Bearer) and OAuth. For more information on these methods, refer to the [Authentication Methods Section](#authentication-methods).\n6. Import the OpenAPI schema. You can either:\n   - Import directly from the OpenAPI schema hosted in your app at `https://your-app-url.com/.well-known/openapi.yaml`.\n   - Copy and paste the contents of [this file](/.well-known/openapi.yaml) into the Schema input area if you only want to expose the query endpoint to the GPT. Remember to change the URL under the `-servers` section of the OpenAPI schema you paste in.\n7. Optionally, you might want to add a fetch endpoint. This would involve editing the [`/server/main.py`](/server/main.py) file to add an endpoint and implement this for your chosen vector database. If you make this change, please consider contributing it back to the project by opening a pull request! Adding the fetch endpoint to the OpenAPI schema would allow the model to fetch more content from a document by ID if some text is cut off in the retrieved result. It might also be useful to pass in a string with the text from the retrieved result and an option to return a fixed length of context before and after the retrieved result.\n8. If you want the GPT to be able to save information back to the vector database, you can give it access to the Retrieval Plugin's `/upsert` endpoint. To do this, copy the contents of [this file](/examples/memory/openapi.yaml) into the schema area. This allows the GPT to store new information it generates or learns during the conversation. More details on this feature can be found at [Memory Feature](#memory-feature) and [in the docs here](/examples/memory).\n\nRemember: ChatGPT and custom GPTs natively support retrieval from uploaded files, so you should use the Retrieval Plugin as a backend only if you want more granular control of your retrieval system (e.g. self-hosting, embedding chunk length, embedding model / size, etc.).\n\n### Retrieval Plugin with Function Calling\n\nThe Retrieval Plugin can be integrated with function calling in both the [Chat Completions API](https://platform.openai.com/docs/guides/function-calling) and the [Assistants API](https://platform.openai.com/docs/assistants/overview). This allows the model to decide when to use your functions (query, fetch, upsert) based on the conversation context.\n\n#### Function Calling with Chat Completions\n\nIn a call to the chat completions API, you can describe functions and have the model generate a JSON object containing arguments to call one or many functions. The latest models (gpt-3.5-turbo-0125 and gpt-4-turbo-preview) have been trained to detect when a function should be called and to respond with JSON that adheres to the function signature.\n\nYou can define the functions for the Retrieval Plugin endpoints and pass them in as tools when you use the Chat Completions API with one of the latest models. The model will then intelligently call the functions. You can use function calling to write queries to your APIs, call the endpoint on the backend, and return the response as a tool message to the model to continue the conversation. The function definitions/schemas and an example can be found [here](/examples/function-calling/).\n\n#### Function Calling with Assistants API\n\nYou can use the same function definitions with the OpenAI [Assistants API](https://platform.openai.com/docs/assistants/overview), specifically the [function calling in tool use](https://platform.openai.com/docs/assistants/tools/function-calling). The Assistants API allows you to build AI assistants within your own applications, leveraging models, tools, and knowledge to respond to user queries. The function definitions/schemas and an example can be found [here](/examples/function-calling/). The Assistants API natively supports retrieval from uploaded files, so you should use the Retrieval Plugin with function calling only if you want more granular control of your retrieval system (e.g. embedding chunk length, embedding model / size, etc.).\n\nParallel function calling is supported for both the Chat Completions API and the Assistants API. This means you can perform multiple tasks, such as querying something and saving something back to the vector database, in the same message.\n\nRead more about function calling with the Retrieval Plugin [here](/examples/function-calling/).\n\n### ChatGPT Plugins Model\n\n(deprecated) We recommend using custom actions with GPTs to make use of the Retrieval Plugin through ChatGPT. Instrucitons for using retrieval with the deprecated plugins model can be found [here](/docs/deprecated/plugins.md).\n\n### API Endpoints\n\nThe Retrieval Plugin is built using FastAPI, a web framework for building APIs with Python. FastAPI allows for easy development, validation, and documentation of API endpoints. Find the FastAPI documentation [here](https://fastapi.tiangolo.com/).\n\nOne of the benefits of using FastAPI is the automatic generation of interactive API documentation with Swagger UI. When the API is running locally, Swagger UI at `<local_host_url i.e. http://0.0.0.0:8000>/docs` can be used to interact with the API endpoints, test their functionality, and view the expected request and response models.\n\nThe plugin exposes the following endpoints for upserting, querying, and deleting documents from the vector database. All requests and responses are in JSON format, and require a valid bearer token as an authorization header.\n\n- `/upsert`: This endpoint allows uploading one or more documents and storing their text and metadata in the vector database. The documents are split into chunks of around 200 tokens, each with a unique ID. The endpoint expects a list of documents in the request body, each with a `text` field, and optional `id` and `metadata` fields. The `metadata` field can contain the following optional subfields: `source`, `source_id`, `url`, `created_at`, and `author`. The endpoint returns a list of the IDs of the inserted documents (an ID is generated if not initially provided).\n\n- `/upsert-file`: This endpoint allows uploading a single file (PDF, TXT, DOCX, PPTX, or MD) and storing its text and metadata in the vector database. The file is converted to plain text and split into chunks of around 200 tokens, each with a unique ID. The endpoint returns a list containing the generated id of the inserted file.\n\n- `/query`: This endpoint allows querying the vector database using one or more natural language queries and optional metadata filters. The endpoint expects a list of queries in the request body, each with a `query` and optional `filter` and `top_k` fields. The `filter` field should contain a subset of the following subfields: `source`, `source_id`, `document_id`, `url`, `created_at`, and `author`. The `top_k` field specifies how many results to return for a given query, and the default value is 3. The endpoint returns a list of objects that each contain a list of the most relevant document chunks for the given query, along with their text, metadata and similarity scores.\n\n- `/delete`: This endpoint allows deleting one or more documents from the vector database using their IDs, a metadata filter, or a delete_all flag. The endpoint expects at least one of the following parameters in the request body: `ids`, `filter`, or `delete_all`. The `ids` parameter should be a list of document IDs to delete; all document chunks for the document with these IDS will be deleted. The `filter` parameter should contain a subset of the following subfields: `source`, `source_id`, `document_id`, `url`, `created_at`, and `author`. The `delete_all` parameter should be a boolean indicating whether to delete all documents from the vector database. The endpoint returns a boolean indicating whether the deletion was successful.\n\nThe detailed specifications and examples of the request and response models can be found by running the app locally and navigating to http://0.0.0.0:8000/openapi.json, or in the OpenAPI schema [here](/.well-known/openapi.yaml). Note that the OpenAPI schema only contains the `/query` endpoint, because that is the only function that ChatGPT needs to access. This way, ChatGPT can use the plugin only to retrieve relevant documents based on natural language queries or needs. However, if developers want to also give ChatGPT the ability to remember things for later, they can use the `/upsert` endpoint to save snippets from the conversation to the vector database. An example of a manifest and OpenAPI schema that gives ChatGPT access to the `/upsert` endpoint can be found [here](/examples/memory).\n\nTo include custom metadata fields, edit the `DocumentMetadata` and `DocumentMetadataFilter` data models [here](/models/models.py), and update the OpenAPI schema [here](/.well-known/openapi.yaml). You can update this easily by running the app locally, copying the JSON found at http://0.0.0.0:8000/sub/openapi.json, and converting it to YAML format with [Swagger Editor](https://editor.swagger.io/). Alternatively, you can replace the `openapi.yaml` file with an `openapi.json` file.\n\n### Memory Feature\n\nA notable feature of the Retrieval Plugin is its capacity to provide ChatGPT with memory. By using the plugin's upsert endpoint, ChatGPT can save snippets from the conversation to the vector database for later reference (only when prompted to do so by the user). This functionality contributes to a more context-aware chat experience by allowing ChatGPT to remember and retrieve information from previous conversations. Learn how to configure the Retrieval Plugin with memory [here](/examples/memory).\n\n### Security\n\nThe Retrieval Plugin allows ChatGPT to search a vector database of content, and then add the best results into the ChatGPT session. This means it doesn\u2019t have any external effects, and the main risk consideration is data authorization and privacy. Developers should only add content into their Retrieval Plugin that they have authorization for and that they are fine with appearing in users\u2019 ChatGPT sessions. You can choose from a number of different authentication methods to secure the plugin (more information [here](#authentication-methods)).\n\n### Choosing an Embeddings Model\n\nThe ChatGPT Retrieval Plugin uses OpenAI's embeddings models to generate embeddings of document chunks. The default model for the Retrieval Plugin is `text-embedding-3-large` with 256 dimensions. OpenAI offers two latest embeddings models, `text-embedding-3-small` and `text-embedding-3-large`, as well as an older model, `text-embedding-ada-002`.\n\nThe new models support shortening embeddings without significant loss of retrieval accuracy, allowing you to balance retrieval accuracy, cost, and speed.\n\nHere's a comparison of the models:\n\n| Model                  | Embedding Size | Average MTEB Score | Cost per 1k Tokens |\n| ---------------------- | -------------- | ------------------ | ------------------ |\n| text-embedding-3-large | 3072           | 64.6%              | $0.00013           |\n| text-embedding-3-large | 1024           | 64.1%              | $0.00013           |\n| text-embedding-3-large | 256            | 62.0%              | $0.00013           |\n| text-embedding-3-small | 1536           | 62.3%              | $0.00002           |\n| text-embedding-3-small | 512            | 61.6%              | $0.00002           |\n| text-embedding-ada-002 | 1536           | 61.0%              | $0.0001            |\n\nWhen choosing a model, consider:\n\n1. **Retrieval Accuracy vs Cost**: `text-embedding-3-large` offers the highest accuracy but at a higher cost. `text-embedding-3-small` is more cost-effective with competitive accuracy. The older `text-embedding-ada-002` model has the lowest accuracy.\n\n2. **Embedding Size**: Larger embeddings provide better accuracy but consume more storage and could be slower to query. You can adjust the size of the embeddings to balance these factors.\n\nFor example, if your vector database supports up to 1024 dimensions, you can use `text-embedding-3-large` and set the dimensions API parameter to 1024. This shortens the embedding from 3072 dimensions, trading off some accuracy for lower storage and query costs.\n\nTo change your chosen embeddings model and size, edit the following environment variables:\n\n```\nEMBEDDING_DIMENSION=256 # edit this value based on the dimension of the embeddings you want to use\nEMBEDDING_MODEL=\"text-embedding-3-large\" # edit this value based on the model you want to use e.g. text-embedding-3-small, text-embedding-ada-002\n```\n\n## Development\n\n### Setup\n\nThis app uses Python 3.10, and [poetry](https://python-poetry.org/) for dependency management.\n\nInstall Python 3.10 on your machine if it isn't already installed. It can be downloaded from the official [Python website](https://www.python.org/downloads/) or with a package manager like `brew` or `apt`, depending on your system.\n\nClone the repository from GitHub:\n\n```\ngit clone https://github.com/openai/chatgpt-retrieval-plugin.git\n```\n\nNavigate to the cloned repository directory:\n\n```\ncd /path/to/chatgpt-retrieval-plugin\n```\n\nInstall poetry:\n\n```\npip install poetry\n```\n\nCreate a new virtual environment that uses Python 3.10:\n\n```\npoetry env use python3.10\npoetry shell\n```\n\nInstall app dependencies using poetry:\n\n```\npoetry install\n```\n\n**Note:** If adding dependencies in the `pyproject.toml`, make sure to run `poetry lock` and `poetry install`.\n\n#### General Environment Variables\n\nThe API requires the following environment variables to work:\n\n| Name             | Required | Description                                                                                                                                                                                                                                                   |\n| ---------------- | -------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `DATASTORE`      | Yes      | This specifies the vector database provider you want to use to store and query embeddings. You can choose from `elasticsearch`, `chroma`, `pinecone`, `weaviate`, `zilliz`, `milvus`, `qdrant`, `redis`, `azuresearch`, `supabase`, `postgres`, `analyticdb`, `mongodb-atlas`. |\n| `BEARER_TOKEN`   | Yes      | This is a secret token that you need to authenticate your requests to the API. You can generate one using any tool or method you prefer, such as [jwt.io](https://jwt.io/).                                                                                   |\n| `OPENAI_API_KEY` | Yes      | This is your OpenAI API key that you need to generate embeddings using the one of the OpenAI embeddings model. You can get an API key by creating an account on [OpenAI](https://openai.com/).                                                                |\n\n### Using the plugin with Azure OpenAI\n\nThe Azure Open AI uses URLs that are specific to your resource and references models not by model name but by the deployment id. As a result, you need to set additional environment variables for this case.\n\nIn addition to the `OPENAI_API_BASE` (your specific URL) and `OPENAI_API_TYPE` (azure), you should also set `OPENAI_EMBEDDINGMODEL_DEPLOYMENTID` which specifies the model to use for getting embeddings on upsert and query. For this, we recommend deploying `text-embedding-ada-002` model and using the deployment name here.\n\nIf you wish to use the data preparation scripts, you will also need to set `OPENAI_METADATA_EXTRACTIONMODEL_DEPLOYMENTID`, used for metadata extraction and\n`OPENAI_COMPLETIONMODEL_DEPLOYMENTID`, used for PII handling.\n\n### Choosing a Vector Database\n\nThe plugin supports several vector database providers, each with different features, performance, and pricing. Depending on which one you choose, you will need to use a different Dockerfile and set different environment variables. The following sections provide brief introductions to each vector database provider.\n\nFor more detailed instructions on setting up and using each vector database provider, please refer to the respective documentation in the `/docs/providers/<datastore_name>/setup.md` file ([folders here](/docs/providers)).\n\n#### Pinecone\n\n[Pinecone](https://www.pinecone.io) is a managed vector database designed for speed, scale, and rapid deployment to production. It supports hybrid search and is currently the only datastore to natively support SPLADE sparse vectors. For detailed setup instructions, refer to [`/docs/providers/pinecone/setup.md`](/docs/providers/pinecone/setup.md).\n\n#### Weaviate\n\n[Weaviate](https://weaviate.io/) is an open-source vector search engine built to scale seamlessly into billions of data objects. It supports hybrid search out-of-the-box, making it suitable for users who require efficient keyword searches. Weaviate can be self-hosted or managed, offering flexibility in deployment. For detailed setup instructions, refer to [`/docs/providers/weaviate/setup.md`](/docs/providers/weaviate/setup.md).\n\n#### Zilliz\n\n[Zilliz](https://zilliz.com) is a managed cloud-native vector database designed for billion-scale data. It offers a wide range of features, including multiple indexing algorithms, distance metrics, scalar filtering, time travel searches, rollback with snapshots, full RBAC, 99.9% uptime, separated storage and compute, and multi-language SDKs. For detailed setup instructions, refer to [`/docs/providers/zilliz/setup.md`](/docs/providers/zilliz/setup.md).\n\n#### Milvus\n\n[Milvus](https://milvus.io/) is an open-source, cloud-native vector database that scales to billions of vectors. It is the open-source version of Zilliz and shares many of its features, such as various indexing algorithms, distance metrics, scalar filtering, time travel searches, rollback with snapshots, multi-language SDKs, storage and compute separation, and cloud scalability. For detailed setup instructions, refer to [`/docs/providers/milvus/setup.md`](/docs/providers/milvus/setup.md).\n\n#### Qdrant\n\n[Qdrant](https://qdrant.tech/) is a vector database capable of storing documents and vector embeddings. It offers both self-hosted and managed [Qdrant Cloud](https://cloud.qdrant.io/) deployment options, providing flexibility for users with different requirements. For detailed setup instructions, refer to [`/docs/providers/qdrant/setup.md`](/docs/providers/qdrant/setup.md).\n\n#### Redis\n\n[Redis](https://redis.com/solutions/use-cases/vector-database/) is a real-time data platform suitable for a variety of use cases, including everyday applications and AI/ML workloads. It can be used as a low-latency vector engine by creating a Redis database with the [Redis Stack docker container](/examples/docker/redis/docker-compose.yml). For a hosted/managed solution, [Redis Cloud](https://app.redislabs.com/#/) is available. For detailed setup instructions, refer to [`/docs/providers/redis/setup.md`](/docs/providers/redis/setup.md).\n\n#### LlamaIndex\n\n[LlamaIndex](https://github.com/jerryjliu/llama_index) is a central interface to connect your LLM's with external data.\nIt provides a suite of in-memory indices over your unstructured and structured data for use with ChatGPT.\nUnlike standard vector databases, LlamaIndex supports a wide range of indexing strategies (e.g. tree, keyword table, knowledge graph) optimized for different use-cases.\nIt is light-weight, easy-to-use, and requires no additional deployment.\nAll you need to do is specifying a few environment variables (optionally point to an existing saved Index json file).\nNote that metadata filters in queries are not yet supported.\nFor detailed setup instructions, refer to [`/docs/providers/llama/setup.md`](/docs/providers/llama/setup.md).\n\n#### Chroma\n\n[Chroma](https://trychroma.com) is an AI-native open-source embedding database designed to make getting started as easy as possible. Chroma runs in-memory, or in a client-server setup. It supports metadata and keyword filtering out of the box. For detailed instructions, refer to [`/docs/providers/chroma/setup.md`](/docs/providers/chroma/setup.md).\n\n#### Azure Cognitive Search\n\n[Azure Cognitive Search](https://azure.microsoft.com/products/search/) is a complete retrieval cloud service that supports vector search, text search, and hybrid (vectors + text combined to yield the best of the two approaches). It also offers an [optional L2 re-ranking step](https://learn.microsoft.com/azure/search/semantic-search-overview) to further improve results quality. For detailed setup instructions, refer to [`/docs/providers/azuresearch/setup.md`](/docs/providers/azuresearch/setup.md)\n\n#### Azure CosmosDB Mongo vCore\n\n[Azure CosmosDB Mongo vCore](https://learn.microsoft.com/en-us/azure/cosmos-db/mongodb/vcore/) supports vector search on embeddings, and it could be used to seamlessly integrate your AI-based applications with your data stored in the Azure CosmosDB. For detailed instructions, refer to [`/docs/providers/azurecosmosdb/setup.md`](/docs/providers/azurecosmosdb/setup.md)\n\n#### Supabase\n\n[Supabase](https://supabase.com/blog/openai-embeddings-postgres-vector) offers an easy and efficient way to store vectors via [pgvector](https://github.com/pgvector/pgvector) extension for Postgres Database. [You can use Supabase CLI](https://github.com/supabase/cli) to set up a whole Supabase stack locally or in the cloud or you can also use docker-compose, k8s and other options available. For a hosted/managed solution, try [Supabase.com](https://supabase.com/) and unlock the full power of Postgres with built-in authentication, storage, auto APIs, and Realtime features. For detailed setup instructions, refer to [`/docs/providers/supabase/setup.md`](/docs/providers/supabase/setup.md).\n\n#### Postgres\n\n[Postgres](https://www.postgresql.org) offers an easy and efficient way to store vectors via [pgvector](https://github.com/pgvector/pgvector) extension. To use pgvector, you will need to set up a PostgreSQL database with the pgvector extension enabled. For example, you can [use docker](https://www.docker.com/blog/how-to-use-the-postgres-docker-official-image/) to run locally. For a hosted/managed solution, you can use any of the cloud vendors which support [pgvector](https://github.com/pgvector/pgvector#hosted-postgres). For detailed setup instructions, refer to [`/docs/providers/postgres/setup.md`](/docs/providers/postgres/setup.md).\n\n#### AnalyticDB\n\n[AnalyticDB](https://www.alibabacloud.com/help/en/analyticdb-for-postgresql/latest/product-introduction-overview) is a distributed cloud-native vector database designed for storing documents and vector embeddings. It is fully compatible with PostgreSQL syntax and managed by Alibaba Cloud. AnalyticDB offers a powerful vector compute engine, processing billions of data vectors and providing features such as indexing algorithms, structured and unstructured data capabilities, real-time updates, distance metrics, scalar filtering, and time travel searches. For detailed setup instructions, refer to [`/docs/providers/analyticdb/setup.md`](/docs/providers/analyticdb/setup.md).\n\n#### Elasticsearch\n\n[Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html) currently supports storing vectors through the `dense_vector` field type and uses them to calculate document scores. Elasticsearch 8.0 builds on this functionality to support fast, approximate nearest neighbor search (ANN). This represents a much more scalable approach, allowing vector search to run efficiently on large datasets. For detailed setup instructions, refer to [`/docs/providers/elasticsearch/setup.md`](/docs/providers/elasticsearch/setup.md).\n\n#### Mongodb-Atlas\n\n[MongoDB Atlas](https://www.mongodb.com/docs/atlas/getting-started/) Currently, the procedure involves generating an Atlas Vector Search index for all collections featuring vector embeddings of 2048 dimensions or fewer in width. This applies to diverse data types coexisting with additional data on your Atlas cluster, and the process is executed through the Atlas UI and Atlas Administration AP, refer to [`/docs/providers/mongodb_atlas/setup.md`](/docs/providers/mongodb_atlas/setup.md).\n\n### Running the API locally\n\nTo run the API locally, you first need to set the requisite environment variables with the `export` command:\n\n```\nexport DATASTORE=<your_datastore>\nexport BEARER_TOKEN=<your_bearer_token>\nexport OPENAI_API_KEY=<your_openai_api_key>\n<Add the environment variables for your chosen vector DB here>\n```\n\nStart the API with:\n\n```\npoetry run start\n```\n\nAppend `docs` to the URL shown in the terminal and open it in a browser to access the API documentation and try out the endpoints (i.e. http://0.0.0.0:8000/docs). Make sure to enter your bearer token and test the API endpoints.\n\n**Note:** If you add new dependencies to the pyproject.toml file, you need to run `poetry lock` and `poetry install` to update the lock file and install the new dependencies.\n\n### Personalization\n\nYou can personalize the Retrieval Plugin for your own use case by doing the following:\n\n- **Replace the logo**: Replace the image in [logo.png](/.well-known/logo.png) with your own logo.\n\n- **Edit the data models**: Edit the `DocumentMetadata` and `DocumentMetadataFilter` data models in [models.py](/models/models.py) to add custom metadata fields. Update the OpenAPI schema in [openapi.yaml](/.well-known/openapi.yaml) accordingly. To update the OpenAPI schema more easily, you can run the app locally, then navigate to `http://0.0.0.0:8000/sub/openapi.json` and copy the contents of the webpage. Then go to [Swagger Editor](https://editor.swagger.io/) and paste in the JSON to convert it to a YAML format. You could also replace the [openapi.yaml](/.well-known/openapi.yaml) file with an openapi.json file in the [.well-known](/.well-known) folder.\n\n- **Change the plugin name, description, and usage instructions**: Update the plugin name, user-facing description, and usage instructions for the model. You can either edit the descriptions in the [main.py](/server/main.py) file or update the [openapi.yaml](/.well-known/openapi.yaml) file. Follow the same instructions as in the previous step to update the OpenAPI schema.\n\n- **Enable ChatGPT to save information from conversations**: See the instructions in the [memory example folder](/examples/memory).\n\n### Authentication Methods\n\nYou can choose from four options for authenticating requests to your plugin:\n\n1. **No Authentication**: Anyone can add your plugin and use its API without any credentials. This option is suitable if you are only exposing documents that are not sensitive or already public. It provides no security for your data. If using this method, copy the contents of this [main.py](/examples/authentication-methods/no-auth/main.py) into the [actual main.py file](/server/main.py). Example manifest [here](/examples/authentication-methods/no-auth/ai-plugin.json).\n\n2. **HTTP Bearer**: You can use a secret token as a header to authorize requests to your plugin. There are two variants of this option:\n\n   - **User Level** (default for this implementation): Each user who adds your plugin to ChatGPT must provide the bearer token when adding the plugin. You can generate and distribute these tokens using any tool or method you prefer, such as [jwt.io](https://jwt.io/). This method provides better security as each user has to enter the shared access token. If you require a unique access token for each user, you will need to implement this yourself in the [main.py](/server/main.py) file. Example manifest [here](/examples/authentication-methods/user-http/ai-plugin.json).\n\n   - **Service Level**: Anyone can add your plugin and use its API without credentials, but you must add a bearer token when registering the plugin. When you install your plugin, you need to add your bearer token, and will then receive a token from ChatGPT that you must include in your hosted manifest file. Your token will be used by ChatGPT to authorize requests to your plugin on behalf of all users who add it. This method is more convenient for users, but it may be less secure as all users share the same token and do not need to add a token to install the plugin. Example manifest [here](/examples/authentication-methods/service-http/ai-plugin.json).\n\n3. **OAuth**: Users must go through an OAuth flow to add your plugin. You can use an OAuth provider to authenticate users who add your plugin and grant them access to your API. This method offers the highest level of security and control, as users authenticate through a trusted third-party provider. However, you will need to implement the OAuth flow yourself in the [main.py](/server/main.py) file and provide the necessary parameters in your manifest file. Example manifest [here](/examples/authentication-methods/oauth/ai-plugin.json).\n\nConsider the benefits and drawbacks of each authentication method before choosing the one that best suits your use case and security requirements. If you choose to use a method different to the default (User Level HTTP), make sure to update the manifest file [here](/.well-known/ai-plugin.json).\n\n## Deployment\n\nYou can deploy your app to different cloud providers, depending on your preferences and requirements. However, regardless of the provider you choose, you will need to update two files in your app: [openapi.yaml](/.well-known/openapi.yaml) and [ai-plugin.json](/.well-known/ai-plugin.json). As outlined above, these files define the API specification and the AI plugin configuration for your app, respectively. You need to change the url field in both files to match the address of your deployed app.\n\nRender has a 1-click deploy option that automatically updates the url field in both files:\n\n[<img src=\"https://render.com/images/deploy-to-render-button.svg\" alt=\"Deploy to Render\" />](https://render.com/deploy?repo=https://github.com/render-examples/chatgpt-retrieval-plugin/tree/main)\n\nBefore deploying your app, you might want to remove unused dependencies from your [pyproject.toml](/pyproject.toml) file to reduce the size of your app and improve its performance. Depending on the vector database provider you choose, you can remove the packages that are not needed for your specific provider. Refer to the respective documentation in the [`/docs/deployment/removing-unused-dependencies.md`](/docs/deployment/removing-unused-dependencies.md) file for information on removing unused dependencies for each provider.\n\nInstructions:\n\n- [Deploying to Fly.io](/docs/deployment/flyio.md)\n- [Deploying to Heroku](/docs/deployment/heroku.md)\n- [Deploying to Render](/docs/deployment/render.md)\n- [Other Deployment Options](/docs/deployment/other-options.md) (Azure Container Apps, Google Cloud Run, AWS Elastic Container Service, etc.)\n\nOnce you have deployed your app, consider uploading an initial batch of documents using one of [these scripts](/scripts) or by calling the `/upsert` endpoint.\n\n## Webhooks\n\nTo keep the documents stored in the vector database up-to-date, consider using tools like [Zapier](https://zapier.com) or [Make](https://www.make.com) to configure incoming webhooks to your plugin's API based on events or schedules. For example, this could allow you to sync new information as you update your notes or receive emails. You can also use a [Zapier Transfer](https://zapier.com/blog/zapier-transfer-guide/) to batch process a collection of existing documents and upload them to the vector database.\n\nIf you need to pass custom fields from these tools to your plugin, you might want to create an additional Retrieval Plugin API endpoint that calls the datastore's upsert function, such as `upsert-email`. This custom endpoint can be designed to accept specific fields from the webhook and process them accordingly.\n\nTo set up an incoming webhook, follow these general steps:\n\n- Choose a webhook tool like Zapier or Make and create an account.\n- Set up a new webhook or transfer in the tool, and configure it to trigger based on events or schedules.\n- Specify the target URL for the webhook, which should be the API endpoint of your Retrieval Plugin (e.g. `https://your-plugin-url.com/upsert`).\n- Configure the webhook payload to include the necessary data fields and format them according to your Retrieval Plugin's API requirements.\n- Test the webhook to ensure it's working correctly and sending data to your Retrieval Plugin as expected.\n\nAfter setting up the webhook, you may want to run a backfill to ensure that any previously missed data is included in the vector database.\n\nRemember that if you want to use incoming webhooks to continuously sync data, you should consider running a backfill after setting these up to avoid missing any data.\n\nIn addition to using tools like Zapier and Make, you can also build your own custom integrations to sync data with your Retrieval Plugin. This allows you to have more control over the data flow and tailor the integration to your specific needs and requirements.\n\n## Scripts\n\nThe `scripts` folder contains scripts to batch upsert or process text documents from different data sources, such as a zip file, JSON file, or JSONL file. These scripts use the plugin's upsert utility functions to upload the documents and their metadata to the vector database, after converting them to plain text and splitting them into chunks. Each script folder has a README file that explains how to use it and what parameters it requires. You can also optionally screen the documents for personally identifiable information (PII) using a language model and skip them if detected, with the [`services.pii_detection`](/services/pii_detection.py) module. This can be helpful if you want to avoid uploading sensitive or private documents to the vector database unintentionally. Additionally, you can optionally extract metadata from the document text using a language model, with the [`services.extract_metadata`](/services/extract_metadata.py) module. This can be useful if you want to enrich the document metadata. **Note:** if using incoming webhooks to continuously sync data, consider running a backfill after setting these up to avoid missing any data.\n\nThe scripts are:\n\n- [`process_json`](scripts/process_json/): This script processes a file dump of documents in a JSON format and stores them in the vector database with some metadata. The format of the JSON file should be a list of JSON objects, where each object represents a document. The JSON object should have a `text` field and optionally other fields to populate the metadata. You can provide custom metadata as a JSON string and flags to screen for PII and extract metadata.\n- [`process_jsonl`](scripts/process_jsonl/): This script processes a file dump of documents in a JSONL format and stores them in the vector database with some metadata. The format of the JSONL file should be a newline-delimited JSON file, where each line is a valid JSON object representing a document. The JSON object should have a `text` field and optionally other fields to populate the metadata. You can provide custom metadata as a JSON string and flags to screen for PII and extract metadata.\n- [`process_zip`](scripts/process_zip/): This script processes a file dump of documents in a zip file and stores them in the vector database with some metadata. The format of the zip file should be a flat zip file folder of docx, pdf, txt, md, pptx or csv files. You can provide custom metadata as a JSON string and flags to screen for PII and extract metadata.\n\n## Pull Request (PR) Checklist\n\nIf you'd like to contribute, please follow the checklist below when submitting a PR. This will help us review and merge your changes faster! Thank you for contributing!\n\n1. **Type of PR**: Indicate the type of PR by adding a label in square brackets at the beginning of the title, such as `[Bugfix]`, `[Feature]`, `[Enhancement]`, `[Refactor]`, or `[Documentation]`.\n\n2. **Short Description**: Provide a brief, informative description of the PR that explains the changes made.\n\n3. **Issue(s) Linked**: Mention any related issue(s) by using the keyword `Fixes` or `Closes` followed by the respective issue number(s) (e.g., Fixes #123, Closes #456).\n\n4. **Branch**: Ensure that you have created a new branch for the changes, and it is based on the latest version of the `main` branch.\n\n5. **Code Changes**: Make sure the code changes are minimal, focused, and relevant to the issue or feature being addressed.\n\n6. **Commit Messages**: Write clear and concise commit messages that explain the purpose of each commit.\n\n7. **Tests**: Include unit tests and/or integration tests for any new code or changes to existing code. Make sure all tests pass before submitting the PR.\n\n8. **Documentation**: Update relevant documentation (e.g., README, inline comments, or external documentation) to reflect any changes made.\n\n9. **Review Requested**: Request a review from at least one other contributor or maintainer of the repository.\n\n10. **Video Submission** (For Complex/Large PRs): If your PR introduces significant changes, complexities, or a large number of lines of code, submit a brief video walkthrough along with the PR. The video should explain the purpose of the changes, the logic behind them, and how they address the issue or add the proposed feature. This will help reviewers to better understand your contribution and expedite the review process.\n\n## Pull Request Naming Convention\n\nUse the following naming convention for your PR branches:\n\n```\n<type>/<short-description>-<issue-number>\n```\n\n- `<type>`: The type of PR, such as `bugfix`, `feature`, `enhancement`, `refactor`, or `docs`. Multiple types are ok and should appear as <type>, <type2>\n- `<short-description>`: A brief description of the changes made, using hyphens to separate words.\n- `<issue-number>`: The issue number associated with the changes made (if applicable).\n\nExample:\n\n```\nfeature/advanced-chunking-strategy-123\n```\n\n## Limitations\n\nWhile the ChatGPT Retrieval Plugin is designed to provide a flexible solution for semantic search and retrieval, it does have some limitations:\n\n- **Keyword search limitations**: The embeddings generated by the chosen OpenAI embeddings model may not always be effective at capturing exact keyword matches. As a result, the plugin might not return the most relevant results for queries that rely heavily on specific keywords. Some vector databases, like Elasticsearch, Pinecone, Weaviate and Azure Cognitive Search, use hybrid search and might perform better for keyword searches.\n- **Sensitive data handling**: The plugin does not automatically detect or filter sensitive data. It is the responsibility of the developers to ensure that they have the necessary authorization to include content in the Retrieval Plugin and that the content complies with data privacy requirements.\n- **Scalability**: The performance of the plugin may vary depending on the chosen vector database provider and the size of the dataset. Some providers may offer better scalability and performance than others.\n- **Metadata extraction**: The optional metadata extraction feature relies on a language model to extract information from the document text. This process may not always be accurate, and the quality of the extracted metadata may vary depending on the document content and structure.\n- **PII detection**: The optional PII detection feature is not foolproof and may not catch all instances of personally identifiable information. Use this feature with caution and verify its effectiveness for your specific use case.\n\n## Future Directions\n\nThe ChatGPT Retrieval Plugin provides a flexible solution for semantic search and retrieval, but there is always potential for further development. We encourage users to contribute to the project by submitting pull requests for new features or enhancements. Notable contributions may be acknowledged with OpenAI credits.\n\nSome ideas for future directions include:\n\n- **More vector database providers**: If you are interested in integrating another vector database provider with the ChatGPT Retrieval Plugin, feel free to submit an implementation.\n- **Additional scripts**: Expanding the range of scripts available for processing and uploading documents from various data sources would make the plugin even more versatile.\n- **User Interface**: Developing a user interface for managing documents and interacting with the plugin could improve the user experience.\n- **Hybrid search / TF-IDF option**: Enhancing the [datastore's upsert function](/datastore/datastore.py#L18) with an option to use hybrid search or TF-IDF indexing could improve the plugin's performance for keyword-based queries.\n- **Advanced chunking strategies and embeddings calculations**: Implementing more sophisticated chunking strategies and embeddings calculations, such as embedding document titles and summaries, performing weighted averaging of document chunks and summaries, or calculating the average embedding for a document, could lead to better search results.\n- **Custom metadata**: Allowing users to add custom metadata to document chunks, such as titles or other relevant information, might improve the retrieved results in some use cases.\n- **Additional optional services**: Integrating more optional services, such as summarizing documents or pre-processing documents before embedding them, could enhance the plugin's functionality and quality of retrieved results. These services could be implemented using language models and integrated directly into the plugin, rather than just being available in the scripts.\n\nWe welcome contributions from the community to help improve the ChatGPT Retrieval Plugin and expand its capabilities. If you have an idea or feature you'd like to contribute, please submit a pull request to the repository.\n\n## Contributors\n\nWe would like to extend our gratitude to the following contributors for their code / documentation contributions, and support in integrating various vector database providers with the ChatGPT Retrieval Plugin:\n\n- [Pinecone](https://www.pinecone.io/)\n  - [acatav](https://github.com/acatav)\n  - [gkogan](https://github.com/gkogan)\n  - [jamescalam](https://github.com/jamescalam)\n- [Weaviate](https://www.semi.technology/)\n  - [byronvoorbach](https://github.com/byronvoorbach)\n  - [hsm207](https://github.com/hsm207)\n  - [sebawita](https://github.com/sebawita)\n- [Zilliz](https://zilliz.com/)\n  - [filip-halt](https://github.com/filip-halt)\n- [Milvus](https://milvus.io/)\n  - [filip-halt](https://github.com/filip-halt)\n- [Qdrant](https://qdrant.tech/)\n  - [kacperlukawski](https://github.com/kacperlukawski)\n- [Redis](https://redis.io/)\n  - [spartee](https://github.com/spartee)\n  - [tylerhutcherson](https://github.com/tylerhutcherson)\n- [LlamaIndex](https://github.com/jerryjliu/llama_index)\n  - [jerryjliu](https://github.com/jerryjliu)\n  - [Disiok](https://github.com/Disiok)\n- [Supabase](https://supabase.com/)\n  - [egor-romanov](https://github.com/egor-romanov)\n- [Postgres](https://www.postgresql.org/)\n  - [egor-romanov](https://github.com/egor-romanov)\n  - [mmmaia](https://github.com/mmmaia)\n- [Elasticsearch](https://www.elastic.co/)\n  - [joemcelroy](https://github.com/joemcelroy)\n",
        "releases": []
    }
}