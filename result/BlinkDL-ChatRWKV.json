{
    "https://api.github.com/repos/BlinkDL/ChatRWKV": {
        "forks": 704,
        "watchers": 9454,
        "stars": 9454,
        "languages": {
            "Python": 304192,
            "Cuda": 16750,
            "C++": 14776
        },
        "commits": [
            "2024-12-07T05:00:14Z",
            "2024-12-06T19:43:45Z",
            "2024-07-11T05:19:11Z",
            "2024-07-11T05:16:36Z",
            "2024-07-03T16:13:52Z",
            "2024-07-02T18:07:54Z",
            "2024-07-01T03:28:36Z",
            "2024-07-01T03:23:51Z",
            "2024-07-01T03:08:13Z",
            "2024-05-12T16:32:29Z",
            "2024-05-12T13:17:53Z",
            "2024-05-11T13:07:54Z",
            "2024-05-10T18:45:44Z",
            "2024-05-10T16:55:27Z",
            "2024-05-10T16:06:25Z",
            "2024-05-10T15:19:40Z",
            "2024-05-09T21:45:44Z",
            "2024-05-09T21:31:13Z",
            "2024-05-09T20:45:17Z",
            "2024-05-09T13:01:55Z",
            "2024-04-26T20:03:17Z",
            "2024-04-16T19:20:18Z",
            "2024-04-16T19:19:18Z",
            "2024-02-17T15:39:08Z",
            "2024-02-15T01:20:59Z",
            "2024-02-10T12:23:28Z",
            "2024-02-09T10:31:22Z",
            "2024-02-08T20:18:31Z",
            "2024-02-07T06:44:35Z",
            "2024-02-03T16:51:30Z"
        ],
        "creation_date": "2023-01-13T08:07:40Z",
        "contributors": 17,
        "topics": [
            "chatbot",
            "chatgpt",
            "language-model",
            "pytorch",
            "rnn",
            "rwkv"
        ],
        "subscribers": 93,
        "readme": "# ChatRWKV (pronounced as \"RwaKuv\" (r\u028ckuv in IPA), from 4 major params: R W K V)\n\nRWKV homepage: https://www.rwkv.com\n\n## Please check https://github.com/BlinkDL/ChatRWKV/blob/main/API_DEMO_CHAT.py first\n\nChatRWKV is like ChatGPT but powered by my RWKV (100% RNN) language model, which is the only RNN (as of now) that can match transformers in quality and scaling, while being faster and saves VRAM. Training sponsored by Stability EleutherAI :)\n\nOur latest version is **RWKV-6** https://arxiv.org/abs/2404.05892 (Preview models: https://huggingface.co/BlinkDL/temp )\n\n**RWKV-6 3B** Demo: https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-1\n\n**RWKV-6 7B** Demo: https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2\n\n![RWKV-v5-benchmark-1](RWKV-v5-benchmark-1.png)\n\n**RWKV-LM main repo**: https://github.com/BlinkDL/RWKV-LM (explanation, fine-tuning, training, etc.)\n\nChat Demo for developers: https://github.com/BlinkDL/ChatRWKV/blob/main/API_DEMO_CHAT.py\n\n## RWKV Discord: https://discord.gg/bDSBUMeFpc (7k+ members)\n\n**Twitter**: https://twitter.com/BlinkDL_AI\n\n**Homepage**: https://www.rwkv.com/\n\n**Raw cutting-edge RWKV weights:** https://huggingface.co/BlinkDL\n\n**HF-compatible RWKV weights:** https://huggingface.co/RWKV\n\nUse v2/convert_model.py to convert a model for a strategy, for faster loading & saves CPU RAM.\n\nNote RWKV_CUDA_ON will build a CUDA kernel (much faster & saves VRAM). Here is how to build it (\"pip install ninja\" first):\n```\n# How to build in Linux: set these and run v2/chat.py\nexport PATH=/usr/local/cuda/bin:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n# How to build in win:\nInstall VS2022 build tools (https://aka.ms/vs/17/release/vs_BuildTools.exe select Desktop C++). Reinstall CUDA 11.7 (install VC++ extensions). Run v2/chat.py in \"x64 native tools command prompt\". \n```\n**RWKV pip package**: https://pypi.org/project/rwkv/ **(please always check for latest version and upgrade)**\n\nhttps://github.com/cgisky1980/ai00_rwkv_server Fastest GPU inference API with vulkan (good for nvidia/amd/intel)\n\nhttps://github.com/cryscan/web-rwkv backend for ai00_rwkv_server\n\nhttps://github.com/saharNooby/rwkv.cpp Fast CPU/cuBLAS/CLBlast inference: int4/int8/fp16/fp32\n\nhttps://github.com/JL-er/RWKV-PEFT lora/pissa/Qlora/Qpissa/state tuning\n\nhttps://github.com/RWKV/RWKV-infctx-trainer Infctx trainer\n\n**World demo script:** https://github.com/BlinkDL/ChatRWKV/blob/main/API_DEMO_WORLD.py\n\n**Raven Q&A demo script:** https://github.com/BlinkDL/ChatRWKV/blob/main/v2/benchmark_more.py\n\n![ChatRWKV-strategy](ChatRWKV-strategy.png)\n\n**RWKV in 150 lines** (model, inference, text generation): https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py\n\n**\ud83d\udd25 RWKV v5 in 250 lines \ud83d\udd25** (with tokenizer too): https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_v5_demo.py\n\n**\ud83d\udd25 Building your own RWKV inference engine \ud83d\udd25**: begin with https://github.com/BlinkDL/ChatRWKV/blob/main/src/model_run.py which is easier to understand (used by https://github.com/BlinkDL/ChatRWKV/blob/main/chat.py).\n\n**RWKV preprint** https://arxiv.org/abs/2305.13048\n\n![RWKV-paper](RWKV-paper.png)\n\nRWKV v6 illustrated:\n\n![RWKV-v6](rwkv-x060.png)\n\n**Cool Community RWKV Projects**:\n\nhttps://github.com/saharNooby/rwkv.cpp fast i4 i8 fp16 fp32 CPU inference using [ggml](https://github.com/ggerganov/ggml)\n\nhttps://github.com/harrisonvanderbyl/rwkv-cpp-cuda fast windows/linux & cuda/rocm/vulkan GPU inference (no need for python & pytorch)\n\nhttps://github.com/Blealtan/RWKV-LM-LoRA LoRA fine-tuning\n\nhttps://github.com/josStorer/RWKV-Runner cool GUI\n\nMore RWKV projects: https://github.com/search?o=desc&q=rwkv&s=updated&type=Repositories\n\nChatRWKV v2: with \"stream\" and \"split\" strategies, and INT8. 3G VRAM is enough to run RWKV 14B :) https://github.com/BlinkDL/ChatRWKV/tree/main/v2\n```python\nos.environ[\"RWKV_JIT_ON\"] = '1'\nos.environ[\"RWKV_CUDA_ON\"] = '0' # if '1' then use CUDA kernel for seq mode (much faster)\nfrom rwkv.model import RWKV                         # pip install rwkv\nmodel = RWKV(model='/fsx/BlinkDL/HF-MODEL/rwkv-4-pile-1b5/RWKV-4-Pile-1B5-20220903-8040', strategy='cuda fp16')\n\nout, state = model.forward([187, 510, 1563, 310, 247], None)   # use 20B_tokenizer.json\nprint(out.detach().cpu().numpy())                   # get logits\nout, state = model.forward([187, 510], None)\nout, state = model.forward([1563], state)           # RNN has state (use deepcopy if you want to clone it)\nout, state = model.forward([310, 247], state)\nprint(out.detach().cpu().numpy())                   # same result as above\n```\n![RWKV-eval](RWKV-eval.png)\n\nHere is https://huggingface.co/BlinkDL/rwkv-4-raven/blob/main/RWKV-4-Raven-14B-v7-Eng-20230404-ctx4096.pth in action:\n![ChatRWKV](ChatRWKV.png)\n\nWhen you build a RWKV chatbot, always check the text corresponding to the state, in order to prevent bugs.\n\n1. Never call raw forward() directly. Instead, put it in a function that will record the text corresponding to the state.\n\n**(For v4-raven models, use Bob/Alice. For v4/v5/v6-world models, use User/Assistant)**\n\n2. The best chat format (check whether your text is of this format):\n```Bob: xxxxxxxxxxxxxxxxxx\\n\\nAlice: xxxxxxxxxxxxx\\n\\nBob: xxxxxxxxxxxxxxxx\\n\\nAlice:```\n\n* There should not be any space after the final \"Alice:\". The generation result will have a space in the beginning, and you can simply strip it.\n* You can use \\n in xxxxx, but avoid \\n\\n. So simply do ```xxxxx = xxxxx.strip().replace('\\r\\n','\\n').replace('\\n\\n','\\n')```\n\nIf you are building your own RWKV inference engine, begin with https://github.com/BlinkDL/ChatRWKV/blob/main/src/model_run.py which is easier to understand (used by https://github.com/BlinkDL/ChatRWKV/blob/main/chat.py)\n\nThe lastest \"Raven\"-series Alpaca-style-tuned RWKV 14B & 7B models are very good (almost ChatGPT-like, good at multiround chat too). Download: https://huggingface.co/BlinkDL/rwkv-4-raven\n\nPrevious old model results:\n![ChatRWKV](misc/sample-1.png)\n![ChatRWKV](misc/sample-2.png)\n![ChatRWKV](misc/sample-3.png)\n![ChatRWKV](misc/sample-4.png)\n![ChatRWKV](misc/sample-5.png)\n![ChatRWKV](misc/sample-6.png)\n![ChatRWKV](misc/sample-7.png)\n\n## \u4e2d\u6587\u6a21\u578b\n\nQQ\u7fa4 553456870\uff08\u52a0\u5165\u65f6\u8bf7\u7b80\u5355\u81ea\u6211\u4ecb\u7ecd\uff09\u3002\u6709\u7814\u53d1\u80fd\u529b\u7684\u670b\u53cb\u52a0\u7fa4 325154699\u3002\n\n\u4e2d\u6587\u4f7f\u7528\u6559\u7a0b\uff1ahttps://zhuanlan.zhihu.com/p/618011122 https://zhuanlan.zhihu.com/p/616351661\n\n\u63a8\u8350UI\uff1ahttps://github.com/l15y/wenda\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=BlinkDL/ChatRWKV&type=Date)](https://star-history.com/#BlinkDL/ChatRWKV&Date)\n",
        "releases": []
    }
}