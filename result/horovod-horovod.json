{
    "https://api.github.com/repos/horovod/horovod": {
        "forks": 2246,
        "watchers": 14340,
        "stars": 14340,
        "languages": {
            "Python": 2427426,
            "C++": 1023283,
            "CMake": 81422,
            "Shell": 27789,
            "Cuda": 27469,
            "Dockerfile": 20907,
            "C": 1420,
            "Mustache": 1045
        },
        "commits": [
            "2024-08-31T11:55:45Z",
            "2024-07-12T13:02:43Z",
            "2024-07-12T13:00:31Z",
            "2024-07-12T09:19:13Z",
            "2024-07-11T20:07:31Z",
            "2024-07-10T14:19:23Z",
            "2024-07-10T07:54:05Z",
            "2024-03-25T12:37:43Z",
            "2024-01-05T20:03:59Z",
            "2023-09-18T08:44:36Z",
            "2023-09-14T12:13:12Z",
            "2023-09-14T06:47:24Z",
            "2023-09-12T06:39:23Z",
            "2023-09-11T17:29:50Z",
            "2023-09-06T17:10:08Z",
            "2023-06-26T06:46:56Z",
            "2023-06-12T09:21:57Z",
            "2023-06-06T21:02:18Z",
            "2023-06-06T14:38:50Z",
            "2023-06-01T13:51:32Z",
            "2023-05-24T16:52:39Z",
            "2023-05-10T09:18:29Z",
            "2023-05-09T08:23:09Z",
            "2023-05-09T08:14:14Z",
            "2023-05-05T13:07:41Z",
            "2023-05-04T07:51:03Z",
            "2023-04-28T20:51:22Z",
            "2023-04-27T21:53:39Z",
            "2023-04-27T20:16:58Z",
            "2023-04-25T07:43:33Z"
        ],
        "creation_date": "2017-08-09T19:39:59Z",
        "contributors": 30,
        "topics": [
            "baidu",
            "deep-learning",
            "deeplearning",
            "keras",
            "machine-learning",
            "machinelearning",
            "mpi",
            "mxnet",
            "pytorch",
            "ray",
            "spark",
            "tensorflow",
            "uber"
        ],
        "subscribers": 334,
        "readme": ".. raw:: html\n\n    <p align=\"center\"><img src=\"https://user-images.githubusercontent.com/16640218/34506318-84d0c06c-efe0-11e7-8831-0425772ed8f2.png\" alt=\"Logo\" width=\"200\"/></p>\n    <br/>\n\nHorovod\n=======\n\n.. raw:: html\n\n   <div align=\"center\">\n\n.. image:: https://badge.fury.io/py/horovod.svg\n   :target: https://badge.fury.io/py/horovod\n   :alt: PyPI Version\n\n.. image:: https://badge.buildkite.com/6f976bc161c69d9960fc00de01b69deb6199b25680a09e5e26.svg?branch=master\n   :target: https://buildkite.com/horovod/horovod\n   :alt: Build Status\n\n.. image:: https://readthedocs.org/projects/horovod/badge/?version=latest\n   :target: https://horovod.readthedocs.io/en/latest/\n   :alt: Documentation Status\n\n.. image:: https://img.shields.io/badge/slack-chat-green.svg?logo=slack\n   :target: https://forms.gle/cPGvty5hp31tGfg79\n   :alt: Slack\n\n.. raw:: html\n\n   </div>\n\n.. raw:: html\n\n   <div align=\"center\">\n\n.. image:: https://img.shields.io/badge/License-Apache%202.0-blue.svg\n   :target: https://img.shields.io/badge/License-Apache%202.0-blue.svg\n   :alt: License\n\n.. image:: https://app.fossa.com/api/projects/git%2Bgithub.com%2Fhorovod%2Fhorovod.svg?type=shield\n   :target: https://app.fossa.com/projects/git%2Bgithub.com%2Fhorovod%2Fhorovod?ref=badge_shield\n   :alt: FOSSA Status\n\n.. image:: https://bestpractices.coreinfrastructure.org/projects/2373/badge\n   :target: https://bestpractices.coreinfrastructure.org/projects/2373\n   :alt: CII Best Practices\n\n.. image:: https://pepy.tech/badge/horovod\n   :target: https://pepy.tech/project/horovod\n   :alt: Downloads\n\n.. raw:: html\n\n   </div>\n\n.. inclusion-marker-start-do-not-remove\n\n|\n\nHorovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.\nThe goal of Horovod is to make distributed deep learning fast and easy to use.\n\n\n.. raw:: html\n\n   <p><img src=\"https://raw.githubusercontent.com/lfai/artwork/master/lfaidata-assets/lfaidata-project-badge/graduate/color/lfaidata-project-badge-graduate-color.png\" alt=\"LF AI & Data\" width=\"200\"/></p>\n\n\nHorovod is hosted by the `LF AI & Data Foundation <https://lfdl.io>`_ (LF AI & Data). If you are a company that is deeply\ncommitted to using open source technologies in artificial intelligence, machine, and deep learning, and want to support\nthe communities of open source projects in these domains, consider joining the LF AI & Data Foundation. For details\nabout who's involved and how Horovod plays a role, read the Linux Foundation `announcement <https://lfdl.io/press/2018/12/13/lf-deep-learning-welcomes-horovod-distributed-training-framework-as-newest-project/>`_.\n\n|\n\n.. contents::\n\n|\n\nDocumentation\n-------------\n\n- `Latest Release <https://horovod.readthedocs.io/en/stable>`_\n- `master <https://horovod.readthedocs.io/en/latest>`_\n\n|\n\nWhy Horovod?\n------------\nThe primary motivation for this project is to make it easy to take a single-GPU training script and successfully scale\nit to train across many GPUs in parallel. This has two aspects:\n\n1. How much modification does one have to make to a program to make it distributed, and how easy is it to run it?\n2. How much faster would it run in distributed mode?\n\nInternally at Uber we found the MPI model to be much more straightforward and require far less code changes than previous\nsolutions such as Distributed TensorFlow with parameter servers. Once a training script has been written for scale with\nHorovod, it can run on a single-GPU, multiple-GPUs, or even multiple hosts without any further code changes.\nSee the `Usage <#usage>`__ section for more details.\n\nIn addition to being easy to use, Horovod is fast. Below is a chart representing the benchmark that was done on 128\nservers with 4 Pascal GPUs each connected by RoCE-capable 25 Gbit/s network:\n\n.. image:: https://user-images.githubusercontent.com/16640218/38965607-bf5c46ca-4332-11e8-895a-b9c137e86013.png\n   :alt: 512-GPU Benchmark\n\nHorovod achieves 90% scaling efficiency for both Inception V3 and ResNet-101, and 68% scaling efficiency for VGG-16.\nSee `Benchmarks <docs/benchmarks.rst>`_ to find out how to reproduce these numbers.\n\nWhile installing MPI and NCCL itself may seem like an extra hassle, it only needs to be done once by the team dealing\nwith infrastructure, while everyone else in the company who builds the models can enjoy the simplicity of training them at\nscale.\n\n\nInstall\n-------\nTo install Horovod on Linux or macOS:\n\n1. Install `CMake <https://cmake.org/install/>`__\n\n.. raw:: html\n\n    <p/>\n\n2. If you've installed TensorFlow from `PyPI <https://pypi.org/project/tensorflow>`__, make sure that ``g++-5`` or above is installed.\n   Starting with TensorFlow 2.10 a C++17-compliant compiler like ``g++8`` or above will be required.\n\n   If you've installed PyTorch from `PyPI <https://pypi.org/project/torch>`__, make sure that ``g++-5`` or above is installed.\n\n   If you've installed either package from `Conda <https://conda.io>`_, make sure that the ``gxx_linux-64`` Conda package is installed.\n\n.. raw:: html\n\n    <p/>\n\n3. Install the ``horovod`` pip package.\n\n   To run on CPUs:\n\n   .. code-block:: bash\n\n      $ pip install horovod\n\n   To run on GPUs with NCCL:\n\n   .. code-block:: bash\n\n      $ HOROVOD_GPU_OPERATIONS=NCCL pip install horovod\n\nFor more details on installing Horovod with GPU support, read `Horovod on GPU <docs/gpus.rst>`_.\n\nFor the full list of Horovod installation options, read the `Installation Guide <docs/install.rst>`_.\n\nIf you want to use MPI, read `Horovod with MPI <docs/mpi.rst>`_.\n\nIf you want to use Conda, read `Building a Conda environment with GPU support for Horovod <docs/conda.rst>`_.\n\nIf you want to use Docker, read `Horovod in Docker <docs/docker.rst>`_.\n\nTo compile Horovod from source, follow the instructions in the `Contributor Guide <docs/contributors.rst>`_.\n\n\nConcepts\n--------\nHorovod core principles are based on `MPI <http://mpi-forum.org/>`_ concepts such as *size*, *rank*,\n*local rank*, **allreduce**, **allgather**, **broadcast**, and **alltoall**. See `this page <docs/concepts.rst>`_\nfor more details.\n\nSupported frameworks\n--------------------\nSee these pages for Horovod examples and best practices:\n\n- `Horovod with TensorFlow <docs/tensorflow.rst>`_\n- `Horovod with XLA in Tensorflow <xla.rst>`_\n- `Horovod with Keras <docs/keras.rst>`_\n- `Horovod with PyTorch <docs/pytorch.rst>`_\n- `Horovod with MXNet <docs/mxnet.rst>`_\n\nUsage\n-----\n\nTo use Horovod, make the following additions to your program:\n\n1. Run ``hvd.init()`` to initialize Horovod.\n\n.. raw:: html\n\n    <p/>\n\n2. Pin each GPU to a single process to avoid resource contention.\n\n   With the typical setup of one GPU per process, set this to *local rank*. The first process on\n   the server will be allocated the first GPU, the second process will be allocated the second GPU, and so forth.\n\n.. raw:: html\n\n    <p/>\n\n\n3. Scale the learning rate by the number of workers.\n\n   Effective batch size in synchronous distributed training is scaled by the number of workers.\n   An increase in learning rate compensates for the increased batch size.\n\n.. raw:: html\n\n    <p/>\n\n\n4. Wrap the optimizer in ``hvd.DistributedOptimizer``.\n\n   The distributed optimizer delegates gradient computation to the original optimizer, averages gradients using **allreduce** or **allgather**, and then applies those averaged gradients.\n\n.. raw:: html\n\n    <p/>\n\n\n5. Broadcast the initial variable states from rank 0 to all other processes.\n\n   This is necessary to ensure consistent initialization of all workers when training is started with random weights or restored from a checkpoint.\n\n.. raw:: html\n\n    <p/>\n\n\n6. Modify your code to save checkpoints only on worker 0 to prevent other workers from corrupting them.\n\n.. raw:: html\n\n    <p/>\n\n\nExample using TensorFlow v1 (see the `examples <https://github.com/horovod/horovod/blob/master/examples/>`_ directory for full training examples):\n\n.. code-block:: python\n\n    import tensorflow as tf\n    import horovod.tensorflow as hvd\n\n\n    # Initialize Horovod\n    hvd.init()\n\n    # Pin GPU to be used to process local rank (one GPU per process)\n    config = tf.ConfigProto()\n    config.gpu_options.visible_device_list = str(hvd.local_rank())\n\n    # Build model...\n    loss = ...\n    opt = tf.train.AdagradOptimizer(0.01 * hvd.size())\n\n    # Add Horovod Distributed Optimizer\n    opt = hvd.DistributedOptimizer(opt)\n\n    # Add hook to broadcast variables from rank 0 to all other processes during\n    # initialization.\n    hooks = [hvd.BroadcastGlobalVariablesHook(0)]\n\n    # Make training operation\n    train_op = opt.minimize(loss)\n\n    # Save checkpoints only on worker 0 to prevent other workers from corrupting them.\n    checkpoint_dir = '/tmp/train_logs' if hvd.rank() == 0 else None\n\n    # The MonitoredTrainingSession takes care of session initialization,\n    # restoring from a checkpoint, saving to a checkpoint, and closing when done\n    # or an error occurs.\n    with tf.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir,\n                                           config=config,\n                                           hooks=hooks) as mon_sess:\n      while not mon_sess.should_stop():\n        # Perform synchronous training.\n        mon_sess.run(train_op)\n\n\nRunning Horovod\n---------------\nThe example commands below show how to run distributed training.\nSee `Run Horovod <docs/running.rst>`_ for more details, including RoCE/InfiniBand tweaks and tips for dealing with hangs.\n\n1. To run on a machine with 4 GPUs:\n\n   .. code-block:: bash\n\n        $ horovodrun -np 4 -H localhost:4 python train.py\n\n2. To run on 4 machines with 4 GPUs each:\n\n   .. code-block:: bash\n\n       $ horovodrun -np 16 -H server1:4,server2:4,server3:4,server4:4 python train.py\n\n3. To run using Open MPI without the ``horovodrun`` wrapper, see `Running Horovod with Open MPI <docs/mpi.rst>`_.\n\n4. To run in Docker, see `Horovod in Docker <docs/docker.rst>`_.\n\n5. To run on Kubernetes, see `Helm Chart <https://github.com/horovod/horovod/tree/master/docker/helm/>`_, `Kubeflow MPI Operator <https://github.com/kubeflow/mpi-operator/>`_, `FfDL <https://github.com/IBM/FfDL/tree/master/etc/examples/horovod/>`_, and `Polyaxon <https://docs.polyaxon.com/integrations/horovod/>`_.\n\n6. To run on Spark, see `Horovod on Spark <docs/spark.rst>`_.\n\n7. To run on Ray, see `Horovod on Ray <docs/ray.rst>`_.\n\n8. To run in Singularity, see `Singularity <https://github.com/sylabs/examples/tree/master/machinelearning/horovod>`_.\n\n9. To run in a LSF HPC cluster (e.g. Summit), see `LSF <docs/lsf.rst>`_.\n\n10. To run on Hadoop Yarn, see `TonY <https://github.com/linkedin/TonY/>`_.\n\nGloo\n----\n`Gloo <https://github.com/facebookincubator/gloo>`_ is an open source collective communications library developed by Facebook.\n\nGloo comes included with Horovod, and allows users to run Horovod without requiring MPI to be installed.\n\nFor environments that have support both MPI and Gloo, you can choose to use Gloo at runtime by passing the ``--gloo`` argument to ``horovodrun``:\n\n.. code-block:: bash\n\n     $ horovodrun --gloo -np 2 python train.py\n\nmpi4py\n------\nHorovod supports mixing and matching Horovod collectives with other MPI libraries, such as `mpi4py <https://mpi4py.scipy.org>`_,\nprovided that the MPI was built with multi-threading support.\n\nYou can check for MPI multi-threading support by querying the ``hvd.mpi_threads_supported()`` function.\n\n.. code-block:: python\n\n    import horovod.tensorflow as hvd\n\n    # Initialize Horovod\n    hvd.init()\n\n    # Verify that MPI multi-threading is supported.\n    assert hvd.mpi_threads_supported()\n\n    from mpi4py import MPI\n    assert hvd.size() == MPI.COMM_WORLD.Get_size()\n\nYou can also initialize Horovod with an `mpi4py` sub-communicator, in which case each sub-communicator\nwill run an independent Horovod training.\n\n.. code-block:: python\n\n    from mpi4py import MPI\n    import horovod.tensorflow as hvd\n\n    # Split COMM_WORLD into subcommunicators\n    subcomm = MPI.COMM_WORLD.Split(color=MPI.COMM_WORLD.rank % 2,\n                                   key=MPI.COMM_WORLD.rank)\n\n    # Initialize Horovod\n    hvd.init(comm=subcomm)\n\n    print('COMM_WORLD rank: %d, Horovod rank: %d' % (MPI.COMM_WORLD.rank, hvd.rank()))\n\n\nInference\n---------\nLearn how to optimize your model for inference and remove Horovod operations from the graph `here <docs/inference.rst>`_.\n\n\nTensor Fusion\n-------------\nOne of the unique things about Horovod is its ability to interleave communication and computation coupled with the ability\nto batch small **allreduce** operations, which results in improved performance. We call this batching feature Tensor Fusion.\n\nSee `here <docs/tensor-fusion.rst>`__ for full details and tweaking instructions.\n\n\nHorovod Timeline\n----------------\nHorovod has the ability to record the timeline of its activity, called Horovod Timeline.\n\n.. image:: https://user-images.githubusercontent.com/16640218/29735271-9e148da0-89ac-11e7-9ae0-11d7a099ac89.png\n   :alt: Horovod Timeline\n\nUse Horovod timeline to analyze Horovod performance.\nSee `here <docs/timeline.rst>`__ for full details and usage instructions.\n\n\nAutomated Performance Tuning\n----------------------------\nSelecting the right values to efficiently make use of Tensor Fusion and other advanced Horovod features can involve\na good amount of trial and error. We provide a system to automate this performance optimization process called\n**autotuning**, which you can enable with a single command line argument to ``horovodrun``.\n\nSee `here <docs/autotune.rst>`__ for full details and usage instructions.\n\n\nHorovod Process Sets\n--------------------\nHorovod allows you to concurrently run distinct collective operations in different groups of processes taking part in\none distributed training. Set up ``hvd.process_set`` objects to make use of this capability.\n\nSee `Process Sets <docs/process_set.rst>`__ for detailed instructions.\n\n\nGuides\n------\n1. Run distributed training in Microsoft Azure using `Batch AI and Horovod <https://github.com/Azure/BatchAI/tree/master/recipes/Horovod>`_.\n2. `Distributed model training using Horovod <https://spell.ml/blog/distributed-model-training-using-horovod-XvqEGRUAACgAa5th>`_.\n\nSend us links to any user guides you want to publish on this site\n\nTroubleshooting\n---------------\nSee `Troubleshooting <docs/troubleshooting.rst>`_ and submit a `ticket <https://github.com/horovod/horovod/issues/new>`_\nif you can't find an answer.\n\n\nCitation\n--------\nPlease cite Horovod in your publications if it helps your research:\n\n::\n\n    @article{sergeev2018horovod,\n      Author = {Alexander Sergeev and Mike Del Balso},\n      Journal = {arXiv preprint arXiv:1802.05799},\n      Title = {Horovod: fast and easy distributed deep learning in {TensorFlow}},\n      Year = {2018}\n    }\n\n\nPublications\n------------\n1. Sergeev, A., Del Balso, M. (2017) *Meet Horovod: Uber\u2019s Open Source Distributed Deep Learning Framework for TensorFlow*.\nRetrieved from `https://eng.uber.com/horovod/ <https://eng.uber.com/horovod/>`_\n\n2. Sergeev, A. (2017) *Horovod - Distributed TensorFlow Made Easy*. Retrieved from\n`https://www.slideshare.net/AlexanderSergeev4/horovod-distributed-tensorflow-made-easy <https://www.slideshare.net/AlexanderSergeev4/horovod-distributed-tensorflow-made-easy>`_\n\n3. Sergeev, A., Del Balso, M. (2018) *Horovod: fast and easy distributed deep learning in TensorFlow*. Retrieved from\n`arXiv:1802.05799 <https://arxiv.org/abs/1802.05799>`_\n\n\nReferences\n----------\nThe Horovod source code was based off the Baidu `tensorflow-allreduce <https://github.com/baidu-research/tensorflow-allreduce>`_\nrepository written by Andrew Gibiansky and Joel Hestness. Their original work is described in the article\n`Bringing HPC Techniques to Deep Learning <http://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/>`_.\n\nGetting Involved\n----------------\n- `Community Slack <https://forms.gle/cPGvty5hp31tGfg79>`_ for collaboration and discussion\n- `Horovod Announce <https://lists.lfai.foundation/g/horovod-announce>`_ for updates on the project\n- `Horovod Technical-Discuss <https://lists.lfai.foundation/g/horovod-technical-discuss>`_ for public discussion\n- `Horovod Security <https://lists.lfai.foundation/g/horovod-security>`_ to report security vulnerabilities\n\n\n.. inclusion-marker-end-do-not-remove\n   Place contents above here if they should also appear in read-the-docs.\n   Contents below are already part of the read-the-docs table of contents.\n",
        "releases": [
            {
                "name": "v0.28.1: Build fixes (ROCm, GCC 12) ",
                "date": "2023-06-12T09:26:22Z"
            },
            {
                "name": "v0.28.0: Keras 2.11+ optimizers, faster reducescatter, fixes for latest TensorFlow, CUDA, NCCL",
                "date": "2023-05-10T09:13:23Z"
            },
            {
                "name": "Custom data loaders in Spark TorchEstimator, more model parallelism in Keras, improved allgather performance, fixes for latest PyTorch and TensorFlow versions",
                "date": "2023-02-01T17:51:19Z"
            },
            {
                "name": "Hotfix: Fixing packaging import during install",
                "date": "2022-10-14T08:20:56Z"
            },
            {
                "name": "Better support for model parallel, more reduction operations for allreduce (min, max, product), grouped allgather and reducedscatter, Petastorm reader level parallel shuffling, NVTabular data loader",
                "date": "2022-10-13T12:29:50Z"
            },
            {
                "name": "Reducescatter for NCCL, MPI and Gloo, AMD GPU XLA Op implementation, Spark Estimator improvements, TensorFlow Data Service Horovod job, Elastic run API",
                "date": "2022-06-21T09:19:16Z"
            },
            {
                "name": "Hotfix: DBFSLocalStore get_localized_path implementation",
                "date": "2022-04-21T08:28:34Z"
            },
            {
                "name": "Hotfix: Fix ignored cuda arch flags",
                "date": "2022-03-10T18:38:34Z"
            },
            {
                "name": "Hotfix: CMake better finding CUDA",
                "date": "2022-03-03T20:39:53Z"
            },
            {
                "name": "Elastic mode improvements, MXNet async dependency engine, fixes for latest PyTorch and TensorFlow versions",
                "date": "2022-03-02T15:57:12Z"
            },
            {
                "name": "Process sets, XLA support, improved GPU backend",
                "date": "2021-10-06T17:52:54Z"
            },
            {
                "name": "Remote filesystem support, estimator fixes",
                "date": "2021-06-10T15:57:35Z"
            },
            {
                "name": "PyTorch Lightning Estimator, Nsight profiling, PyTorch 1.9 support",
                "date": "2021-05-19T15:17:41Z"
            },
            {
                "name": "Local Gradient Aggregation, Grouped Allreduce",
                "date": "2020-11-23T23:57:42Z"
            },
            {
                "name": "Elastic Horovod on Ray",
                "date": "2020-10-01T14:56:31Z"
            },
            {
                "name": "Hotfix: build without MXNet installed",
                "date": "2020-09-26T02:58:05Z"
            },
            {
                "name": "Bugfixes, Databricks Runtime support for Estimators, ElasticSampler",
                "date": "2020-09-25T19:38:01Z"
            },
            {
                "name": "Elastic Horovod, Ray integration, All-to-All, Gradient Predivide, CMake build system",
                "date": "2020-09-04T00:34:45Z"
            },
            {
                "name": "Hotfix for adding PYTHONPATH to mpirun env",
                "date": "2020-06-24T16:44:08Z"
            },
            {
                "name": "Hotfix for sync batch norm in PyTorch 1.5, mixed precision in TensorFlow 2.2",
                "date": "2020-05-28T22:28:14Z"
            },
            {
                "name": "Hotfix for horovodrun network interface discovery process",
                "date": "2020-05-22T16:54:52Z"
            },
            {
                "name": "Platform LSF support, Spark on Gloo, and Sync Batch Norm",
                "date": "2020-05-13T21:18:46Z"
            },
            {
                "name": "TensorFlow 2.2 compatibility, MPI args for horovodrun",
                "date": "2020-05-13T20:56:39Z"
            },
            {
                "name": "Horovod Spark Estimators, Spark 3, Join, Interactive Run",
                "date": "2020-01-14T16:39:30Z"
            }
        ]
    }
}