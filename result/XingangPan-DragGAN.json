{
    "https://api.github.com/repos/XingangPan/DragGAN": {
        "forks": 3460,
        "watchers": 35840,
        "stars": 35840,
        "languages": {
            "Python": 1193439,
            "Cuda": 250786,
            "C++": 68635,
            "Dockerfile": 689,
            "Batchfile": 524,
            "Shell": 502
        },
        "commits": [
            "2023-10-11T08:11:16Z",
            "2023-07-18T13:50:48Z",
            "2023-07-10T14:08:09Z",
            "2023-07-10T00:17:56Z",
            "2023-07-07T11:36:42Z",
            "2023-07-02T04:41:39Z",
            "2023-06-29T15:30:00Z",
            "2023-06-29T14:28:14Z",
            "2023-06-29T14:19:25Z",
            "2023-06-29T14:18:46Z",
            "2023-06-29T14:03:59Z",
            "2023-06-29T14:03:01Z",
            "2023-06-29T10:54:27Z",
            "2023-06-29T09:43:52Z",
            "2023-06-29T07:34:02Z",
            "2023-06-29T01:37:13Z",
            "2023-06-29T01:31:25Z",
            "2023-06-29T01:28:26Z",
            "2023-06-29T01:27:48Z",
            "2023-06-28T22:21:10Z",
            "2023-06-28T19:56:28Z",
            "2023-06-28T16:02:49Z",
            "2023-06-28T06:41:49Z",
            "2023-06-27T16:31:12Z",
            "2023-06-27T16:28:16Z",
            "2023-06-27T16:27:08Z",
            "2023-06-27T12:56:30Z",
            "2023-06-27T12:26:03Z",
            "2023-06-27T07:19:20Z",
            "2023-06-27T02:31:22Z"
        ],
        "creation_date": "2023-05-18T10:08:02Z",
        "contributors": 15,
        "topics": [
            "artificial-intelligence",
            "generative-adversarial-network",
            "generative-models",
            "image-manipulation"
        ],
        "subscribers": 987,
        "readme": "<p align=\"center\">\n\n  <h1 align=\"center\">Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold</h1>\n  <p align=\"center\">\n    <a href=\"https://xingangpan.github.io/\"><strong>Xingang Pan</strong></a>\n    \u00b7\n    <a href=\"https://ayushtewari.com/\"><strong>Ayush Tewari</strong></a>\n    \u00b7\n    <a href=\"https://people.mpi-inf.mpg.de/~tleimkue/\"><strong>Thomas Leimk\u00fchler</strong></a>\n    \u00b7\n    <a href=\"https://lingjie0206.github.io/\"><strong>Lingjie Liu</strong></a>\n    \u00b7\n    <a href=\"https://www.meka.page/\"><strong>Abhimitra Meka</strong></a>\n    \u00b7\n    <a href=\"http://www.mpi-inf.mpg.de/~theobalt/\"><strong>Christian Theobalt</strong></a>\n  </p>\n  <h2 align=\"center\">SIGGRAPH 2023 Conference Proceedings</h2>\n  <div align=\"center\">\n    <img src=\"DragGAN.gif\", width=\"600\">\n  </div>\n\n  <p align=\"center\">\n  <br>\n    <a href=\"https://pytorch.org/get-started/locally/\"><img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white\"></a>\n    <a href=\"https://twitter.com/XingangP\"><img alt='Twitter' src=\"https://img.shields.io/twitter/follow/XingangP?label=%40XingangP\"></a>\n    <a href=\"https://arxiv.org/abs/2305.10973\">\n      <img src='https://img.shields.io/badge/Paper-PDF-green?style=for-the-badge&logo=adobeacrobatreader&logoWidth=20&logoColor=white&labelColor=66cc00&color=94DD15' alt='Paper PDF'>\n    </a>\n    <a href='https://vcai.mpi-inf.mpg.de/projects/DragGAN/'>\n      <img src='https://img.shields.io/badge/DragGAN-Page-orange?style=for-the-badge&logo=Google%20chrome&logoColor=white&labelColor=D35400' alt='Project Page'></a>\n    <a href=\"https://colab.research.google.com/drive/1mey-IXPwQC_qSthI5hO-LTX7QL4ivtPh?usp=sharing\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n  </p>\n</p>\n\n## Web Demos\n\n[![Open in OpenXLab](https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg)](https://openxlab.org.cn/apps/detail/XingangPan/DragGAN)\n\n<p align=\"left\">\n  <a href=\"https://huggingface.co/spaces/radames/DragGan\"><img alt=\"Huggingface\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DragGAN-orange\"></a>\n</p>\n\n## Requirements\n\nIf you have CUDA graphic card, please follow the requirements of [NVlabs/stylegan3](https://github.com/NVlabs/stylegan3#requirements).  \n\nThe usual installation steps involve the following commands, they should set up the correct CUDA version and all the python packages\n\n```\nconda env create -f environment.yml\nconda activate stylegan3\n```\n\nThen install the additional requirements\n\n```\npip install -r requirements.txt\n```\n\nOtherwise (for GPU acceleration on MacOS with Silicon Mac M1/M2, or just CPU) try the following:\n\n```sh\ncat environment.yml | \\\n  grep -v -E 'nvidia|cuda' > environment-no-nvidia.yml && \\\n    conda env create -f environment-no-nvidia.yml\nconda activate stylegan3\n\n# On MacOS\nexport PYTORCH_ENABLE_MPS_FALLBACK=1\n```\n\n## Run Gradio visualizer in Docker \n\nProvided docker image is based on NGC PyTorch repository. To quickly try out visualizer in Docker, run the following:  \n\n```sh\n# before you build the docker container, make sure you have cloned this repo, and downloaded the pretrained model by `python scripts/download_model.py`.\ndocker build . -t draggan:latest  \ndocker run -p 7860:7860 -v \"$PWD\":/workspace/src -it draggan:latest bash\n# (Use GPU)if you want to utilize your Nvidia gpu to accelerate in docker, please add command tag `--gpus all`, like:\n#   docker run --gpus all  -p 7860:7860 -v \"$PWD\":/workspace/src -it draggan:latest bash\n\ncd src && python visualizer_drag_gradio.py --listen\n```\nNow you can open a shared link from Gradio (printed in the terminal console).   \nBeware the Docker image takes about 25GB of disk space!\n\n## Download pre-trained StyleGAN2 weights\n\nTo download pre-trained weights, simply run:\n\n```\npython scripts/download_model.py\n```\nIf you want to try StyleGAN-Human and the Landscapes HQ (LHQ) dataset, please download weights from these links: [StyleGAN-Human](https://drive.google.com/file/d/1dlFEHbu-WzQWJl7nBBZYcTyo000H9hVm/view?usp=sharing), [LHQ](https://drive.google.com/file/d/16twEf0T9QINAEoMsWefoWiyhcTd-aiWc/view?usp=sharing), and put them under `./checkpoints`.\n\nFeel free to try other pretrained StyleGAN.\n\n## Run DragGAN GUI\n\nTo start the DragGAN GUI, simply run:\n```sh\nsh scripts/gui.sh\n```\nIf you are using windows, you can run:\n```\n.\\scripts\\gui.bat\n```\n\nThis GUI supports editing GAN-generated images. To edit a real image, you need to first perform GAN inversion using tools like [PTI](https://github.com/danielroich/PTI). Then load the new latent code and model weights to the GUI.\n\nYou can run DragGAN Gradio demo as well, this is universal for both windows and linux:\n```sh\npython visualizer_drag_gradio.py\n```\n\n## Acknowledgement\n\nThis code is developed based on [StyleGAN3](https://github.com/NVlabs/stylegan3). Part of the code is borrowed from [StyleGAN-Human](https://github.com/stylegan-human/StyleGAN-Human).\n\n(cheers to the community as well)\n## License\n\nThe code related to the DragGAN algorithm is licensed under [CC-BY-NC](https://creativecommons.org/licenses/by-nc/4.0/).\nHowever, most of this project are available under a separate license terms: all codes used or modified from [StyleGAN3](https://github.com/NVlabs/stylegan3) is under the [Nvidia Source Code License](https://github.com/NVlabs/stylegan3/blob/main/LICENSE.txt).\n\nAny form of use and derivative of this code must preserve the watermarking functionality showing \"AI Generated\".\n\n## BibTeX\n\n```bibtex\n@inproceedings{pan2023draggan,\n    title={Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold},\n    author={Pan, Xingang and Tewari, Ayush, and Leimk{\\\"u}hler, Thomas and Liu, Lingjie and Meka, Abhimitra and Theobalt, Christian},\n    booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},\n    year={2023}\n}\n```\n",
        "releases": []
    }
}