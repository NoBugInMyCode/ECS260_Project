{
    "https://api.github.com/repos/emilwallner/Screenshot-to-code": {
        "forks": 1562,
        "watchers": 16460,
        "stars": 16460,
        "languages": {
            "HTML": 29778,
            "Jupyter Notebook": 28320,
            "Python": 9025,
            "CSS": 5990
        },
        "commits": [
            "2024-08-16T22:05:07Z",
            "2024-08-16T22:04:37Z",
            "2024-03-25T02:09:25Z",
            "2022-05-24T14:52:26Z",
            "2022-05-24T14:46:14Z",
            "2021-05-26T10:03:29Z",
            "2021-02-09T15:17:25Z",
            "2019-12-08T13:43:24Z",
            "2019-09-10T13:58:46Z",
            "2019-09-09T19:54:32Z",
            "2019-09-03T14:56:46Z",
            "2019-08-13T15:42:47Z",
            "2019-07-26T09:03:22Z",
            "2019-06-12T14:05:13Z",
            "2019-03-03T16:31:52Z",
            "2019-02-21T21:24:01Z",
            "2019-02-21T21:21:03Z",
            "2019-02-21T21:19:22Z",
            "2019-02-20T17:41:54Z",
            "2019-02-20T17:40:36Z",
            "2018-11-17T13:42:20Z",
            "2018-11-17T13:26:50Z",
            "2018-11-17T13:10:45Z",
            "2018-11-17T12:45:44Z",
            "2018-09-13T12:54:00Z",
            "2018-09-04T12:37:02Z",
            "2018-08-10T17:56:20Z",
            "2018-08-10T12:37:18Z",
            "2018-08-09T16:00:02Z",
            "2018-08-09T15:58:22Z"
        ],
        "creation_date": "2017-10-16T11:41:48Z",
        "contributors": 6,
        "topics": [
            "cnn",
            "cnn-keras",
            "deep-learning",
            "encoder-decoder",
            "floydhub",
            "jupyter",
            "jupyter-notebook",
            "keras",
            "lstm",
            "machine-learning",
            "seq2seq"
        ],
        "subscribers": 535,
        "readme": "<img src=\"/README_images/screenshot-to-code.svg?raw=true\" width=\"800px\">\n\n---\n\n**A detailed tutorial covering the code in this repository:** [Turning design mockups into code with deep learning](https://emilwallner.medium.com/how-you-can-train-an-ai-to-convert-your-design-mockups-into-html-and-css-cc7afd82fed4).\n\n**Plug:** \ud83d\udc49 Check out my 60-page guide, [No ML Degree](https://www.emilwallner.com/p/no-ml-degree), on how to land a machine learning job without a degree.\n\nThe neural network is built in three iterations. Starting with a Hello World version, followed by the main neural network layers, and ending by training it to generalize. \n\nThe models are based on Tony Beltramelli's [pix2code](https://github.com/tonybeltramelli/pix2code), and inspired by Airbnb's [sketching interfaces](https://airbnb.design/sketching-interfaces/), and Harvard's [im2markup](https://github.com/harvardnlp/im2markup).\n\n**Note:** only the Bootstrap version can generalize on new design mock-ups. It uses 16 domain-specific tokens which are translated into HTML/CSS. It has a 97% accuracy. The best model uses a GRU instead of an LSTM. This version can be trained on a few GPUs. The raw HTML version has potential to generalize, but is still unproven and requires a significant amount of GPUs to train. The current model is also trained on a homogeneous and small dataset, thus it's hard to tell how well it behaves on more complex layouts.\n\nDataset: https://github.com/tonybeltramelli/pix2code/tree/master/datasets\n\nA quick overview of the process: \n\n### 1) Give a design image to the trained neural network\n\n![Insert image](https://i.imgur.com/LDmoLLV.png)\n\n### 2) The neural network converts the image into HTML markup \n\n<img src=\"/README_images/html_display.gif?raw=true\" width=\"800px\">\n\n### 3) Rendered output\n\n![Screenshot](https://i.imgur.com/tEAfyZ8.png)\n\n\n## Installation\n\n### FloydHub\n\n[![Run on FloydHub](https://static.floydhub.com/button/button.svg)](https://floydhub.com/run?template=https://github.com/floydhub/pix2code-template)\n\nClick this button to open a [Workspace](https://blog.floydhub.com/workspaces/) on [FloydHub](https://www.floydhub.com/?utm_medium=readme&utm_source=pix2code&utm_campaign=aug_2018) where you will find the same environment and dataset used for the *Bootstrap version*. You can also find the trained models for testing.\n\n### Local\n``` bash\npip install keras tensorflow pillow h5py jupyter\n```\n```\ngit clone https://github.com/emilwallner/Screenshot-to-code.git\ncd Screenshot-to-code/\njupyter notebook\n```\nGo do the desired notebook, files that end with '.ipynb'. To run the model, go to the menu then click on Cell > Run all\n\nThe final version, the Bootstrap version, is prepared with a small set to test run the model. If you want to try it with all the data, you need to download the data here: https://www.floydhub.com/emilwallner/datasets/imagetocode, and specify the correct ```dir_name```.\n\n## Folder structure\n\n``` bash\n  |  |-Bootstrap                           #The Bootstrap version\n  |  |  |-compiler                         #A compiler to turn the tokens to HTML/CSS (by pix2code)\n  |  |  |-resources\t\t\t\t\t\t\t\t\t\t\t\n  |  |  |  |-eval_light                    #10 test images and markup\n  |  |-Hello_world                         #The Hello World version\n  |  |-HTML                                #The HTML version\n  |  |  |-Resources_for_index_file         #CSS,images and scripts to test index.html file\n  |  |  |-html                             #HTML files to train it on\n  |  |  |-images                           #Screenshots for training\n  |-readme_images                          #Images for the readme page\n```\n\n\n## Hello World\n<p align=\"center\"><img src=\"/README_images/Hello_world_model.png?raw=true\" width=\"400px\"></p>\n\n\n## HTML\n<p align=\"center\"><img src=\"/README_images/HTML_model.png?raw=true\" width=\"400px\"></p>\n\n\n## Bootstrap\n<p align=\"center\"><img src=\"/README_images/Bootstrap_model.png?raw=true\" width=\"400px\"></p>\n\n## Model weights\n- [Bootstrap](https://www.floydhub.com/emilwallner/datasets/imagetocode) (The pre-trained model uses GRUs instead of LSTMs)\n- [HTML](https://www.floydhub.com/emilwallner/datasets/html_models)\n\n## Acknowledgments\n- Thanks to IBM for donating computing power through their PowerAI platform\n- The code is largely influenced by Tony Beltramelli's pix2code paper. [Code](https://github.com/tonybeltramelli/pix2code) [Paper](https://arxiv.org/abs/1705.07962)\n- The structure and some of the functions are from Jason Brownlee's [excellent tutorial](https://machinelearningmastery.com/develop-a-caption-generation-model-in-keras/)\n",
        "releases": []
    }
}