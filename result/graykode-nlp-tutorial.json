{
    "https://api.github.com/repos/graykode/nlp-tutorial": {
        "forks": 3948,
        "watchers": 14386,
        "stars": 14386,
        "languages": {
            "Jupyter Notebook": 150590,
            "Python": 96147
        },
        "commits": [
            "2021-07-25T14:52:13Z",
            "2021-07-25T14:51:53Z",
            "2021-07-24T06:33:56Z",
            "2021-05-02T13:49:27Z",
            "2020-10-20T02:33:56Z",
            "2020-08-15T08:33:42Z",
            "2020-08-15T08:33:25Z",
            "2020-08-15T08:24:14Z",
            "2020-08-15T08:02:05Z",
            "2020-08-14T10:17:55Z",
            "2020-08-14T07:13:01Z",
            "2020-08-13T06:37:15Z",
            "2020-08-13T06:36:44Z",
            "2020-08-13T06:35:10Z",
            "2020-08-13T06:26:40Z",
            "2020-05-25T07:04:14Z",
            "2020-05-24T22:54:35Z",
            "2019-09-26T09:40:32Z",
            "2019-09-11T14:19:37Z",
            "2019-07-29T03:05:50Z",
            "2019-07-08T12:40:01Z",
            "2019-05-14T01:44:33Z",
            "2019-05-04T02:44:15Z",
            "2019-05-02T14:33:59Z",
            "2019-04-09T14:41:02Z",
            "2019-04-06T04:19:42Z",
            "2019-04-04T15:23:45Z",
            "2019-04-04T14:58:57Z",
            "2019-04-04T14:28:46Z",
            "2019-04-04T02:41:10Z"
        ],
        "creation_date": "2019-01-09T11:44:20Z",
        "contributors": 11,
        "topics": [
            "attention",
            "bert",
            "natural-language-processing",
            "nlp",
            "paper",
            "pytorch",
            "tensorflow",
            "transformer",
            "tutorial"
        ],
        "subscribers": 289,
        "readme": "## nlp-tutorial\n\n<p align=\"center\"><img width=\"100\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/TensorFlowLogo.svg/225px-TensorFlowLogo.svg.png\" />  <img width=\"100\" src=\"https://media-thumbs.golden.com/OLqzmrmwAzY1P7Sl29k2T9WjJdM=/200x200/smart/golden-storage-production.s3.amazonaws.com/topic_images/e08914afa10a4179893eeb07cb5e4713.png\" /></p>\n\n`nlp-tutorial` is a tutorial for who is studying NLP(Natural Language Processing) using **Pytorch**. Most of the models in NLP were implemented with less than **100 lines** of code.(except comments or blank lines)\n\n- [08-14-2020] Old TensorFlow v1 code is archived in [the archive folder](archive). For beginner readability, only pytorch version 1.0 or higher is supported.\n\n\n## Curriculum - (Example Purpose)\n\n#### 1. Basic Embedding Model\n\n- 1-1. [NNLM(Neural Network Language Model)](1-1.NNLM) - **Predict Next Word**\n  - Paper -  [A Neural Probabilistic Language Model(2003)](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n  - Colab - [NNLM.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-1.NNLM/NNLM.ipynb)\n- 1-2. [Word2Vec(Skip-gram)](1-2.Word2Vec) - **Embedding Words and Show Graph**\n  - Paper - [Distributed Representations of Words and Phrases\n    and their Compositionality(2013)](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n  - Colab - [Word2Vec.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-2.Word2Vec/Word2Vec_Skipgram(Softmax).ipynb)\n- 1-3. [FastText(Application Level)](1-3.FastText) - **Sentence Classification**\n  - Paper - [Bag of Tricks for Efficient Text Classification(2016)](https://arxiv.org/pdf/1607.01759.pdf)\n  - Colab - [FastText.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/1-3.FastText/FastText.ipynb)\n\n\n\n#### 2. CNN(Convolutional Neural Network)\n\n- 2-1. [TextCNN](2-1.TextCNN) - **Binary Sentiment Classification**\n  - Paper - [Convolutional Neural Networks for Sentence Classification(2014)](http://www.aclweb.org/anthology/D14-1181)\n  - [TextCNN.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/2-1.TextCNN/TextCNN.ipynb)\n\n\n\n#### 3. RNN(Recurrent Neural Network)\n\n- 3-1. [TextRNN](3-1.TextRNN) - **Predict Next Step**\n  - Paper - [Finding Structure in Time(1990)](http://psych.colorado.edu/~kimlab/Elman1990.pdf)\n  - Colab - [TextRNN.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-1.TextRNN/TextRNN.ipynb)\n- 3-2. [TextLSTM](https://github.com/graykode/nlp-tutorial/tree/master/3-2.TextLSTM) - **Autocomplete**\n  - Paper - [LONG SHORT-TERM MEMORY(1997)](https://www.bioinf.jku.at/publications/older/2604.pdf)\n  - Colab - [TextLSTM.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-2.TextLSTM/TextLSTM.ipynb)\n- 3-3. [Bi-LSTM](3-3.Bi-LSTM) - **Predict Next Word in Long Sentence**\n  - Colab - [Bi_LSTM.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/3-3.Bi-LSTM/Bi_LSTM.ipynb)\n\n\n\n#### 4. Attention Mechanism\n\n- 4-1. [Seq2Seq](4-1.Seq2Seq) - **Change Word**\n  - Paper - [Learning Phrase Representations using RNN Encoder\u2013Decoder\n    for Statistical Machine Translation(2014)](https://arxiv.org/pdf/1406.1078.pdf)\n  - Colab - [Seq2Seq.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-1.Seq2Seq/Seq2Seq.ipynb)\n- 4-2. [Seq2Seq with Attention](4-2.Seq2Seq(Attention)) - **Translate**\n  - Paper - [Neural Machine Translation by Jointly Learning to Align and Translate(2014)](https://arxiv.org/abs/1409.0473)\n  - Colab - [Seq2Seq(Attention).ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-2.Seq2Seq(Attention)/Seq2Seq(Attention).ipynb)\n- 4-3. [Bi-LSTM with Attention](4-3.Bi-LSTM(Attention)) - **Binary Sentiment Classification**\n  - Colab - [Bi_LSTM(Attention).ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/4-3.Bi-LSTM(Attention)/Bi_LSTM(Attention).ipynb)\n\n\n\n#### 5. Model based on Transformer\n\n- 5-1.  [The Transformer](5-1.Transformer) - **Translate**\n  - Paper - [Attention Is All You Need(2017)](https://arxiv.org/abs/1706.03762)\n  - Colab - [Transformer.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer.ipynb), [Transformer(Greedy_decoder).ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer(Greedy_decoder).ipynb)\n- 5-2. [BERT](5-2.BERT) - **Classification Next Sentence & Predict Masked Tokens**\n  - Paper - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2018)](https://arxiv.org/abs/1810.04805)\n  - Colab - [BERT.ipynb](https://colab.research.google.com/github/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT.ipynb)\n\n\n\n## Dependencies\n\n- Python 3.5+\n- Pytorch 1.0.0+\n\n\n\n## Author\n\n- Tae Hwan Jung(Jeff Jung) @graykode\n- Author Email : nlkey2022@gmail.com\n- Acknowledgements to [mojitok](http://mojitok.com/) as NLP Research Internship.\n",
        "releases": []
    }
}