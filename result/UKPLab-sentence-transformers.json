{
    "https://api.github.com/repos/UKPLab/sentence-transformers": {
        "forks": 2525,
        "watchers": 15814,
        "stars": 15814,
        "languages": {
            "Python": 1040350,
            "Makefile": 509
        },
        "commits": [
            "2025-01-21T20:19:49Z",
            "2025-01-21T16:49:45Z",
            "2025-01-21T13:13:24Z",
            "2025-01-20T12:40:59Z",
            "2025-01-17T15:37:02Z",
            "2025-01-17T13:50:21Z",
            "2025-01-17T13:49:10Z",
            "2025-01-17T13:47:54Z",
            "2025-01-10T14:37:09Z",
            "2025-01-10T12:19:18Z",
            "2025-01-06T17:41:13Z",
            "2025-01-06T17:39:37Z",
            "2025-01-06T16:04:31Z",
            "2024-12-23T14:29:58Z",
            "2024-12-12T18:49:49Z",
            "2024-12-12T10:58:51Z",
            "2024-12-12T10:56:56Z",
            "2024-12-12T10:56:36Z",
            "2024-12-10T08:54:41Z",
            "2024-12-04T12:41:49Z",
            "2024-12-03T12:25:36Z",
            "2024-12-02T13:43:14Z",
            "2024-12-02T11:52:07Z",
            "2024-12-02T10:08:33Z",
            "2024-12-02T09:12:49Z",
            "2024-11-28T15:28:28Z",
            "2024-11-28T14:17:05Z",
            "2024-11-28T14:12:45Z",
            "2024-11-27T13:21:54Z",
            "2024-11-26T15:04:52Z"
        ],
        "creation_date": "2019-07-24T10:53:51Z",
        "contributors": 30,
        "topics": [],
        "subscribers": 143,
        "readme": "<!--- BADGES: START --->\n[![HF Models](https://img.shields.io/badge/%F0%9F%A4%97-models-yellow)](https://huggingface.co/models?library=sentence-transformers)\n[![GitHub - License](https://img.shields.io/github/license/UKPLab/sentence-transformers?logo=github&style=flat&color=green)][#github-license]\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/sentence-transformers?logo=pypi&style=flat&color=blue)][#pypi-package]\n[![PyPI - Package Version](https://img.shields.io/pypi/v/sentence-transformers?logo=pypi&style=flat&color=orange)][#pypi-package]\n[![Docs - GitHub.io](https://img.shields.io/static/v1?logo=github&style=flat&color=pink&label=docs&message=sentence-transformers)][#docs-package]\n<!-- [![PyPI - Downloads](https://img.shields.io/pypi/dm/sentence-transformers?logo=pypi&style=flat&color=green)][#pypi-package] -->\n\n[#github-license]: https://github.com/UKPLab/sentence-transformers/blob/master/LICENSE\n[#pypi-package]: https://pypi.org/project/sentence-transformers/\n[#conda-forge-package]: https://anaconda.org/conda-forge/sentence-transformers\n[#docs-package]: https://www.sbert.net/\n<!--- BADGES: END --->\n\n# Sentence Transformers: Multilingual Sentence, Paragraph, and Image Embeddings using BERT & Co.\n\nThis framework provides an easy method to compute dense vector representations for **sentences**, **paragraphs**, and **images**. The models are based on transformer networks like BERT / RoBERTa / XLM-RoBERTa etc. and achieve state-of-the-art performance in various tasks. Text is embedded in vector space such that similar text are closer and can efficiently be found using cosine similarity.\n\nWe provide an increasing number of **[state-of-the-art pretrained models](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html)** for more than 100 languages, fine-tuned for various use-cases.\n\nFurther, this framework allows an easy  **[fine-tuning of custom embeddings models](https://www.sbert.net/docs/sentence_transformer/training_overview.html)**, to achieve maximal performance on your specific task.\n\nFor the **full documentation**, see **[www.SBERT.net](https://www.sbert.net)**.\n\n## Installation\n\nWe recommend **Python 3.9+**, **[PyTorch 1.11.0+](https://pytorch.org/get-started/locally/)**, and **[transformers v4.34.0+](https://github.com/huggingface/transformers)**.\n\n**Install with pip**\n\n```\npip install -U sentence-transformers\n```\n\n**Install with conda**\n\n```\nconda install -c conda-forge sentence-transformers\n```\n\n**Install from sources**\n\nAlternatively, you can also clone the latest version from the [repository](https://github.com/UKPLab/sentence-transformers) and install it directly from the source code:\n\n````\npip install -e .\n```` \n\n**PyTorch with CUDA**\n\nIf you want to use a GPU / CUDA, you must install PyTorch with the matching CUDA Version. Follow\n[PyTorch - Get Started](https://pytorch.org/get-started/locally/) for further details how to install PyTorch.\n\n## Getting Started\n\nSee [Quickstart](https://www.sbert.net/docs/quickstart.html) in our documentation.\n\nFirst download a pretrained model.\n\n````python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n````\n\nThen provide some sentences to the model.\n\n````python\nsentences = [\n    \"The weather is lovely today.\",\n    \"It's so sunny outside!\",\n    \"He drove to the stadium.\",\n]\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# => (3, 384)\n````\n\nAnd that's already it. We now have a numpy arrays with the embeddings, one for each text. We can use these to compute similarities.\n\n````python\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n# tensor([[1.0000, 0.6660, 0.1046],\n#         [0.6660, 1.0000, 0.1411],\n#         [0.1046, 0.1411, 1.0000]])\n````\n\n## Pre-Trained Models\n\nWe provide a large list of [Pretrained Models](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html) for more than 100 languages. Some models are general purpose models, while others produce embeddings for specific use cases. Pre-trained models can be loaded by just passing the model name: `SentenceTransformer('model_name')`.\n\n## Training\n\nThis framework allows you to fine-tune your own sentence embedding methods, so that you get task-specific sentence embeddings. You have various options to choose from in order to get perfect sentence embeddings for your specific task. \n\nSee [Training Overview](https://www.sbert.net/docs/sentence_transformer/training_overview.html) for an introduction how to train your own embedding models. We provide [various examples](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training) how to train models on various datasets.\n\nSome highlights are:\n- Support of various transformer networks including BERT, RoBERTa, XLM-R, DistilBERT, Electra, BART, ...\n- Multi-Lingual and multi-task learning\n- Evaluation during training to find optimal model\n- [20+ loss-functions](https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html) allowing to tune models specifically for semantic search, paraphrase mining, semantic similarity comparison, clustering, triplet loss, contrastive loss, etc.\n\n## Application Examples\n\nYou can use this framework for:\n\n- [Computing Sentence Embeddings](https://www.sbert.net/examples/applications/computing-embeddings/README.html)\n- [Semantic Textual Similarity](https://www.sbert.net/docs/usage/semantic_textual_similarity.html)\n- [Semantic Search](https://www.sbert.net/examples/applications/semantic-search/README.html)\n- [Retrieve & Re-Rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) \n- [Clustering](https://www.sbert.net/examples/applications/clustering/README.html)\n- [Paraphrase Mining](https://www.sbert.net/examples/applications/paraphrase-mining/README.html)\n- [Translated Sentence Mining](https://www.sbert.net/examples/applications/parallel-sentence-mining/README.html)\n- [Multilingual Image Search, Clustering & Duplicate Detection](https://www.sbert.net/examples/applications/image-search/README.html)\n\nand many more use-cases.\n\nFor all examples, see [examples/applications](https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications).\n\n## Development setup\n\nAfter cloning the repo (or a fork) to your machine, in a virtual environment, run:\n\n```\npython -m pip install -e \".[dev]\"\n\npre-commit install\n```\n\nTo test your changes, run:\n\n```\npytest\n```\n\n## Citing & Authors\n\nIf you find this repository helpful, feel free to cite our publication [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084):\n\n```bibtex \n@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n```\n\nIf you use one of the multilingual models, feel free to cite our publication [Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation](https://arxiv.org/abs/2004.09813):\n\n```bibtex\n@inproceedings{reimers-2020-multilingual-sentence-bert,\n    title = \"Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2020\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/2004.09813\",\n}\n```\n\nPlease have a look at [Publications](https://www.sbert.net/docs/publications.html) for our different publications that are integrated into SentenceTransformers.\n\nMaintainer: [Tom Aarsen](https://github.com/tomaarsen), \ud83e\udd17 Hugging Face\n\nhttps://www.ukp.tu-darmstadt.de/\n\nDon't hesitate to open an issue if something is broken (and it shouldn't be) or if you have further questions.\n\n> This repository contains experimental software and is published for the sole purpose of giving additional background details on the respective publication.\n",
        "releases": [
            {
                "name": "v3.3.1 - Patch private model loading without environment variable",
                "date": "2024-11-18T14:38:20Z"
            },
            {
                "name": "v3.3.0 - Massive CPU speedup with OpenVINO int8 quantization; Training with Prompts for stronger models; NanoBEIR IR evaluation; PEFT compatibility; Transformers v4.46.0 compatibility",
                "date": "2024-11-11T12:02:19Z"
            },
            {
                "name": "v3.2.1 - Patch CLIP loading, small ONNX fix, compatibility with other libraries",
                "date": "2024-10-21T12:19:51Z"
            },
            {
                "name": "v3.2.0 - ONNX and OpenVINO backends offering 2-3x speedup; Static Embeddings offering 50x-500x speedups at ~10-20% performance cost",
                "date": "2024-10-10T17:59:24Z"
            },
            {
                "name": "v3.1.1 - Patch hard negative mining & remove `numpy<2` restriction",
                "date": "2024-09-19T14:18:18Z"
            },
            {
                "name": "v3.1.0 - Hard Negatives Mining utility; new loss function for symmetric tasks; streaming datasets; custom modules",
                "date": "2024-09-11T14:06:41Z"
            },
            {
                "name": "v3.0.1 - Patch introducing new Trainer features, model card improvements and evaluator fixes",
                "date": "2024-06-07T13:01:30Z"
            },
            {
                "name": "v3.0.0 - Sentence Transformer Training Refactor; new similarity methods; hyperparameter optimization; 50+ datasets release",
                "date": "2024-05-28T11:54:03Z"
            },
            {
                "name": "v2.7.0 - CachedGISTEmbedLoss, easy Matryoshka inference & evaluation, CrossEncoder, Intel Gaudi2",
                "date": "2024-04-17T13:16:06Z"
            },
            {
                "name": "v2.6.1 - Fix Quantized Semantic Search rescoring",
                "date": "2024-03-26T09:01:26Z"
            },
            {
                "name": "v2.6.0 - Embedding Quantization, GISTEmbedLoss",
                "date": "2024-03-22T15:29:12Z"
            },
            {
                "name": "v2.5.1 - fix CrossEncoder.rank bug with default top_k",
                "date": "2024-03-01T08:18:33Z"
            },
            {
                "name": "v2.5.0 - 2D Matryoshka & Adaptive Layer models, CrossEncoder (re)ranking",
                "date": "2024-02-29T14:07:11Z"
            },
            {
                "name": "v2.4.0 - Matryoshka models, SOTA loss functions, prompt templates, INSTRUCTOR support",
                "date": "2024-02-23T13:12:22Z"
            },
            {
                "name": "v2.3.1 - Patch for local models with Normalize modules",
                "date": "2024-01-30T19:43:59Z"
            },
            {
                "name": "v2.3.0 - Bug fixes, improved model loading & Cached MNRL",
                "date": "2024-01-29T08:32:04Z"
            },
            {
                "name": "v2.2.2 - Bugfix huggingface_hub for Python 3.6",
                "date": "2022-06-26T19:52:06Z"
            },
            {
                "name": "v2.2.1 - Update huggingface_hub & fixes",
                "date": "2022-06-23T12:59:53Z"
            },
            {
                "name": "v2.2.0 - T5 Encoder & Private models",
                "date": "2022-02-10T13:12:24Z"
            },
            {
                "name": "v2.1.0 - New Loss Functions",
                "date": "2021-10-01T09:10:30Z"
            },
            {
                "name": "v2.0.0 - Integration into Huggingface Model Hub",
                "date": "2021-06-24T16:16:11Z"
            },
            {
                "name": "v1.2.1 - Forward compatibility with version 2",
                "date": "2021-06-24T14:20:43Z"
            },
            {
                "name": "v1.2.0 - Unsupervised Learning, New Training Examples, Improved Models",
                "date": "2021-05-12T13:14:10Z"
            },
            {
                "name": "Unsupervised Sentence Embedding Learning",
                "date": "2021-04-21T13:12:31Z"
            },
            {
                "name": "v1.0.4 - Patch CLIPModel.save",
                "date": "2021-04-01T06:35:11Z"
            },
            {
                "name": "v1.0.3 - Patch  util.paraphrase_mining",
                "date": "2021-03-22T08:15:39Z"
            },
            {
                "name": "v1.0.2 - Patch CLIPModel",
                "date": "2021-03-19T21:44:45Z"
            },
            {
                "name": "v1.0.0 - Improvements, New Models, Text-Image Models",
                "date": "2021-03-18T20:57:58Z"
            },
            {
                "name": "v0.4.1 - Faster Tokenization & Asymmetric Models",
                "date": "2021-01-04T14:04:01Z"
            },
            {
                "name": "v0.4.0 - Upgrade Transformers Version",
                "date": "2020-12-22T13:42:13Z"
            },
            {
                "name": "v0.3.9 - Small updates",
                "date": "2020-11-18T08:25:11Z"
            },
            {
                "name": "v0.3.8 - CrossEncoder, Data Augmentation, new Models",
                "date": "2020-10-19T14:23:08Z"
            },
            {
                "name": "v0.3.7 - Upgrade transformers, Model Distillation Example, Multi-Input to Transformers Model",
                "date": "2020-09-29T20:17:02Z"
            },
            {
                "name": "v0.3.6 - Update transformers to v3.1.0",
                "date": "2020-09-11T08:06:16Z"
            },
            {
                "name": "v0.3.5 - Automatic Mixed Precision & Bugfixes",
                "date": "2020-09-01T13:09:07Z"
            },
            {
                "name": "v0.3.4 - Improved Documentation, Improved Tokenization Speed, Mutli-GPU encoding",
                "date": "2020-08-24T16:24:24Z"
            },
            {
                "name": "v0.3.3 - Multi-Process Tokenization and Information Retrieval Improvements",
                "date": "2020-08-06T08:16:50Z"
            },
            {
                "name": "v0.3.2 - Lazy tokenization for Parallel Sentence Training & Improved Semantic Search",
                "date": "2020-07-23T15:03:59Z"
            },
            {
                "name": "v0.3.1 - Updates on Multilingual Training",
                "date": "2020-07-22T13:54:26Z"
            },
            {
                "name": "v0.3.0 - Transformers Updated to Version 3",
                "date": "2020-07-09T15:11:23Z"
            },
            {
                "name": "v0.2.6 - Transformers Update - AutoModel - WKPooling",
                "date": "2020-04-16T14:12:34Z"
            },
            {
                "name": "v0.2.5 - Transformers updates, T5 and XML-RoBERTa added",
                "date": "2020-01-10T09:30:10Z"
            },
            {
                "name": "v0.2.4 - Transformer Update - DistilBERT and ALBERT added",
                "date": "2019-12-06T14:12:01Z"
            },
            {
                "name": "v0.2.3 - Windows bugfixes",
                "date": "2019-08-20T17:21:51Z"
            },
            {
                "name": "v0.2.2 - RoBERTa support",
                "date": "2019-08-19T14:38:06Z"
            },
            {
                "name": "v0.2.1 - Bugfix pypi",
                "date": "2019-08-16T21:19:12Z"
            },
            {
                "name": "v0.2.0 - New Architecture & Models",
                "date": "2019-08-16T08:14:24Z"
            },
            {
                "name": "v0.1.0",
                "date": "2019-07-25T08:02:27Z"
            }
        ]
    }
}