{
    "https://api.github.com/repos/Mikubill/sd-webui-controlnet": {
        "forks": 1985,
        "watchers": 17283,
        "stars": 17283,
        "languages": {
            "Python": 5898102,
            "Cuda": 166914,
            "C++": 90584,
            "JavaScript": 32407,
            "CMake": 6795,
            "Shell": 4071,
            "CSS": 3851,
            "Jupyter Notebook": 1203,
            "Dockerfile": 951
        },
        "commits": [
            "2024-07-25T20:52:52Z",
            "2024-07-15T17:54:22Z",
            "2024-07-15T07:36:41Z",
            "2024-07-15T07:33:42Z",
            "2024-07-15T01:17:29Z",
            "2024-07-13T14:42:11Z",
            "2024-07-13T14:40:56Z",
            "2024-07-09T18:34:32Z",
            "2024-07-09T18:27:14Z",
            "2024-07-03T17:31:50Z",
            "2024-07-03T17:28:40Z",
            "2024-07-01T20:54:54Z",
            "2024-06-09T17:01:39Z",
            "2024-06-05T01:37:51Z",
            "2024-05-28T21:16:05Z",
            "2024-05-27T15:26:28Z",
            "2024-05-23T12:54:13Z",
            "2024-05-22T21:16:24Z",
            "2024-05-22T19:03:45Z",
            "2024-05-20T18:25:07Z",
            "2024-05-20T16:32:08Z",
            "2024-05-20T01:22:14Z",
            "2024-05-20T00:41:24Z",
            "2024-05-19T19:56:46Z",
            "2024-05-19T18:36:29Z",
            "2024-05-19T18:07:41Z",
            "2024-05-19T15:12:49Z",
            "2024-05-19T02:48:29Z",
            "2024-05-16T14:47:53Z",
            "2024-05-15T02:15:27Z"
        ],
        "creation_date": "2023-02-12T16:26:27Z",
        "contributors": 30,
        "topics": [],
        "subscribers": 148,
        "readme": "# ControlNet for Stable Diffusion WebUI\n\nThe WebUI extension for ControlNet and other injection-based SD controls.\n![image](https://github.com/Mikubill/sd-webui-controlnet/assets/20929282/261f9a50-ba9c-472f-b398-fced61929c4a)\n\nThis extension is for AUTOMATIC1111's [Stable Diffusion web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui), allows the Web UI to add [ControlNet](https://github.com/lllyasviel/ControlNet) to the original Stable Diffusion model to generate images. The addition is on-the-fly, the merging is not required.\n\n# News\n\n- [2024-07-09] \ud83d\udd25[v1.1.454] ControlNet union model support [Discussion thread: https://github.com/Mikubill/sd-webui-controlnet/discussions/2989]\n- [2024-07-01] \ud83d\udd25[v1.1.452] Depth Anything V2 - UDAV2 depth Preprocessor [Pull thread: https://github.com/Mikubill/sd-webui-controlnet/pull/2969]\n- [2024-05-19] \ud83d\udd25[v1.1.449] Anyline Preprocessor & MistoLine SDXL model [Discussion thread: https://github.com/Mikubill/sd-webui-controlnet/discussions/2907]\n- [2024-05-04] \ud83d\udd25[v1.1.447] PuLID [Discussion thread: https://github.com/Mikubill/sd-webui-controlnet/discussions/2841]\n- [2024-04-30] \ud83d\udd25[v1.1.446] Effective region mask supported for ControlNet/IPAdapter [Discussion thread: https://github.com/Mikubill/sd-webui-controlnet/discussions/2831]\n- [2024-04-27] \ud83d\udd25ControlNet-lllite Normal Dsine released [Discussion thread: https://github.com/Mikubill/sd-webui-controlnet/discussions/2813]\n- [2024-04-19] \ud83d\udd25[v1.1.445] IPAdapter advanced weight [Instant Style] [Discussion thread: https://github.com/Mikubill/sd-webui-controlnet/discussions/2770]\n- [2024-04-17] \ud83d\udd25[v1.1.444] Marigold depth preprocessor [Discussion thread: https://github.com/Mikubill/sd-webui-controlnet/discussions/2760]\n- [2024-04-15] \ud83d\udd25ControlNet++ models released [Discussion thread: https://github.com/Mikubill/sd-webui-controlnet/discussions/2778]\n- [2024-04-13] \ud83d\udd25TTPLanet_SDXL_Controlnet_Tile_Realistic v2 released [[Civitai Page](https://civitai.com/models/330313/ttplanetsdxlcontrolnettilerealistic)]\n- [2024-03-31] \ud83d\udd25[v1.1.443] IP-Adapter CLIP mask and ip-adapter-auto preprocessor [Discussion thread: https://github.com/Mikubill/sd-webui-controlnet/discussions/2723]\n- [2024-03-20] \ud83d\udd25IPAdapter Composition [Discussion thread: https://github.com/Mikubill/sd-webui-controlnet/discussions/2781]\n\n# Installation\n\n1. Open \"Extensions\" tab.\n2. Open \"Install from URL\" tab in the tab.\n3. Enter `https://github.com/Mikubill/sd-webui-controlnet.git` to \"URL for extension's git repository\".\n4. Press \"Install\" button.\n5. Wait for 5 seconds, and you will see the message \"Installed into stable-diffusion-webui\\extensions\\sd-webui-controlnet. Use Installed tab to restart\".\n6. Go to \"Installed\" tab, click \"Check for updates\", and then click \"Apply and restart UI\". (The next time you can also use these buttons to update ControlNet.)\n7. Completely restart A1111 webui including your terminal. (If you do not know what is a \"terminal\", you can reboot your computer to achieve the same effect.)\n8. Download models (see below).\n9. After you put models in the correct folder, you may need to refresh to see the models. The refresh button is right to your \"Model\" dropdown.\n\n# Download Models\nYou can find all download links here: https://github.com/Mikubill/sd-webui-controlnet/wiki/Model-download.\n\n# Features in ControlNet 1.1\n\n### Perfect Support for All ControlNet 1.0/1.1 and T2I Adapter Models.\n\nNow we have perfect support all available models and preprocessors, including perfect support for T2I style adapter and ControlNet 1.1 Shuffle. (Make sure that your YAML file names and model file names are same, see also YAML files in \"stable-diffusion-webui\\extensions\\sd-webui-controlnet\\models\".)\n\n### Perfect Support for A1111 High-Res. Fix\n\nNow if you turn on High-Res Fix in A1111, each controlnet will output two different control images: a small one and a large one. The small one is for your basic generating, and the big one is for your High-Res Fix generating. The two control images are computed by a smart algorithm called \"super high-quality control image resampling\". This is turned on by default, and you do not need to change any setting.\n\n### Perfect Support for All A1111 Img2Img or Inpaint Settings and All Mask Types\n\nNow ControlNet is extensively tested with A1111's different types of masks, including \"Inpaint masked\"/\"Inpaint not masked\", and \"Whole picture\"/\"Only masked\", and \"Only masked padding\"&\"Mask blur\". The resizing perfectly matches A1111's \"Just resize\"/\"Crop and resize\"/\"Resize and fill\". This means you can use ControlNet in nearly everywhere in your A1111 UI without difficulty!\n\n### The New \"Pixel-Perfect\" Mode\n\nNow if you turn on pixel-perfect mode, you do not need to set preprocessor (annotator) resolutions manually. The ControlNet will automatically compute the best annotator resolution for you so that each pixel perfectly matches Stable Diffusion.\n\n### User-Friendly GUI and Preprocessor Preview\n\nWe reorganized some previously confusing UI like \"canvas width/height for new canvas\" and it is in the \ud83d\udcdd button now. Now the preview GUI is controlled by the \"allow preview\" option and the trigger button \ud83d\udca5. The preview image size is better than before, and you do not need to scroll up and down - your a1111 GUI will not be messed up anymore!\n\n### Support for Almost All Upscaling Scripts\n\nNow ControlNet 1.1 can support almost all Upscaling/Tile methods. ControlNet 1.1 support the script \"Ultimate SD upscale\" and almost all other tile-based extensions. Please do not confuse [\"Ultimate SD upscale\"](https://github.com/Coyote-A/ultimate-upscale-for-automatic1111) with \"SD upscale\" - they are different scripts. Note that the most recommended upscaling method is [\"Tiled VAE/Diffusion\"](https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111) but we test as many methods/extensions as possible. Note that \"SD upscale\" is supported since 1.1.117, and if you use it, you need to leave all ControlNet images as blank (We do not recommend \"SD upscale\" since it is somewhat buggy and cannot be maintained - use the \"Ultimate SD upscale\" instead).\n\n### More Control Modes (previously called Guess Mode)\n\nWe have fixed many bugs in previous 1.0\u2019s Guess Mode and now it is called Control Mode\n\n![image](https://user-images.githubusercontent.com/19834515/236641759-6c44ddf6-c7ad-4bda-92be-e90a52911d75.png)\n\nNow you can control which aspect is more important (your prompt or your ControlNet)\uff1a\n\n* \"Balanced\": ControlNet on both sides of CFG scale, same as turning off \"Guess Mode\" in ControlNet 1.0\n\n* \"My prompt is more important\": ControlNet on both sides of CFG scale, with progressively reduced SD U-Net injections (layer_weight*=0.825**I, where 0<=I <13, and the 13 means ControlNet injected SD 13 times). In this way, you can make sure that your prompts are perfectly displayed in your generated images.\n\n* \"ControlNet is more important\": ControlNet only on the Conditional Side of CFG scale (the cond in A1111's batch-cond-uncond). This means the ControlNet will be X times stronger if your cfg-scale is X. For example, if your cfg-scale is 7, then ControlNet is 7 times stronger. Note that here the X times stronger is different from \"Control Weights\" since your weights are not modified. This \"stronger\" effect usually has less artifact and give ControlNet more room to guess what is missing from your prompts (and in the previous 1.0, it is called \"Guess Mode\"). \n\n<table width=\"100%\">\n<tr>\n<td width=\"25%\" style=\"text-align: center\">Input (depth+canny+hed)</td>\n<td width=\"25%\" style=\"text-align: center\">\"Balanced\"</td>\n<td width=\"25%\" style=\"text-align: center\">\"My prompt is more important\"</td>\n<td width=\"25%\" style=\"text-align: center\">\"ControlNet is more important\"</td>\n</tr>\n<tr>\n<td width=\"25%\" style=\"text-align: center\"><img src=\"samples/cm1.png\"></td>\n<td width=\"25%\" style=\"text-align: center\"><img src=\"samples/cm2.png\"></td>\n<td width=\"25%\" style=\"text-align: center\"><img src=\"samples/cm3.png\"></td>\n<td width=\"25%\" style=\"text-align: center\"><img src=\"samples/cm4.png\"></td>\n</tr>\n</table>\n\n### Reference-Only Control\n\nNow we have a `reference-only` preprocessor that does not require any control models. It can guide the diffusion directly using images as references.\n\n(Prompt \"a dog running on grassland, best quality, ...\")\n\n![image](samples/ref.png)\n\nThis method is similar to inpaint-based reference but it does not make your image disordered. \n\nMany professional A1111 users know a trick to diffuse image with references by inpaint. For example, if you have a 512x512 image of a dog, and want to generate another 512x512 image with the same dog, some users will connect the 512x512 dog image and a 512x512 blank image into a 1024x512 image, send to inpaint, and mask out the blank 512x512 part to diffuse a dog with similar appearance. However, that method is usually not very satisfying since images are connected and many distortions will appear.\n\nThis `reference-only` ControlNet can directly link the attention layers of your SD to any independent images, so that your SD will read arbitrary images for reference. You need at least ControlNet 1.1.153 to use it.\n\nTo use, just select `reference-only` as preprocessor and put an image. Your SD will just use the image as reference.\n\n*Note that this method is as \"non-opinioned\" as possible. It only contains very basic connection codes, without any personal preferences, to connect the attention layers with your reference images. However, even if we tried best to not include any opinioned codes, we still need to write some subjective implementations to deal with weighting, cfg-scale, etc - tech report is on the way.*\n\nMore examples [here](https://github.com/Mikubill/sd-webui-controlnet/discussions/1236).\n\n# Technical Documents\n\nSee also the documents of ControlNet 1.1: \n\nhttps://github.com/lllyasviel/ControlNet-v1-1-nightly#model-specification\n\n# Default Setting\n\nThis is my setting. If you run into any problem, you can use this setting as a sanity check\n\n![image](https://user-images.githubusercontent.com/19834515/235620638-17937171-8ac1-45bc-a3cb-3aebf605b4ef.png)\n\n# Use Previous Models\n\n### Use ControlNet 1.0 Models\n\nhttps://huggingface.co/lllyasviel/ControlNet/tree/main/models\n\nYou can still use all previous models in the previous ControlNet 1.0. Now, the previous \"depth\" is now called \"depth_midas\", the previous \"normal\" is called \"normal_midas\", the previous \"hed\" is called \"softedge_hed\". And starting from 1.1, all line maps, edge maps, lineart maps, boundary maps will have black background and white lines.\n\n### Use T2I-Adapter Models\n\n(From TencentARC/T2I-Adapter)\n\nTo use T2I-Adapter models:\n\n1. Download files from https://huggingface.co/TencentARC/T2I-Adapter/tree/main/models\n2. Put them in \"stable-diffusion-webui\\extensions\\sd-webui-controlnet\\models\".\n3. Make sure that the file names of pth files and yaml files are consistent.\n\n*Note that \"CoAdapter\" is not implemented yet.*\n\n# Gallery\n\nThe below results are from ControlNet 1.0.\n\n| Source | Input | Output |\n|:-------------------------:|:-------------------------:|:-------------------------:|\n| (no preprocessor) |  <img width=\"256\" alt=\"\" src=\"https://github.com/Mikubill/sd-webui-controlnet/blob/main/samples/bal-source.png?raw=true\"> | <img width=\"256\" alt=\"\" src=\"https://github.com/Mikubill/sd-webui-controlnet/blob/main/samples/bal-gen.png?raw=true\"> |\n| (no preprocessor) |  <img width=\"256\" alt=\"\" src=\"https://github.com/Mikubill/sd-webui-controlnet/blob/main/samples/dog_rel.jpg?raw=true\"> | <img width=\"256\" alt=\"\" src=\"https://github.com/Mikubill/sd-webui-controlnet/blob/main/samples/dog_rel.png?raw=true\"> |\n|<img width=\"256\" alt=\"\" src=\"https://github.com/Mikubill/sd-webui-controlnet/blob/main/samples/mahiro_input.png?raw=true\">  |  <img width=\"256\" alt=\"\" src=\"https://github.com/Mikubill/sd-webui-controlnet/blob/main/samples/mahiro_canny.png?raw=true\"> | <img width=\"256\" alt=\"\" src=\"https://github.com/Mikubill/sd-webui-controlnet/blob/main/samples/mahiro-out.png?raw=true\"> |\n|<img width=\"256\" alt=\"\" src=\"https://github.com/Mikubill/sd-webui-controlnet/blob/main/samples/evt_source.jpg?raw=true\">  |  <img width=\"256\" alt=\"\" src=\"https://github.com/Mikubill/sd-webui-controlnet/blob/main/samples/evt_hed.png?raw=true\"> | <img width=\"256\" alt=\"\" src=\"https://github.com/Mikubill/sd-webui-controlnet/blob/main/samples/evt_gen.png?raw=true\"> |\n|<img width=\"256\" alt=\"\" src=\"https://github.com/Mikubill/sd-webui-controlnet/blob/main/samples/an-source.jpg?raw=true\">  |  <img width=\"256\" alt=\"\" src=\"https://github.com/Mikubill/sd-webui-controlnet/blob/main/samples/an-pose.png?raw=true\"> | <img width=\"256\" alt=\"\" src=\"https://github.com/Mikubill/sd-webui-controlnet/blob/main/samples/an-gen.png?raw=true\"> |\n|<img width=\"256\" alt=\"\" src=\"https://github.com/Mikubill/sd-webui-controlnet/blob/main/samples/sk-b-src.png?raw=true\">  |  <img width=\"256\" alt=\"\" src=\"https://github.com/Mikubill/sd-webui-controlnet/blob/main/samples/sk-b-dep.png?raw=true\"> | <img width=\"256\" alt=\"\" src=\"https://github.com/Mikubill/sd-webui-controlnet/blob/main/samples/sk-b-out.png?raw=true\"> |\n\nThe below examples are from T2I-Adapter.\n\nFrom `t2iadapter_color_sd14v1.pth` :\n\n| Source | Input | Output |\n|:-------------------------:|:-------------------------:|:-------------------------:|\n| <img width=\"256\" alt=\"\" src=\"https://user-images.githubusercontent.com/31246794/222947416-ec9e52a4-a1d0-48d8-bb81-736bf636145e.jpeg\"> | <img width=\"256\" alt=\"\" src=\"https://user-images.githubusercontent.com/31246794/222947435-1164e7d8-d857-42f9-ab10-2d4a4b25f33a.png\"> | <img width=\"256\" alt=\"\" src=\"https://user-images.githubusercontent.com/31246794/222947557-5520d5f8-88b4-474d-a576-5c9cd3acac3a.png\"> |\n\nFrom `t2iadapter_style_sd14v1.pth` :\n\n| Source | Input | Output |\n|:-------------------------:|:-------------------------:|:-------------------------:|\n| <img width=\"256\" alt=\"\" src=\"https://user-images.githubusercontent.com/31246794/222947416-ec9e52a4-a1d0-48d8-bb81-736bf636145e.jpeg\"> | (clip, non-image) | <img width=\"256\" alt=\"\" src=\"https://user-images.githubusercontent.com/31246794/222965711-7b884c9e-7095-45cb-a91c-e50d296ba3a2.png\"> |\n\n# Minimum Requirements\n\n* (Windows) (NVIDIA: Ampere) 4gb - with `--xformers` enabled, and `Low VRAM` mode ticked in the UI, goes up to 768x832\n\n# Multi-ControlNet\n\nThis option allows multiple ControlNet inputs for a single generation. To enable this option, change `Multi ControlNet: Max models amount (requires restart)` in the settings. Note that you will need to restart the WebUI for changes to take effect.\n\n<table width=\"100%\">\n<tr>\n<td width=\"25%\" style=\"text-align: center\">Source A</td>\n<td width=\"25%\" style=\"text-align: center\">Source B</td>\n<td width=\"25%\" style=\"text-align: center\">Output</td>\n</tr>\n<tr>\n<td width=\"25%\" style=\"text-align: center\"><img src=\"https://user-images.githubusercontent.com/31246794/220448620-cd3ede92-8d3f-43d5-b771-32dd8417618f.png\"></td>\n<td width=\"25%\" style=\"text-align: center\"><img src=\"https://user-images.githubusercontent.com/31246794/220448619-beed9bdb-f6bb-41c2-a7df-aa3ef1f653c5.png\"></td>\n<td width=\"25%\" style=\"text-align: center\"><img src=\"https://user-images.githubusercontent.com/31246794/220448613-c99a9e04-0450-40fd-bc73-a9122cefaa2c.png\"></td>\n</tr>\n</table>\n\n# Control Weight/Start/End\n\nWeight is the weight of the controlnet \"influence\". It's analogous to prompt attention/emphasis. E.g. (myprompt: 1.2). Technically, it's the factor by which to multiply the ControlNet outputs before merging them with original SD Unet.\n\nGuidance Start/End is the percentage of total steps the controlnet applies (guidance strength = guidance end). It's analogous to prompt editing/shifting. E.g. \\[myprompt::0.8\\] (It applies from the beginning until 80% of total steps)\n\n# Batch Mode\n\nPut any unit into batch mode to activate batch mode for all units. Specify a batch directory for each unit, or use the new textbox in the img2img batch tab as a fallback. Although the textbox is located in the img2img batch tab, you can use it to generate images in the txt2img tab as well.\n\nNote that this feature is only available in the gradio user interface. Call the APIs as many times as you want for custom batch scheduling.\n\n# API and Script Access\n\nThis extension can accept txt2img or img2img tasks via API or external extension call. Note that you may need to enable `Allow other scripts to control this extension` in settings for external calls.\n\nTo use the API: start WebUI with argument `--api` and go to `http://webui-address/docs` for documents or checkout [examples](https://github.com/Mikubill/sd-webui-controlnet/blob/main/example/txt2img_example/api_txt2img.py).\n\nTo use external call: Checkout [Wiki](https://github.com/Mikubill/sd-webui-controlnet/wiki/API)\n\n# Command Line Arguments\n\nThis extension adds these command line arguments to the webui:\n\n```\n    --controlnet-dir <path to directory with controlnet models>                                ADD a controlnet models directory\n    --controlnet-annotator-models-path <path to directory with annotator model directories>    SET the directory for annotator models\n    --no-half-controlnet                                                                       load controlnet models in full precision\n    --controlnet-preprocessor-cache-size                                                       Cache size for controlnet preprocessor results\n    --controlnet-loglevel                                                                      Log level for the controlnet extension\n    --controlnet-tracemalloc                                                                   Enable malloc memory tracing\n```\n\n# MacOS Support\n\nTested with pytorch nightly: https://github.com/Mikubill/sd-webui-controlnet/pull/143#issuecomment-1435058285\n\nTo use this extension with mps and normal pytorch, currently you may need to start WebUI with `--no-half`.\n\n# Archive of Deprecated Versions\n\nThe previous version (sd-webui-controlnet 1.0) is archived in \n\nhttps://github.com/lllyasviel/webui-controlnet-v1-archived\n\nUsing this version is not a temporary stop of updates. You will stop all updates forever.\n\nPlease consider this version if you work with professional studios that requires 100% reproducing of all previous results pixel by pixel.\n\n# Thanks\n\nThis implementation is inspired by kohya-ss/sd-webui-additional-networks\n",
        "releases": []
    }
}