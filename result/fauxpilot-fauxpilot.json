{
    "https://api.github.com/repos/fauxpilot/fauxpilot": {
        "forks": 631,
        "watchers": 14642,
        "stars": 14642,
        "languages": {
            "Python": 44140,
            "Shell": 8328,
            "Dockerfile": 2503
        },
        "commits": [
            "2023-05-29T20:00:43Z",
            "2023-04-27T04:22:12Z",
            "2023-03-28T08:06:20Z",
            "2023-03-28T08:02:03Z",
            "2023-03-15T10:20:15Z",
            "2023-03-13T09:00:01Z",
            "2023-02-21T18:18:54Z",
            "2023-02-15T08:17:07Z",
            "2023-02-14T10:45:15Z",
            "2023-02-13T17:29:29Z",
            "2023-02-13T15:15:37Z",
            "2023-02-13T15:09:53Z",
            "2023-02-13T15:07:45Z",
            "2023-02-13T14:19:21Z",
            "2023-02-13T14:28:07Z",
            "2023-02-13T14:27:56Z",
            "2023-02-13T14:25:14Z",
            "2023-02-13T14:24:18Z",
            "2023-01-31T08:20:38Z",
            "2023-01-31T08:15:02Z",
            "2023-01-30T15:02:50Z",
            "2023-01-24T18:10:44Z",
            "2023-01-12T01:43:06Z",
            "2023-01-11T06:51:36Z",
            "2023-01-04T00:11:55Z",
            "2023-01-03T11:18:53Z",
            "2023-01-03T05:51:16Z",
            "2023-01-03T02:28:34Z",
            "2023-01-02T13:24:51Z",
            "2023-01-02T10:04:44Z"
        ],
        "creation_date": "2022-08-03T02:14:22Z",
        "contributors": 13,
        "topics": [],
        "subscribers": 121,
        "readme": "\n# FauxPilot\n\nThis is an attempt to build a locally hosted alternative to [GitHub Copilot](https://copilot.github.com/). It uses the [SalesForce CodeGen](https://github.com/salesforce/CodeGen) models inside of NVIDIA's [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) with the [FasterTransformer backend](https://github.com/triton-inference-server/fastertransformer_backend/).\n\n<p align=\"right\">\n  <img width=\"50%\" align=\"right\" src=\"./img/fauxpilot.png\">\n</p>\n\n## Prerequisites\n\nYou'll need:\n\n* Docker\n* `docker compose` >= 1.28\n* An NVIDIA GPU with Compute Capability >= 6.0 and enough VRAM to run the model you want.\n* [`nvidia-docker`](https://github.com/NVIDIA/nvidia-docker)\n* `curl` and `zstd` for downloading and unpacking the models.\n\nNote that the VRAM requirements listed by `setup.sh` are *total* -- if you have multiple GPUs, you can split the model across them. So, if you have two NVIDIA RTX 3080 GPUs, you *should* be able to run the 6B model by putting half on each GPU.\n\n\n## Support and Warranty\n\nlmao\n\nOkay, fine, we now have some minimal information on [the wiki](https://github.com/moyix/fauxpilot/wiki) and a [discussion forum](https://github.com/moyix/fauxpilot/discussions) where you can ask questions. Still no formal support or warranty though!\n\n\n\n## Setup\n\nThis section describes how to install a Fauxpilot server and clients.\n\n### Setting up a FauxPilot Server\n\nRun the setup script to choose a model to use. This will download the model from [Huggingface/Moyix](https://huggingface.co/Moyix) in GPT-J format and then convert it for use with FasterTransformer.\n\nPlease refer to [How to set-up a FauxPilot server](documentation/server.md).\n\n\n### Client configuration for FauxPilot\n\nWe offer some ways to connect to FauxPilot Server. For example, you can create a client by how to open the Openai API, Copilot Plugin, REST API.\n\nPlease refer to [How to set-up a client](documentation/client.md).\n\n\n## Terminology\n * API: Application Programming Interface\n * CC: Compute Capability\n * CUDA: Compute Unified Device Architecture\n * FT: Faster Transformer\n * JSON: JavaScript Object Notation \n * gRPC: Remote Procedure call by Google\n * GPT-J: A transformer model trained using Ben Wang's Mesh Transformer JAX \n * REST: REpresentational State Transfer\n",
        "releases": []
    }
}