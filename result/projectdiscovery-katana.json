{
    "https://api.github.com/repos/projectdiscovery/katana": {
        "forks": 673,
        "watchers": 12893,
        "stars": 12893,
        "languages": {
            "Go": 243821,
            "Shell": 880,
            "Makefile": 450,
            "Dockerfile": 300
        },
        "commits": [
            "2024-12-06T12:06:49Z",
            "2024-12-05T14:06:19Z",
            "2024-12-02T10:13:41Z",
            "2024-12-02T09:09:45Z",
            "2024-12-02T08:28:21Z",
            "2024-12-02T08:28:02Z",
            "2024-12-02T08:15:47Z",
            "2024-12-02T08:15:20Z",
            "2024-12-02T08:14:58Z",
            "2024-12-02T08:14:24Z",
            "2024-12-02T08:13:58Z",
            "2024-12-02T08:02:24Z",
            "2024-12-02T08:02:18Z",
            "2024-12-02T08:02:14Z",
            "2024-12-02T07:53:20Z",
            "2024-11-29T11:52:03Z",
            "2024-11-25T13:39:56Z",
            "2024-11-25T13:38:21Z",
            "2024-11-25T13:15:52Z",
            "2024-11-25T13:15:46Z",
            "2024-11-25T13:14:32Z",
            "2024-11-25T13:13:39Z",
            "2024-11-25T12:51:04Z",
            "2024-11-25T12:51:00Z",
            "2024-11-18T13:59:55Z",
            "2024-11-18T13:33:57Z",
            "2024-11-18T13:33:36Z",
            "2024-11-18T13:31:37Z",
            "2024-11-18T13:07:13Z",
            "2024-11-18T13:07:12Z"
        ],
        "creation_date": "2021-01-02T16:56:05Z",
        "contributors": 30,
        "topics": [
            "cli",
            "crawler",
            "gocrawler",
            "headless",
            "spider-framework",
            "web-spider"
        ],
        "subscribers": 91,
        "readme": "<h1 align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/8293321/196779266-421c79d4-643a-4f73-9b54-3da379bbac09.png\" alt=\"katana\" width=\"200px\">\n  <br>\n</h1>\n\n<h4 align=\"center\">A next-generation crawling and spidering framework</h4>\n\n<p align=\"center\">\n<a href=\"https://goreportcard.com/report/github.com/projectdiscovery/katana\"><img src=\"https://goreportcard.com/badge/github.com/projectdiscovery/katana\"></a>\n<a href=\"https://github.com/projectdiscovery/katana/issues\"><img src=\"https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat\"></a>\n<a href=\"https://github.com/projectdiscovery/katana/releases\"><img src=\"https://img.shields.io/github/release/projectdiscovery/katana\"></a>\n<a href=\"https://twitter.com/pdiscoveryio\"><img src=\"https://img.shields.io/twitter/follow/pdiscoveryio.svg?logo=twitter\"></a>\n<a href=\"https://discord.gg/projectdiscovery\"><img src=\"https://img.shields.io/discord/695645237418131507.svg?logo=discord\"></a>\n</p>\n\n<p align=\"center\">\n  <a href=\"#features\">Features</a> \u2022\n  <a href=\"#installation\">Installation</a> \u2022\n  <a href=\"#usage\">Usage</a> \u2022\n  <a href=\"#scope-control\">Scope</a> \u2022\n  <a href=\"#crawler-configuration\">Config</a> \u2022\n  <a href=\"#filters\">Filters</a> \u2022\n  <a href=\"https://discord.gg/projectdiscovery\">Join Discord</a>\n</p>\n\n\n# Features\n\n![image](https://user-images.githubusercontent.com/8293321/199371558-daba03b6-bf9c-4883-8506-76497c6c3a44.png)\n\n - Fast And fully configurable web crawling\n - **Standard** and **Headless** mode\n - **JavaScript** parsing / crawling\n - Customizable **automatic form filling**\n - **Scope control** - Preconfigured field / Regex \n - **Customizable output** - Preconfigured fields\n - INPUT - **STDIN**, **URL** and **LIST**\n - OUTPUT - **STDOUT**, **FILE** and **JSON**\n\n\n## Installation\n\nkatana requires **Go 1.18** to install successfully. To install, just run the below command or download pre-compiled binary from [release page](https://github.com/projectdiscovery/katana/releases).\n\n```console\nCGO_ENABLED=1 go install github.com/projectdiscovery/katana/cmd/katana@latest\n```\n\n**More options to install / run katana-**\n\n<details>\n  <summary>Docker</summary>\n\n> To install / update docker to latest tag -\n\n```sh\ndocker pull projectdiscovery/katana:latest\n```\n\n> To run katana in standard mode using docker -\n\n\n```sh\ndocker run projectdiscovery/katana:latest -u https://tesla.com\n```\n\n> To run katana in headless mode using docker -\n\n```sh\ndocker run projectdiscovery/katana:latest -u https://tesla.com -system-chrome -headless\n```\n\n</details>\n\n<details>\n  <summary>Ubuntu</summary>\n\n> It's recommended to install the following prerequisites -\n\n```sh\nsudo apt update\nsudo snap refresh\nsudo apt install zip curl wget git\nsudo snap install golang --classic\nwget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add - \nsudo sh -c 'echo \"deb http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google.list'\nsudo apt update \nsudo apt install google-chrome-stable\n```\n\n> install katana -\n\n\n```sh\ngo install github.com/projectdiscovery/katana/cmd/katana@latest\n```\n\n</details>\n\n## Usage\n\n```console\nkatana -h\n```\n\nThis will display help for the tool. Here are all the switches it supports.\n\n```console\nKatana is a fast crawler focused on execution in automation\npipelines offering both headless and non-headless crawling.\n\nUsage:\n  ./katana [flags]\n\nFlags:\nINPUT:\n   -u, -list string[]  target url / list to crawl\n   -resume string      resume scan using resume.cfg\n   -e, -exclude string[]  exclude host matching specified filter ('cdn', 'private-ips', cidr, ip, regex)\n\nCONFIGURATION:\n   -r, -resolvers string[]       list of custom resolver (file or comma separated)\n   -d, -depth int                maximum depth to crawl (default 3)\n   -jc, -js-crawl                enable endpoint parsing / crawling in javascript file\n   -jsl, -jsluice                enable jsluice parsing in javascript file (memory intensive)\n   -ct, -crawl-duration value    maximum duration to crawl the target for (s, m, h, d) (default s)\n   -kf, -known-files string      enable crawling of known files (all,robotstxt,sitemapxml), a minimum depth of 3 is required to ensure all known files are properly crawled.\n   -mrs, -max-response-size int  maximum response size to read (default 9223372036854775807)\n   -timeout int                  time to wait for request in seconds (default 10)\n   -aff, -automatic-form-fill    enable automatic form filling (experimental)\n   -fx, -form-extraction         extract form, input, textarea & select elements in jsonl output\n   -retry int                    number of times to retry the request (default 1)\n   -proxy string                 http/socks5 proxy to use\n   -H, -headers string[]         custom header/cookie to include in all http request in header:value format (file)\n   -config string                path to the katana configuration file\n   -fc, -form-config string      path to custom form configuration file\n   -flc, -field-config string    path to custom field configuration file\n   -s, -strategy string          Visit strategy (depth-first, breadth-first) (default \"depth-first\")\n   -iqp, -ignore-query-params    Ignore crawling same path with different query-param values\n   -tlsi, -tls-impersonate       enable experimental client hello (ja3) tls randomization\n   -dr, -disable-redirects       disable following redirects (default false)\n\nDEBUG:\n   -health-check, -hc        run diagnostic check up\n   -elog, -error-log string  file to write sent requests error log\n\nHEADLESS:\n   -hl, -headless                    enable headless hybrid crawling (experimental)\n   -sc, -system-chrome               use local installed chrome browser instead of katana installed\n   -sb, -show-browser                show the browser on the screen with headless mode\n   -ho, -headless-options string[]   start headless chrome with additional options\n   -nos, -no-sandbox                 start headless chrome in --no-sandbox mode\n   -cdd, -chrome-data-dir string     path to store chrome browser data\n   -scp, -system-chrome-path string  use specified chrome browser for headless crawling\n   -noi, -no-incognito               start headless chrome without incognito mode\n   -cwu, -chrome-ws-url string       use chrome browser instance launched elsewhere with the debugger listening at this URL\n   -xhr, -xhr-extraction             extract xhr request url,method in jsonl output\n\nSCOPE:\n   -cs, -crawl-scope string[]       in scope url regex to be followed by crawler\n   -cos, -crawl-out-scope string[]  out of scope url regex to be excluded by crawler\n   -fs, -field-scope string         pre-defined scope field (dn,rdn,fqdn) or custom regex (e.g., '(company-staging.io|company.com)') (default \"rdn\")\n   -ns, -no-scope                   disables host based default scope\n   -do, -display-out-scope          display external endpoint from scoped crawling\n\nFILTER:\n   -mr, -match-regex string[]       regex or list of regex to match on output url (cli, file)\n   -fr, -filter-regex string[]      regex or list of regex to filter on output url (cli, file)\n   -f, -field string                field to display in output (url,path,fqdn,rdn,rurl,qurl,qpath,file,ufile,key,value,kv,dir,udir)\n   -sf, -store-field string         field to store in per-host output (url,path,fqdn,rdn,rurl,qurl,qpath,file,ufile,key,value,kv,dir,udir)\n   -em, -extension-match string[]   match output for given extension (eg, -em php,html,js)\n   -ef, -extension-filter string[]  filter output for given extension (eg, -ef png,css)\n   -mdc, -match-condition string    match response with dsl based condition\n   -fdc, -filter-condition string   filter response with dsl based condition\n\nRATE-LIMIT:\n   -c, -concurrency int          number of concurrent fetchers to use (default 10)\n   -p, -parallelism int          number of concurrent inputs to process (default 10)\n   -rd, -delay int               request delay between each request in seconds\n   -rl, -rate-limit int          maximum requests to send per second (default 150)\n   -rlm, -rate-limit-minute int  maximum number of requests to send per minute\n\nUPDATE:\n   -up, -update                 update katana to latest version\n   -duc, -disable-update-check  disable automatic katana update check\n\nOUTPUT:\n   -o, -output string                file to write output to\n   -sr, -store-response              store http requests/responses\n   -srd, -store-response-dir string  store http requests/responses to custom directory\n   -sfd, -store-field-dir string     store per-host field to custom directory\n   -or, -omit-raw                    omit raw requests/responses from jsonl output\n   -ob, -omit-body                   omit response body from jsonl output\n   -j, -jsonl                        write output in jsonl format\n   -nc, -no-color                    disable output content coloring (ANSI escape codes)\n   -silent                           display output only\n   -v, -verbose                      display verbose output\n   -debug                            display debug output\n   -version                          display project version\n```\n\n## Running Katana\n\n### Input for katana\n\n**katana** requires **url** or **endpoint** to crawl and accepts single or multiple inputs.\n\nInput URL can be provided using `-u` option, and multiple values can be provided using comma-separated input, similarly **file** input is supported using `-list` option and additionally piped input (stdin) is also supported.\n\n#### URL Input\n\n```sh\nkatana -u https://tesla.com\n```\n\n#### Multiple URL Input (comma-separated)\n\n```sh\nkatana -u https://tesla.com,https://google.com\n```\n\n#### List Input\n```bash\n$ cat url_list.txt\n\nhttps://tesla.com\nhttps://google.com\n```\n\n```\nkatana -list url_list.txt\n```\n\n#### STDIN (piped) Input\n\n```sh\necho https://tesla.com | katana\n```\n\n```sh\ncat domains | httpx | katana\n```\n\nExample running katana -\n\n```console\nkatana -u https://youtube.com\n\n   __        __                \n  / /_____ _/ /____ ____  ___ _\n /  '_/ _  / __/ _  / _ \\/ _  /\n/_/\\_\\\\_,_/\\__/\\_,_/_//_/\\_,_/ v0.0.1                     \n\n      projectdiscovery.io\n\n[WRN] Use with caution. You are responsible for your actions.\n[WRN] Developers assume no liability and are not responsible for any misuse or damage.\nhttps://www.youtube.com/\nhttps://www.youtube.com/about/\nhttps://www.youtube.com/about/press/\nhttps://www.youtube.com/about/copyright/\nhttps://www.youtube.com/t/contact_us/\nhttps://www.youtube.com/creators/\nhttps://www.youtube.com/ads/\nhttps://www.youtube.com/t/terms\nhttps://www.youtube.com/t/privacy\nhttps://www.youtube.com/about/policies/\nhttps://www.youtube.com/howyoutubeworks?utm_campaign=ytgen&utm_source=ythp&utm_medium=LeftNav&utm_content=txt&u=https%3A%2F%2Fwww.youtube.com%2Fhowyoutubeworks%3Futm_source%3Dythp%26utm_medium%3DLeftNav%26utm_campaign%3Dytgen\nhttps://www.youtube.com/new\nhttps://m.youtube.com/\nhttps://www.youtube.com/s/desktop/4965577f/jsbin/desktop_polymer.vflset/desktop_polymer.js\nhttps://www.youtube.com/s/desktop/4965577f/cssbin/www-main-desktop-home-page-skeleton.css\nhttps://www.youtube.com/s/desktop/4965577f/cssbin/www-onepick.css\nhttps://www.youtube.com/s/_/ytmainappweb/_/ss/k=ytmainappweb.kevlar_base.0Zo5FUcPkCg.L.B1.O/am=gAE/d=0/rs=AGKMywG5nh5Qp-BGPbOaI1evhF5BVGRZGA\nhttps://www.youtube.com/opensearch?locale=en_GB\nhttps://www.youtube.com/manifest.webmanifest\nhttps://www.youtube.com/s/desktop/4965577f/cssbin/www-main-desktop-watch-page-skeleton.css\nhttps://www.youtube.com/s/desktop/4965577f/jsbin/web-animations-next-lite.min.vflset/web-animations-next-lite.min.js\nhttps://www.youtube.com/s/desktop/4965577f/jsbin/custom-elements-es5-adapter.vflset/custom-elements-es5-adapter.js\nhttps://www.youtube.com/s/desktop/4965577f/jsbin/webcomponents-sd.vflset/webcomponents-sd.js\nhttps://www.youtube.com/s/desktop/4965577f/jsbin/intersection-observer.min.vflset/intersection-observer.min.js\nhttps://www.youtube.com/s/desktop/4965577f/jsbin/scheduler.vflset/scheduler.js\nhttps://www.youtube.com/s/desktop/4965577f/jsbin/www-i18n-constants-en_GB.vflset/www-i18n-constants.js\nhttps://www.youtube.com/s/desktop/4965577f/jsbin/www-tampering.vflset/www-tampering.js\nhttps://www.youtube.com/s/desktop/4965577f/jsbin/spf.vflset/spf.js\nhttps://www.youtube.com/s/desktop/4965577f/jsbin/network.vflset/network.js\nhttps://www.youtube.com/howyoutubeworks/\nhttps://www.youtube.com/trends/\nhttps://www.youtube.com/jobs/\nhttps://www.youtube.com/kids/\n```\n\n\n## Crawling Mode\n\n### Standard Mode\n\nStandard crawling modality uses the standard go http library under the hood to handle HTTP requests/responses. This modality is much faster as it doesn't have the browser overhead. Still, it analyzes HTTP responses body as is, without any javascript or DOM rendering, potentially missing post-dom-rendered endpoints or asynchronous endpoint calls that might happen in complex web applications depending, for example, on browser-specific events.\n\n### Headless Mode\n\nHeadless mode hooks internal headless calls to handle HTTP requests/responses directly within the browser context. This offers two advantages:\n- The HTTP fingerprint (TLS and user agent) fully identify the client as a legitimate browser\n- Better coverage since the endpoints are discovered analyzing the standard raw response, as in the previous modality, and also the browser-rendered one with javascript enabled.\n\nHeadless crawling is optional and can be enabled using `-headless` option.\n\nHere are other headless CLI options -\n\n```console\nkatana -h headless\n\nFlags:\nHEADLESS:\n   -hl, -headless                    enable headless hybrid crawling (experimental)\n   -sc, -system-chrome               use local installed chrome browser instead of katana installed\n   -sb, -show-browser                show the browser on the screen with headless mode\n   -ho, -headless-options string[]   start headless chrome with additional options\n   -nos, -no-sandbox                 start headless chrome in --no-sandbox mode\n   -cdd, -chrome-data-dir string     path to store chrome browser data\n   -scp, -system-chrome-path string  use specified chrome browser for headless crawling\n   -noi, -no-incognito               start headless chrome without incognito mode\n   -cwu, -chrome-ws-url string       use chrome browser instance launched elsewhere with the debugger listening at this URL\n   -xhr, -xhr-extraction             extract xhr requests\n```\n\n*`-no-sandbox`*\n----\n\nRuns headless chrome browser with **no-sandbox** option, useful when running as root user.\n\n```console\nkatana -u https://tesla.com -headless -no-sandbox\n```\n\n*`-no-incognito`*\n----\n\nRuns headless chrome browser without incognito mode, useful when using the local browser.\n\n```console\nkatana -u https://tesla.com -headless -no-incognito\n```\n\n*`-headless-options`*\n----\n\nWhen crawling in headless mode, additional chrome options can be specified using `-headless-options`, for example -\n\n\n```console\nkatana -u https://tesla.com -headless -system-chrome -headless-options --disable-gpu,proxy-server=http://127.0.0.1:8080\n```\n\n\n## Scope Control\n\nCrawling can be endless if not scoped, as such katana comes with multiple support to define the crawl scope.\n\n*`-field-scope`*\n----\nMost handy option to define scope with predefined field name, `rdn` being default option for field scope.\n\n   - `rdn` - crawling scoped to root domain name and all subdomains (e.g. `*example.com`) (default)\n   - `fqdn` - crawling scoped to given sub(domain) (e.g. `www.example.com` or `api.example.com`)\n   - `dn` - crawling scoped to domain name keyword (e.g. `example`)\n\n```console\nkatana -u https://tesla.com -fs dn\n```\n\n\n*`-crawl-scope`*\n------\n\nFor advanced scope control, `-cs` option can be used that comes with **regex** support.\n\n```console\nkatana -u https://tesla.com -cs login\n```\n\nFor multiple in scope rules, file input with multiline string / regex can be passed.\n\n```bash\n$ cat in_scope.txt\n\nlogin/\nadmin/\napp/\nwordpress/\n```\n\n```console\nkatana -u https://tesla.com -cs in_scope.txt\n```\n\n\n*`-crawl-out-scope`*\n-----\n\nFor defining what not to crawl, `-cos` option can be used and also support **regex** input.\n\n```console\nkatana -u https://tesla.com -cos logout\n```\n\nFor multiple out of scope rules, file input with multiline string / regex can be passed.\n\n```bash\n$ cat out_of_scope.txt\n\n/logout\n/log_out\n```\n\n```console\nkatana -u https://tesla.com -cos out_of_scope.txt\n```\n\n*`-no-scope`*\n----\n\nKatana is default to scope `*.domain`, to disable this `-ns` option can be used and also to crawl the internet.\n\n```console\nkatana -u https://tesla.com -ns\n```\n\n*`-display-out-scope`*\n----\n\nAs default, when scope option is used, it also applies for the links to display as output, as such **external URLs are default to exclude** and to overwrite this behavior, `-do` option can be used to display all the external URLs that exist in targets scoped URL / Endpoint.\n\n```\nkatana -u https://tesla.com -do\n```\n\nHere is all the CLI options for the scope control -\n\n\n```console\nkatana -h scope\n\nFlags:\nSCOPE:\n   -cs, -crawl-scope string[]       in scope url regex to be followed by crawler\n   -cos, -crawl-out-scope string[]  out of scope url regex to be excluded by crawler\n   -fs, -field-scope string         pre-defined scope field (dn,rdn,fqdn) (default \"rdn\")\n   -ns, -no-scope                   disables host based default scope\n   -do, -display-out-scope          display external endpoint from scoped crawling\n```\n\n\n## Crawler Configuration\n\nKatana comes with multiple options to configure and control the crawl as the way we want.\n\n*`-depth`*\n----\n\nOption to define the `depth` to follow the urls for crawling, the more depth the more number of endpoint being crawled + time for crawl.\n\n```\nkatana -u https://tesla.com -d 5\n```\n\n*`-js-crawl`*\n----\n\nOption to enable JavaScript file parsing + crawling the endpoints discovered in JavaScript files, disabled as default.\n\n```\nkatana -u https://tesla.com -jc\n```\n\n*`-crawl-duration`*\n----\n\nOption to predefined crawl duration, disabled as default.\n\n```\nkatana -u https://tesla.com -ct 2\n```\n\n*`-known-files`*\n----\nOption to enable crawling `robots.txt` and `sitemap.xml` file, disabled as default.\n\n```\nkatana -u https://tesla.com -kf robotstxt,sitemapxml\n```\n\n*`-automatic-form-fill`*\n----\n\nOption to enable automatic form filling for known / unknown fields, known field values can be customized as needed by updating form config file at `$HOME/.config/katana/form-config.yaml`.\n\nAutomatic form filling is experimental feature.\n\n```\nkatana -u https://tesla.com -aff\n```\n\n## Authenticated Crawling\n\nAuthenticated crawling involves including custom headers or cookies in HTTP requests to access protected resources. These headers provide authentication or authorization information, allowing you to crawl authenticated content / endpoint. You can specify headers directly in the command line or provide them as a file with katana to perfrom authenticated crawling.\n\n> **Note**: User needs to be manually perform the authentication and export the session cookie / header to file to use with katana.\n\n*`-headers`*\n----\n\nOption to add a custom header or cookie to the request. \n> Syntax of [headers](https://datatracker.ietf.org/doc/html/rfc7230#section-3.2) in the HTTP specification\n\nHere is an example of adding a cookie to the request:\n```\nkatana -u https://tesla.com -H 'Cookie: usrsess=AmljNrESo'\n```\n\nIt is also possible to supply headers or cookies as a file. For example:\n\n```\n$ cat cookie.txt\n\nCookie: PHPSESSIONID=XXXXXXXXX\nX-API-KEY: XXXXX\nTOKEN=XX\n```\n\n```\nkatana -u https://tesla.com -H cookie.txt\n```\n\n\nThere are more options to configure when needed, here is all the config related CLI options - \n\n```console\nkatana -h config\n\nFlags:\nCONFIGURATION:\n   -r, -resolvers string[]       list of custom resolver (file or comma separated)\n   -d, -depth int                maximum depth to crawl (default 3)\n   -jc, -js-crawl                enable endpoint parsing / crawling in javascript file\n   -ct, -crawl-duration int      maximum duration to crawl the target for\n   -kf, -known-files string      enable crawling of known files (all,robotstxt,sitemapxml)\n   -mrs, -max-response-size int  maximum response size to read (default 9223372036854775807)\n   -timeout int                  time to wait for request in seconds (default 10)\n   -aff, -automatic-form-fill    enable automatic form filling (experimental)\n   -fx, -form-extraction         enable extraction of form, input, textarea & select elements\n   -retry int                    number of times to retry the request (default 1)\n   -proxy string                 http/socks5 proxy to use\n   -H, -headers string[]         custom header/cookie to include in request\n   -config string                path to the katana configuration file\n   -fc, -form-config string      path to custom form configuration file\n   -flc, -field-config string    path to custom field configuration file\n   -s, -strategy string          Visit strategy (depth-first, breadth-first) (default \"depth-first\")\n```\n\n### Connecting to Active Browser Session\n\nKatana can also connect to active browser session where user is already logged in and authenticated. and use it for crawling. The only requirement for this is to start browser with remote debugging enabled.\n\nHere is an example of starting chrome browser with remote debugging enabled and using it with katana -\n\n**step 1) First Locate path of chrome executable**\n\n| Operating System | Chromium Executable Location | Google Chrome Executable Location |\n|------------------|------------------------------|-----------------------------------|\n| Windows (64-bit) | `C:\\Program Files (x86)\\Google\\Chromium\\Application\\chrome.exe` | `C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe` |\n| Windows (32-bit) | `C:\\Program Files\\Google\\Chromium\\Application\\chrome.exe` | `C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe` |\n| macOS | `/Applications/Chromium.app/Contents/MacOS/Chromium` | `/Applications/Google Chrome.app/Contents/MacOS/Google Chrome` |\n| Linux | `/usr/bin/chromium` | `/usr/bin/google-chrome` |\n\n**step 2) Start chrome with remote debugging enabled and it will return websocker url. For example, on MacOS, you can start chrome with remote debugging enabled using following command** -\n\n```console\n$ /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9222\n\n\nDevTools listening on ws://127.0.0.1:9222/devtools/browser/c5316c9c-19d6-42dc-847a-41d1aeebf7d6\n```\n\n> Now login to the website you want to crawl and keep the browser open.\n\n**step 3) Now use the websocket url with katana to connect to the active browser session and crawl the website**\n\n```console\nkatana -headless -u https://tesla.com -cwu ws://127.0.0.1:9222/devtools/browser/c5316c9c-19d6-42dc-847a-41d1aeebf7d6 -no-incognito\n```\n\n> **Note**: you can use `-cdd` option to specify custom chrome data directory to store browser data and cookies but that does not save session data if cookie is set to `Session` only or expires after certain time.\n\n\n## Filters\n\n*`-field`*\n----\n\nKatana comes with built in fields that can be used to filter the output for the desired information, `-f` option can be used to specify any of the available fields.\n\n```\n   -f, -field string  field to display in output (url,path,fqdn,rdn,rurl,qurl,qpath,file,key,value,kv,dir,udir)\n```\n\nHere is a table with examples of each field and expected output when used - \n\n\n| FIELD   | DESCRIPTION                 | EXAMPLE                                                      |\n| ------- | --------------------------- | ------------------------------------------------------------ |\n| `url`   | URL Endpoint                | `https://admin.projectdiscovery.io/admin/login?user=admin&password=admin` |\n| `qurl`  | URL including query param   | `https://admin.projectdiscovery.io/admin/login.php?user=admin&password=admin` |\n| `qpath` | Path including query param  | `/login?user=admin&password=admin`                           |\n| `path`  | URL Path                    | `https://admin.projectdiscovery.io/admin/login`              |\n| `fqdn`  | Fully Qualified Domain name | `admin.projectdiscovery.io`                                  |\n| `rdn`   | Root Domain name            | `projectdiscovery.io`                                        |\n| `rurl`  | Root URL                    | `https://admin.projectdiscovery.io`                          |\n| `ufile` | URL with File               | `https://admin.projectdiscovery.io/login.js`                 |\n| `file`  | Filename in URL             | `login.php`                                                  |\n| `key`   | Parameter keys in URL       | `user,password`                                              |\n| `value` | Parameter values in URL     | `admin,admin`                                                |\n| `kv`    | Keys=Values in URL          | `user=admin&password=admin`                                  |\n| `dir`   | URL Directory name          | `/admin/`                                                    |\n| `udir`  | URL with Directory          | `https://admin.projectdiscovery.io/admin/`                   |\n\nHere is an example of using field option to only display all the urls with query parameter in it -\n\n```\nkatana -u https://tesla.com -f qurl -silent\n\nhttps://shop.tesla.com/en_au?redirect=no\nhttps://shop.tesla.com/en_nz?redirect=no\nhttps://shop.tesla.com/product/men_s-raven-lightweight-zip-up-bomber-jacket?sku=1740250-00-A\nhttps://shop.tesla.com/product/tesla-shop-gift-card?sku=1767247-00-A\nhttps://shop.tesla.com/product/men_s-chill-crew-neck-sweatshirt?sku=1740176-00-A\nhttps://www.tesla.com/about?redirect=no\nhttps://www.tesla.com/about/legal?redirect=no\nhttps://www.tesla.com/findus/list?redirect=no\n```\n\n### Custom Fields\n\nYou can create custom fields to extract and store specific information from page responses using regex rules. These custom fields are defined using a YAML config file and are loaded from the default location at `$HOME/.config/katana/field-config.yaml`. Alternatively, you can use the `-flc` option to load a custom field config file from a different location.\nHere is example custom field.\n\n```yaml\n- name: email\n  type: regex\n  regex:\n  - '([a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)'\n  - '([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)'\n\n- name: phone\n  type: regex\n  regex:\n  - '\\d{3}-\\d{8}|\\d{4}-\\d{7}'\n```\n\nWhen defining custom fields, following attributes are supported:\n\n- **name** (required)\n\n> The value of **name** attribute is used as the `-field` cli option value.\n\n- **type** (required)\n\n> The type of custom attribute, currenly supported option - `regex` \n\n- **part** (optional)\n\n> The part of the response to extract the information from. The default value is `response`, which includes both the header and body. Other possible values are `header` and `body`.\n\n- group (optional)\n\n> You can use this attribute to select a specific matched group in regex, for example: `group: 1`\n\n#### Running katana using custom field:\n\n```console\nkatana -u https://tesla.com -f email,phone\n```\n\n*`-store-field`*\n---\n\nTo compliment `field` option which is useful to filter output at run time, there is `-sf, -store-fields` option which works exactly like field option except instead of filtering, it stores all the information on the disk under `katana_field` directory sorted by target url. Use `-sfd` or `-store-field-dir` to store data in a different location.\n\n```\nkatana -u https://tesla.com -sf key,fqdn,qurl -silent\n```\n\n```bash\n$ ls katana_field/\n\nhttps_www.tesla.com_fqdn.txt\nhttps_www.tesla.com_key.txt\nhttps_www.tesla.com_qurl.txt\n```\n\nThe `-store-field` option can be useful for collecting information to build a targeted wordlist for various purposes, including but not limited to:\n\n- Identifying the most commonly used parameters\n- Discovering frequently used paths\n- Finding commonly used files\n- Identifying related or unknown subdomains\n\n### Katana Filters\n\n*`-extension-match`*\n---\n\nCrawl output can be easily matched for specific extension using `-em` option to ensure to display only output containing given extension.\n\n```\nkatana -u https://tesla.com -silent -em js,jsp,json\n```\n\n*`-extension-filter`*\n---\n\nCrawl output can be easily filtered for specific extension using `-ef` option which ensure to remove all the urls containing given extension.\n\n```\nkatana -u https://tesla.com -silent -ef css,txt,md\n```\n\n*`-match-regex`*\n---\nThe `-match-regex` or `-mr` flag allows you to filter output URLs using regular expressions. When using this flag, only URLs that match the specified regular expression will be printed in the output.\n\n```\nkatana -u https://tesla.com -mr 'https://shop\\.tesla\\.com/*' -silent\n```\n*`-filter-regex`*\n---\nThe `-filter-regex` or `-fr` flag allows you to filter output URLs using regular expressions. When using this flag, it will skip the URLs that are match the specified regular expression.\n\n```\nkatana -u https://tesla.com -fr 'https://www\\.tesla\\.com/*' -silent\n```\n\n### Advance Filtering\n\nKatana supports DSL-based expressions for advanced matching and filtering capabilities:\n\n- To match endpoints with a 200 status code:\n```shell\nkatana -u https://www.hackerone.com -mdc 'status_code == 200'\n```\n- To match endpoints that contain \"default\" and have a status code other than 403:\n```shell\nkatana -u https://www.hackerone.com -mdc 'contains(endpoint, \"default\") && status_code != 403'\n```\n- To match endpoints with PHP technologies:\n```shell\nkatana -u https://www.hackerone.com -mdc 'contains(to_lower(technologies), \"php\")'\n```\n- To filter out endpoints running on Cloudflare:\n```shell\nkatana -u https://www.hackerone.com -fdc 'contains(to_lower(technologies), \"cloudflare\")'\n```\nDSL functions can be applied to any keys in the jsonl output. For more information on available DSL functions, please visit the [dsl project](https://github.com/projectdiscovery/dsl).\n\nHere are additional filter options -\n\n```console\nkatana -h filter\n\nFlags:\nFILTER:\n   -mr, -match-regex string[]       regex or list of regex to match on output url (cli, file)\n   -fr, -filter-regex string[]      regex or list of regex to filter on output url (cli, file)\n   -f, -field string                field to display in output (url,path,fqdn,rdn,rurl,qurl,qpath,file,ufile,key,value,kv,dir,udir)\n   -sf, -store-field string         field to store in per-host output (url,path,fqdn,rdn,rurl,qurl,qpath,file,ufile,key,value,kv,dir,udir)\n   -em, -extension-match string[]   match output for given extension (eg, -em php,html,js)\n   -ef, -extension-filter string[]  filter output for given extension (eg, -ef png,css)\n   -mdc, -match-condition string    match response with dsl based condition\n   -fdc, -filter-condition string   filter response with dsl based condition\n```\n\n\n## Rate Limit\n\nIt's easy to get blocked / banned while crawling if not following target websites limits, katana comes with multiple option to tune the crawl to go as fast / slow we want.\n\n*`-delay`*\n-----\n\noption to introduce a delay in seconds between each new request katana makes while crawling, disabled as default.\n\n```\nkatana -u https://tesla.com -delay 20\n```\n\n*`-concurrency`*\n-----\noption to control the number of urls per target to fetch at the same time.\n\n```\nkatana -u https://tesla.com -c 20\n```\n\n\n*`-parallelism`*\n-----\noption to define number of target to process at same time from list input.\n\n```\nkatana -u https://tesla.com -p 20\n```\n\n*`-rate-limit`*\n-----\noption to use to define max number of request can go out per second.\n\n```\nkatana -u https://tesla.com -rl 100\n```\n\n*`-rate-limit-minute`*\n-----\noption to use to define max number of request can go out per minute.\n\n```\nkatana -u https://tesla.com -rlm 500\n```\n\nHere is all long / short CLI options for rate limit control -\n\n```console\nkatana -h rate-limit\n\nFlags:\nRATE-LIMIT:\n   -c, -concurrency int          number of concurrent fetchers to use (default 10)\n   -p, -parallelism int          number of concurrent inputs to process (default 10)\n   -rd, -delay int               request delay between each request in seconds\n   -rl, -rate-limit int          maximum requests to send per second (default 150)\n   -rlm, -rate-limit-minute int  maximum number of requests to send per minute\n```\n\n## Output\n\nKatana support both file output in plain text format as well as JSON which includes additional information like, `source`, `tag`, and `attribute` name to co-related the discovered endpoint.\n\n*`-output`*\n\nBy default, katana outputs the crawled endpoints in plain text format. The results can be written to a file by using the -output option.\n\n\n```console\nkatana -u https://example.com -no-scope -output example_endpoints.txt\n```\n\n*`-jsonl`*\n---\n\n```console\nkatana -u https://example.com -jsonl | jq .\n```\n\n```json\n{\n  \"timestamp\": \"2023-03-20T16:23:58.027559+05:30\",\n  \"request\": {\n    \"method\": \"GET\",\n    \"endpoint\": \"https://example.com\",\n    \"raw\": \"GET / HTTP/1.1\\r\\nHost: example.com\\r\\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\\r\\nAccept-Encoding: gzip\\r\\n\\r\\n\"\n  },\n  \"response\": {\n    \"status_code\": 200,\n    \"headers\": {\n      \"accept_ranges\": \"bytes\",\n      \"expires\": \"Mon, 27 Mar 2023 10:53:58 GMT\",\n      \"last_modified\": \"Thu, 17 Oct 2019 07:18:26 GMT\",\n      \"content_type\": \"text/html; charset=UTF-8\",\n      \"server\": \"ECS (dcb/7EA3)\",\n      \"vary\": \"Accept-Encoding\",\n      \"etag\": \"\\\"3147526947\\\"\",\n      \"cache_control\": \"max-age=604800\",\n      \"x_cache\": \"HIT\",\n      \"date\": \"Mon, 20 Mar 2023 10:53:58 GMT\",\n      \"age\": \"331239\"\n    },\n    \"body\": \"<!doctype html>\\n<html>\\n<head>\\n    <title>Example Domain</title>\\n\\n    <meta charset=\\\"utf-8\\\" />\\n    <meta http-equiv=\\\"Content-type\\\" content=\\\"text/html; charset=utf-8\\\" />\\n    <meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1\\\" />\\n    <style type=\\\"text/css\\\">\\n    body {\\n        background-color: #f0f0f2;\\n        margin: 0;\\n        padding: 0;\\n        font-family: -apple-system, system-ui, BlinkMacSystemFont, \\\"Segoe UI\\\", \\\"Open Sans\\\", \\\"Helvetica Neue\\\", Helvetica, Arial, sans-serif;\\n        \\n    }\\n    div {\\n        width: 600px;\\n        margin: 5em auto;\\n        padding: 2em;\\n        background-color: #fdfdff;\\n        border-radius: 0.5em;\\n        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\\n    }\\n    a:link, a:visited {\\n        color: #38488f;\\n        text-decoration: none;\\n    }\\n    @media (max-width: 700px) {\\n        div {\\n            margin: 0 auto;\\n            width: auto;\\n        }\\n    }\\n    </style>    \\n</head>\\n\\n<body>\\n<div>\\n    <h1>Example Domain</h1>\\n    <p>This domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.</p>\\n    <p><a href=\\\"https://www.iana.org/domains/example\\\">More information...</a></p>\\n</div>\\n</body>\\n</html>\\n\",\n    \"technologies\": [\n      \"Azure\",\n      \"Amazon ECS\",\n      \"Amazon Web Services\",\n      \"Docker\",\n      \"Azure CDN\"\n    ],\n    \"raw\": \"HTTP/1.1 200 OK\\r\\nContent-Length: 1256\\r\\nAccept-Ranges: bytes\\r\\nAge: 331239\\r\\nCache-Control: max-age=604800\\r\\nContent-Type: text/html; charset=UTF-8\\r\\nDate: Mon, 20 Mar 2023 10:53:58 GMT\\r\\nEtag: \\\"3147526947\\\"\\r\\nExpires: Mon, 27 Mar 2023 10:53:58 GMT\\r\\nLast-Modified: Thu, 17 Oct 2019 07:18:26 GMT\\r\\nServer: ECS (dcb/7EA3)\\r\\nVary: Accept-Encoding\\r\\nX-Cache: HIT\\r\\n\\r\\n<!doctype html>\\n<html>\\n<head>\\n    <title>Example Domain</title>\\n\\n    <meta charset=\\\"utf-8\\\" />\\n    <meta http-equiv=\\\"Content-type\\\" content=\\\"text/html; charset=utf-8\\\" />\\n    <meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1\\\" />\\n    <style type=\\\"text/css\\\">\\n    body {\\n        background-color: #f0f0f2;\\n        margin: 0;\\n        padding: 0;\\n        font-family: -apple-system, system-ui, BlinkMacSystemFont, \\\"Segoe UI\\\", \\\"Open Sans\\\", \\\"Helvetica Neue\\\", Helvetica, Arial, sans-serif;\\n        \\n    }\\n    div {\\n        width: 600px;\\n        margin: 5em auto;\\n        padding: 2em;\\n        background-color: #fdfdff;\\n        border-radius: 0.5em;\\n        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\\n    }\\n    a:link, a:visited {\\n        color: #38488f;\\n        text-decoration: none;\\n    }\\n    @media (max-width: 700px) {\\n        div {\\n            margin: 0 auto;\\n            width: auto;\\n        }\\n    }\\n    </style>    \\n</head>\\n\\n<body>\\n<div>\\n    <h1>Example Domain</h1>\\n    <p>This domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.</p>\\n    <p><a href=\\\"https://www.iana.org/domains/example\\\">More information...</a></p>\\n</div>\\n</body>\\n</html>\\n\"\n  }\n}\n```\n\n*`-store-response`*\n----\n\nThe `-store-response` option allows for writing all crawled endpoint requests and responses to a text file. When this option is used, text files including the request and response will be written to the **katana_response** directory. If you would like to specify a custom directory, you can use the `-store-response-dir` option.\n\n```console\nkatana -u https://example.com -no-scope -store-response\n```\n\n```bash\n$ cat katana_response/index.txt\n\nkatana_response/example.com/327c3fda87ce286848a574982ddd0b7c7487f816.txt https://example.com (200 OK)\nkatana_response/www.iana.org/bfc096e6dd93b993ca8918bf4c08fdc707a70723.txt http://www.iana.org/domains/reserved (200 OK)\n```\n\n**Note:**\n\n*`-store-response` option is not supported in `-headless` mode.*\n\nHere are additional CLI options related to output -\n\n```console\nkatana -h output\n\nOUTPUT:\n   -o, -output string                file to write output to\n   -sr, -store-response              store http requests/responses\n   -srd, -store-response-dir string  store http requests/responses to custom directory\n   -j, -json                         write output in JSONL(ines) format\n   -nc, -no-color                    disable output content coloring (ANSI escape codes)\n   -silent                           display output only\n   -v, -verbose                      display verbose output\n   -version                          display project version\n```\n\n## Katana as a library\n`katana` can be used as a library by creating an instance of the `Option` struct and populating it with the same options that would be specified via CLI. Using the options you can create `crawlerOptions` and so standard or hybrid `crawler`.\n`crawler.Crawl` method should be called to crawl the input.\n\n```go\npackage main\n\nimport (\n\t\"math\"\n\n\t\"github.com/projectdiscovery/gologger\"\n\t\"github.com/projectdiscovery/katana/pkg/engine/standard\"\n\t\"github.com/projectdiscovery/katana/pkg/output\"\n\t\"github.com/projectdiscovery/katana/pkg/types\"\n)\n\nfunc main() {\n\toptions := &types.Options{\n\t\tMaxDepth:     3,             // Maximum depth to crawl\n\t\tFieldScope:   \"rdn\",         // Crawling Scope Field\n\t\tBodyReadSize: math.MaxInt,   // Maximum response size to read\n\t\tTimeout:      10,            // Timeout is the time to wait for request in seconds\n\t\tConcurrency:  10,            // Concurrency is the number of concurrent crawling goroutines\n\t\tParallelism:  10,            // Parallelism is the number of urls processing goroutines\n\t\tDelay:        0,             // Delay is the delay between each crawl requests in seconds\n\t\tRateLimit:    150,           // Maximum requests to send per second\n\t\tStrategy:     \"depth-first\", // Visit strategy (depth-first, breadth-first)\n\t\tOnResult: func(result output.Result) { // Callback function to execute for result\n\t\t\tgologger.Info().Msg(result.Request.URL)\n\t\t},\n\t}\n\tcrawlerOptions, err := types.NewCrawlerOptions(options)\n\tif err != nil {\n\t\tgologger.Fatal().Msg(err.Error())\n\t}\n\tdefer crawlerOptions.Close()\n\tcrawler, err := standard.New(crawlerOptions)\n\tif err != nil {\n\t\tgologger.Fatal().Msg(err.Error())\n\t}\n\tdefer crawler.Close()\n\tvar input = \"https://www.hackerone.com\"\n\terr = crawler.Crawl(input)\n\tif err != nil {\n\t\tgologger.Warning().Msgf(\"Could not crawl %s: %s\", input, err.Error())\n\t}\n}\n```\n--------\n\n<div align=\"center\">\n\nkatana is made with \u2764\ufe0f by the [projectdiscovery](https://projectdiscovery.io) team and distributed under [MIT License](LICENSE.md).\n\n\n<a href=\"https://discord.gg/projectdiscovery\"><img src=\"https://raw.githubusercontent.com/projectdiscovery/nuclei-burp-plugin/main/static/join-discord.png\" width=\"300\" alt=\"Join Discord\"></a>\n\n</div>\n",
        "releases": [
            {
                "name": "v1.1.2",
                "date": "2024-12-02T10:30:58Z"
            },
            {
                "name": "v1.1.1",
                "date": "2024-10-28T13:38:52Z"
            },
            {
                "name": "v1.1.0",
                "date": "2024-03-26T09:21:33Z"
            },
            {
                "name": "v1.0.5",
                "date": "2024-01-11T22:04:21Z"
            },
            {
                "name": "v1.0.4",
                "date": "2023-09-14T17:35:36Z"
            },
            {
                "name": "v1.0.3",
                "date": "2023-08-01T12:26:06Z"
            },
            {
                "name": "v1.0.2",
                "date": "2023-06-12T22:40:50Z"
            },
            {
                "name": "v1.0.1",
                "date": "2023-04-08T08:37:40Z"
            },
            {
                "name": "v1.0.0",
                "date": "2023-03-20T11:03:10Z"
            },
            {
                "name": "v0.0.3",
                "date": "2023-01-13T13:29:13Z"
            },
            {
                "name": "v0.0.2",
                "date": "2022-11-11T09:42:21Z"
            },
            {
                "name": "v0.0.1",
                "date": "2022-11-07T14:34:55Z"
            }
        ]
    }
}