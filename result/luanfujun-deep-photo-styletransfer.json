{
    "https://api.github.com/repos/luanfujun/deep-photo-styletransfer": {
        "forks": 1403,
        "watchers": 9987,
        "stars": 9987,
        "languages": {
            "MATLAB": 79475,
            "Lua": 39103,
            "Cuda": 15006,
            "Python": 1247,
            "Makefile": 565,
            "Shell": 269,
            "M": 231
        },
        "commits": [
            "2017-07-13T23:14:14Z",
            "2017-07-13T23:13:33Z",
            "2017-07-13T23:11:44Z",
            "2017-07-13T23:10:06Z",
            "2017-07-13T19:18:55Z",
            "2017-07-13T19:18:37Z",
            "2017-07-13T19:18:14Z",
            "2017-07-13T19:17:29Z",
            "2017-07-13T19:16:29Z",
            "2017-04-19T21:52:04Z",
            "2017-04-18T07:23:16Z",
            "2017-04-13T20:38:30Z",
            "2017-04-13T20:36:42Z",
            "2017-04-10T03:27:27Z",
            "2017-04-10T00:28:53Z",
            "2017-04-06T00:44:06Z",
            "2017-04-04T23:42:19Z",
            "2017-04-04T23:39:50Z",
            "2017-03-31T20:46:27Z",
            "2017-03-31T18:28:24Z",
            "2017-03-29T21:56:08Z",
            "2017-03-29T21:01:59Z",
            "2017-03-29T21:01:38Z",
            "2017-03-29T14:54:11Z",
            "2017-03-29T14:22:22Z",
            "2017-03-28T15:15:02Z",
            "2017-03-28T13:34:33Z",
            "2017-03-28T03:09:49Z",
            "2017-03-28T03:07:51Z",
            "2017-03-28T02:59:22Z"
        ],
        "creation_date": "2017-03-22T04:47:29Z",
        "contributors": 5,
        "topics": [],
        "subscribers": 428,
        "readme": "# deep-photo-styletransfer\nCode and data for paper \"[Deep Photo Style Transfer](https://arxiv.org/abs/1703.07511)\"\n\n## Disclaimer \n**This software is published for academic and non-commercial use only.**\n\n## Setup\nThis code is based on torch. It has been tested on Ubuntu 14.04 LTS.\n\nDependencies:\n* [Torch](https://github.com/torch/torch7) (with [matio-ffi](https://github.com/soumith/matio-ffi.torch) and [loadcaffe](https://github.com/szagoruyko/loadcaffe))\n* [Matlab](https://www.mathworks.com/) or [Octave](https://www.gnu.org/software/octave/)\n\nCUDA backend:\n* [CUDA](https://developer.nvidia.com/cuda-downloads)\n* [cudnn](https://developer.nvidia.com/cudnn)\n\nDownload VGG-19:\n```\nsh models/download_models.sh\n```\n\nCompile ``cuda_utils.cu`` (Adjust ``PREFIX`` and ``NVCC_PREFIX`` in ``makefile`` for your machine):\n```\nmake clean && make\n```\n\n## Usage\n### Quick start\nTo generate all results (in ``examples/``) using the provided scripts, simply run\n```\nrun('gen_laplacian/gen_laplacian.m')\n```\nin Matlab or Octave and then\n```\npython gen_all.py\n```\nin Python. The final output will be in ``examples/final_results/``.\n\n### Basic usage\n1. Given input and style images with semantic segmentation masks, put them in ``examples/`` respectively. They will have the following filename form: ``examples/input/in<id>.png``, ``examples/style/tar<id>.png`` and ``examples/segmentation/in<id>.png``, ``examples/segmentation/tar<id>.png``;\n2. Compute the matting Laplacian matrix using ``gen_laplacian/gen_laplacian.m`` in Matlab. The output matrix will have the following filename form: ``gen_laplacian/Input_Laplacian_3x3_1e-7_CSR<id>.mat``; \n\n**Note: Please make sure that the content image resolution is consistent for Matting Laplacian computation in Matlab and style transfer in Torch, otherwise the result won't be correct.**\n\n3. Run the following script to generate segmented intermediate result:\n```\nth neuralstyle_seg.lua -content_image <input> -style_image <style> -content_seg <inputMask> -style_seg <styleMask> -index <id> -serial <intermediate_folder>\n```\n4. Run the following script to generate final result:\n```\nth deepmatting_seg.lua -content_image <input> -style_image <style> -content_seg <inputMask> -style_seg <styleMask> -index <id> -init_image <intermediate_folder/out<id>_t_1000.png> -serial <final_folder> -f_radius 15 -f_edge 0.01\n```\n\nYou can pass `-backend cudnn` and `-cudnn_autotune` to both Lua scripts (step 3.\nand 4.) to potentially improve speed and memory usage. `libcudnn.so` must be in\nyour `LD_LIBRARY_PATH`. This requires [cudnn.torch](https://github.com/soumith/cudnn.torch).\n\n### Image segmentation\n\nNote: In the main paper we generate all comparison results using automatic scene segmentation algorithm modified from [DilatedNet](https://arxiv.org/abs/1606.00915). Manual segmentation enables more diverse tasks hence we provide the masks in ``examples/segmentation/``.\n\nThe mask colors we used (you could add more colors in `ExtractMask` function in two `*.lua` files):\n\n| Color variable  | RGB Value | Hex Value | \n| ------------- | ------------- | ------------- |\n| `blue`  | `0 0 255`  | `0000ff`  | \n| `green`  | `0 255 0`  | `00ff00`  |\n| `black`  | `0 0 0`  | `000000`  |\n| `white`  | `255 255 255`  | `ffffff`  |\n| `red`  | `255 0 0`  | `ff0000`  |\n| `yellow`  | `255 255 0`  | `ffff00`  |\n| `grey`  | `128 128 128`  | `808080`  |\n| `lightblue`  | `0 255 255`  | `00ffff`  |\n| `purple`  | `255 0 255`  | `ff00ff ` |\n\nHere are some automatic and manual tools for creating a segmentation mask for a photo image:\n\n#### Automatic:\n* [MIT Scene Parsing](http://sceneparsing.csail.mit.edu/)\n* [SuperParsing](http://www.cs.unc.edu/~jtighe/Papers/ECCV10/)\n* [Nonparametric Scene Parsing](http://people.csail.mit.edu/celiu/LabelTransfer/)\n* [Berkeley Contour Detection and Image Segmentation Resources](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html)\n* [CRF-RNN for Semantic Image Segmentation](https://github.com/torrvision/crfasrnn)\n* [Selective Search](https://github.com/belltailjp/selective_search_py)\n* [DeepLab-TensorFlow](https://github.com/DrSleep/tensorflow-deeplab-lfov)\n\n#### Manual:\n* [Photoshop Quick Selection Tool](https://helpx.adobe.com/photoshop/using/making-quick-selections.html)\n* [GIMP Selection Tool](https://docs.gimp.org/en/gimp-tools-selection.html)\n* [GIMP G'MIC Interactive Foreground Extraction tool](http://gmic.eu/gimp.shtml)\n\n## Examples\nHere are some results from our algorithm (from left to right are input, style and our output):\n<p align='center'>\n  <img src='examples/input/in3.png' height='194' width='290'/>\n  <img src='examples/style/tar3.png' height='194' width='290'/>\n  <img src='examples/refine_posterization/refine_3.png' height='194' width='290'/>\n</p>\n\n<p align='center'>\n  <img src='examples/input/in4.png' height='194' width='290'/>\n  <img src='examples/style/tar4.png' height='194' width='290'/>\n  <img src='examples/refine_posterization/refine_4.png' height='194' width='290'/>\n</p>\n\n<p align='center'>\n  <img src='examples/input/in13.png' height='194' width='290'/>\n  <img src='examples/style/tar13.png' height='194' width='290'/>\n  <img src='examples/refine_posterization/refine_13.png' height='194' width='290'/>\n</p>\n\n<p align='center'>\n  <img src='examples/input/in9.png' height='194' width='290'/>\n  <img src='examples/style/tar9.png' height='194' width='290'/>\n  <img src='examples/refine_posterization/refine_9.png' height='194' width='290'/>\n</p>\n\n<p align='center'>\n  <img src='examples/input/in20.png' height='194' width='290'/>\n  <img src='examples/style/tar20.png' height='194' width='290'/>\n  <img src='examples/refine_posterization/refine_20.png' height='194' width='290'/>\n</p>\n\n<p align='center'>\n  <img src='examples/input/in1.png' height='194' width='290'/>\n  <img src='examples/style/tar1.png' height='194' width='290'/>\n  <img src='examples/refine_posterization/refine_1.png' height='194' width='290'/>\n</p>\n\n<p align='center'>\n  <img src='examples/input/in39.png' height='194' width='290'/>\n  <img src='examples/style/tar39.png' height='194' width='290'/>\n  <img src='examples/refine_posterization/refine_39.png' height='194' width='290'/>\n</p>\n\n<p align='center'>\n  <img src='examples/input/in57.png' height='194' width='290'/>\n  <img src='examples/style/tar57.png' height='194' width='290'/>\n  <img src='examples/refine_posterization/refine_57.png' height='194' width='290'/>\n</p>\n\n<p align='center'>\n  <img src='examples/input/in47.png' height='194' width='290'/>\n  <img src='examples/style/tar47.png' height='194' width='290'/>\n  <img src='examples/refine_posterization/refine_47.png' height='194' width='290'/>\n</p>\n\n<p align='center'>\n  <img src='examples/input/in58.png' height='194' width='290'/>\n  <img src='examples/style/tar58.png' height='194' width='290'/>\n  <img src='examples/refine_posterization/refine_58.png' height='194' width='290'/>\n</p>\n\n<p align='center'>\n  <img src='examples/input/in51.png' height='194' width='290'/>\n  <img src='examples/style/tar51.png' height='194' width='290'/>\n  <img src='examples/refine_posterization/refine_51.png' height='194' width='290'/>\n</p>\n\n<p align='center'>\n  <img src='examples/input/in7.png' height='194' width='290'/>\n  <img src='examples/style/tar7.png' height='194' width='290'/>\n  <img src='examples/refine_posterization/refine_7.png' height='194' width='290'/>\n</p>\n\n<p align='center'>\n  <img src='examples/input/in23.png' width='290'/>\n  <img src='examples/input/in23.png' width='290'/>\n  <img src='examples/final_results/best23_t_1000.png' width='290'/>\n</p>\n\n<p align='center'>\n  <img src='examples/input/in16.png' height='194' width='290'/>\n  <img src='examples/style/tar16.png' height='194' width='290'/>\n  <img src='examples/refine_posterization/refine_16.png' height='194' width='290'/>\n</p>\n\n<p align='center'>\n  <img src='examples/input/in30.png' height='194' width='290'/>\n  <img src='examples/style/tar30.png' height='194' width='290'/>\n  <img src='examples/refine_posterization/refine_30.png' height='194' width='290'/>\n</p>\n\n<p align='center'>\n  <img src='examples/input/in2.png' height='194' width='290'/>\n  <img src='examples/style/tar2.png' height='194' width='290'/>\n  <img src='examples/final_results/best2_t_1000.png' height='194' width='290'/>\n</p>\n\n<p align='center'>\n  <img src='examples/input/in11.png'  width='290'/>\n  <img src='examples/style/tar11.png' width='290'/>\n  <img src='examples/refine_posterization/refine_11.png'  width='290'/>\n</p>\n\n\n## Acknowledgement\n* Our torch implementation is based on Justin Johnson's [code](https://github.com/jcjohnson/neural-style);\n* We use Anat Levin's Matlab [code](http://www.wisdom.weizmann.ac.il/~levina/matting.tar.gz) to compute the matting Laplacian matrix.\n\n## Citation\nIf you find this work useful for your research, please cite:\n```\n@article{luan2017deep,\n  title={Deep Photo Style Transfer},\n  author={Luan, Fujun and Paris, Sylvain and Shechtman, Eli and Bala, Kavita},\n  journal={arXiv preprint arXiv:1703.07511},\n  year={2017}\n}\n```\n\n## Contact\nFeel free to contact me if there is any question (Fujun Luan fl356@cornell.edu).\n\n",
        "releases": []
    }
}