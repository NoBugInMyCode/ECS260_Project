{
    "https://api.github.com/repos/WongKinYiu/yolov7": {
        "forks": 4259,
        "watchers": 13556,
        "stars": 13556,
        "languages": {
            "Jupyter Notebook": 43823193,
            "Python": 524049,
            "Shell": 2850,
            "Dockerfile": 821
        },
        "commits": [
            "2023-11-03T02:05:01Z",
            "2023-11-03T02:00:11Z",
            "2023-05-30T06:17:22Z",
            "2023-05-30T06:15:49Z",
            "2023-05-30T06:15:05Z",
            "2023-05-30T06:14:10Z",
            "2023-03-04T07:39:12Z",
            "2023-01-01T20:29:01Z",
            "2022-12-29T15:45:30Z",
            "2022-12-28T03:40:57Z",
            "2022-12-28T03:39:34Z",
            "2022-12-27T05:38:33Z",
            "2022-12-02T11:21:44Z",
            "2022-10-07T21:50:04Z",
            "2022-10-04T15:53:54Z",
            "2022-09-28T08:56:31Z",
            "2022-09-16T18:14:01Z",
            "2022-09-12T14:45:16Z",
            "2022-09-12T14:37:21Z",
            "2022-09-12T14:28:19Z",
            "2022-09-12T14:22:26Z",
            "2022-08-23T00:27:01Z",
            "2022-08-23T00:25:08Z",
            "2022-08-16T13:23:16Z",
            "2022-08-15T23:10:07Z",
            "2022-08-12T07:16:17Z",
            "2022-08-12T07:08:57Z",
            "2022-08-11T11:53:09Z",
            "2022-08-11T11:48:22Z",
            "2022-08-11T11:17:24Z"
        ],
        "creation_date": "2022-07-06T15:14:06Z",
        "contributors": 29,
        "topics": [
            "darknet",
            "pytorch",
            "scaled-yolov4",
            "yolor",
            "yolov3",
            "yolov4",
            "yolov7"
        ],
        "subscribers": 110,
        "readme": "# Official YOLOv7\n\nImplementation of paper - [YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors](https://arxiv.org/abs/2207.02696)\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/yolov7-trainable-bag-of-freebies-sets-new/real-time-object-detection-on-coco)](https://paperswithcode.com/sota/real-time-object-detection-on-coco?p=yolov7-trainable-bag-of-freebies-sets-new)\n[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/akhaliq/yolov7)\n<a href=\"https://colab.research.google.com/gist/AlexeyAB/b769f5795e65fdab80086f6cb7940dae/yolov7detection.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n[![arxiv.org](http://img.shields.io/badge/cs.CV-arXiv%3A2207.02696-B31B1B.svg)](https://arxiv.org/abs/2207.02696)\n\n<div align=\"center\">\n    <a href=\"./\">\n        <img src=\"./figure/performance.png\" width=\"79%\"/>\n    </a>\n</div>\n\n## Web Demo\n\n- Integrated into [Huggingface Spaces \ud83e\udd17](https://huggingface.co/spaces/akhaliq/yolov7) using Gradio. Try out the Web Demo [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/akhaliq/yolov7)\n\n## Performance \n\nMS COCO\n\n| Model | Test Size | AP<sup>test</sup> | AP<sub>50</sub><sup>test</sup> | AP<sub>75</sub><sup>test</sup> | batch 1 fps | batch 32 average time |\n| :-- | :-: | :-: | :-: | :-: | :-: | :-: |\n| [**YOLOv7**](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt) | 640 | **51.4%** | **69.7%** | **55.9%** | 161 *fps* | 2.8 *ms* |\n| [**YOLOv7-X**](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7x.pt) | 640 | **53.1%** | **71.2%** | **57.8%** | 114 *fps* | 4.3 *ms* |\n|  |  |  |  |  |  |  |\n| [**YOLOv7-W6**](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6.pt) | 1280 | **54.9%** | **72.6%** | **60.1%** | 84 *fps* | 7.6 *ms* |\n| [**YOLOv7-E6**](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6.pt) | 1280 | **56.0%** | **73.5%** | **61.2%** | 56 *fps* | 12.3 *ms* |\n| [**YOLOv7-D6**](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-d6.pt) | 1280 | **56.6%** | **74.0%** | **61.8%** | 44 *fps* | 15.0 *ms* |\n| [**YOLOv7-E6E**](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e.pt) | 1280 | **56.8%** | **74.4%** | **62.1%** | 36 *fps* | 18.7 *ms* |\n\n## Installation\n\nDocker environment (recommended)\n<details><summary> <b>Expand</b> </summary>\n\n``` shell\n# create the docker container, you can change the share memory size if you have more.\nnvidia-docker run --name yolov7 -it -v your_coco_path/:/coco/ -v your_code_path/:/yolov7 --shm-size=64g nvcr.io/nvidia/pytorch:21.08-py3\n\n# apt install required packages\napt update\napt install -y zip htop screen libgl1-mesa-glx\n\n# pip install required packages\npip install seaborn thop\n\n# go to code folder\ncd /yolov7\n```\n\n</details>\n\n## Testing\n\n[`yolov7.pt`](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt) [`yolov7x.pt`](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7x.pt) [`yolov7-w6.pt`](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6.pt) [`yolov7-e6.pt`](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6.pt) [`yolov7-d6.pt`](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-d6.pt) [`yolov7-e6e.pt`](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e.pt)\n\n``` shell\npython test.py --data data/coco.yaml --img 640 --batch 32 --conf 0.001 --iou 0.65 --device 0 --weights yolov7.pt --name yolov7_640_val\n```\n\nYou will get the results:\n\n```\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.51206\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.69730\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.55521\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.35247\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.55937\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.66693\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.38453\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.63765\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.68772\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.53766\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.73549\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.83868\n```\n\nTo measure accuracy, download [COCO-annotations for Pycocotools](http://images.cocodataset.org/annotations/annotations_trainval2017.zip) to the `./coco/annotations/instances_val2017.json`\n\n## Training\n\nData preparation\n\n``` shell\nbash scripts/get_coco.sh\n```\n\n* Download MS COCO dataset images ([train](http://images.cocodataset.org/zips/train2017.zip), [val](http://images.cocodataset.org/zips/val2017.zip), [test](http://images.cocodataset.org/zips/test2017.zip)) and [labels](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/coco2017labels-segments.zip). If you have previously used a different version of YOLO, we strongly recommend that you delete `train2017.cache` and `val2017.cache` files, and redownload [labels](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/coco2017labels-segments.zip) \n\nSingle GPU training\n\n``` shell\n# train p5 models\npython train.py --workers 8 --device 0 --batch-size 32 --data data/coco.yaml --img 640 640 --cfg cfg/training/yolov7.yaml --weights '' --name yolov7 --hyp data/hyp.scratch.p5.yaml\n\n# train p6 models\npython train_aux.py --workers 8 --device 0 --batch-size 16 --data data/coco.yaml --img 1280 1280 --cfg cfg/training/yolov7-w6.yaml --weights '' --name yolov7-w6 --hyp data/hyp.scratch.p6.yaml\n```\n\nMultiple GPU training\n\n``` shell\n# train p5 models\npython -m torch.distributed.launch --nproc_per_node 4 --master_port 9527 train.py --workers 8 --device 0,1,2,3 --sync-bn --batch-size 128 --data data/coco.yaml --img 640 640 --cfg cfg/training/yolov7.yaml --weights '' --name yolov7 --hyp data/hyp.scratch.p5.yaml\n\n# train p6 models\npython -m torch.distributed.launch --nproc_per_node 8 --master_port 9527 train_aux.py --workers 8 --device 0,1,2,3,4,5,6,7 --sync-bn --batch-size 128 --data data/coco.yaml --img 1280 1280 --cfg cfg/training/yolov7-w6.yaml --weights '' --name yolov7-w6 --hyp data/hyp.scratch.p6.yaml\n```\n\n## Transfer learning\n\n[`yolov7_training.pt`](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt) [`yolov7x_training.pt`](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7x_training.pt) [`yolov7-w6_training.pt`](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6_training.pt) [`yolov7-e6_training.pt`](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6_training.pt) [`yolov7-d6_training.pt`](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-d6_training.pt) [`yolov7-e6e_training.pt`](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e_training.pt)\n\nSingle GPU finetuning for custom dataset\n\n``` shell\n# finetune p5 models\npython train.py --workers 8 --device 0 --batch-size 32 --data data/custom.yaml --img 640 640 --cfg cfg/training/yolov7-custom.yaml --weights 'yolov7_training.pt' --name yolov7-custom --hyp data/hyp.scratch.custom.yaml\n\n# finetune p6 models\npython train_aux.py --workers 8 --device 0 --batch-size 16 --data data/custom.yaml --img 1280 1280 --cfg cfg/training/yolov7-w6-custom.yaml --weights 'yolov7-w6_training.pt' --name yolov7-w6-custom --hyp data/hyp.scratch.custom.yaml\n```\n\n## Re-parameterization\n\nSee [reparameterization.ipynb](tools/reparameterization.ipynb)\n\n## Inference\n\nOn video:\n``` shell\npython detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --source yourvideo.mp4\n```\n\nOn image:\n``` shell\npython detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --source inference/images/horses.jpg\n```\n\n<div align=\"center\">\n    <a href=\"./\">\n        <img src=\"./figure/horses_prediction.jpg\" width=\"59%\"/>\n    </a>\n</div>\n\n\n## Export\n\n**Pytorch to CoreML (and inference on MacOS/iOS)** <a href=\"https://colab.research.google.com/github/WongKinYiu/yolov7/blob/main/tools/YOLOv7CoreML.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n\n**Pytorch to ONNX with NMS (and inference)** <a href=\"https://colab.research.google.com/github/WongKinYiu/yolov7/blob/main/tools/YOLOv7onnx.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n```shell\npython export.py --weights yolov7-tiny.pt --grid --end2end --simplify \\\n        --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 --img-size 640 640 --max-wh 640\n```\n\n**Pytorch to TensorRT with NMS (and inference)** <a href=\"https://colab.research.google.com/github/WongKinYiu/yolov7/blob/main/tools/YOLOv7trt.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n\n```shell\nwget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt\npython export.py --weights ./yolov7-tiny.pt --grid --end2end --simplify --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 --img-size 640 640\ngit clone https://github.com/Linaom1214/tensorrt-python.git\npython ./tensorrt-python/export.py -o yolov7-tiny.onnx -e yolov7-tiny-nms.trt -p fp16\n```\n\n**Pytorch to TensorRT another way** <a href=\"https://colab.research.google.com/gist/AlexeyAB/fcb47ae544cf284eb24d8ad8e880d45c/yolov7trtlinaom.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <details><summary> <b>Expand</b> </summary>\n\n\n```shell\nwget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt\npython export.py --weights yolov7-tiny.pt --grid --include-nms\ngit clone https://github.com/Linaom1214/tensorrt-python.git\npython ./tensorrt-python/export.py -o yolov7-tiny.onnx -e yolov7-tiny-nms.trt -p fp16\n\n# Or use trtexec to convert ONNX to TensorRT engine\n/usr/src/tensorrt/bin/trtexec --onnx=yolov7-tiny.onnx --saveEngine=yolov7-tiny-nms.trt --fp16\n```\n\n</details>\n\nTested with: Python 3.7.13, Pytorch 1.12.0+cu113\n\n## Pose estimation\n\n[`code`](https://github.com/WongKinYiu/yolov7/tree/pose) [`yolov7-w6-pose.pt`](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6-pose.pt)\n\nSee [keypoint.ipynb](https://github.com/WongKinYiu/yolov7/blob/main/tools/keypoint.ipynb).\n\n<div align=\"center\">\n    <a href=\"./\">\n        <img src=\"./figure/pose.png\" width=\"39%\"/>\n    </a>\n</div>\n\n\n## Instance segmentation (with NTU)\n\n[`code`](https://github.com/WongKinYiu/yolov7/tree/mask) [`yolov7-mask.pt`](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-mask.pt)\n\nSee [instance.ipynb](https://github.com/WongKinYiu/yolov7/blob/main/tools/instance.ipynb).\n\n<div align=\"center\">\n    <a href=\"./\">\n        <img src=\"./figure/mask.png\" width=\"59%\"/>\n    </a>\n</div>\n\n## Instance segmentation\n\n[`code`](https://github.com/WongKinYiu/yolov7/tree/u7/seg) [`yolov7-seg.pt`](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-seg.pt)\n\nYOLOv7 for instance segmentation (YOLOR + YOLOv5 + YOLACT)\n\n| Model | Test Size | AP<sup>box</sup> | AP<sub>50</sub><sup>box</sup> | AP<sub>75</sub><sup>box</sup> | AP<sup>mask</sup> | AP<sub>50</sub><sup>mask</sup> | AP<sub>75</sub><sup>mask</sup> |\n| :-- | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n| **YOLOv7-seg** | 640 | **51.4%** | **69.4%** | **55.8%** | **41.5%** | **65.5%** | **43.7%** |\n\n## Anchor free detection head\n\n[`code`](https://github.com/WongKinYiu/yolov7/tree/u6) [`yolov7-u6.pt`](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-u6.pt)\n\nYOLOv7 with decoupled TAL head (YOLOR + YOLOv5 + YOLOv6)\n\n| Model | Test Size | AP<sup>val</sup> | AP<sub>50</sub><sup>val</sup> | AP<sub>75</sub><sup>val</sup> |\n| :-- | :-: | :-: | :-: | :-: |\n| **YOLOv7-u6** | 640 | **52.6%** | **69.7%** | **57.3%** |\n\n\n## Citation\n\n```\n@inproceedings{wang2023yolov7,\n  title={{YOLOv7}: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},\n  author={Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2023}\n}\n```\n\n```\n@article{wang2023designing,\n  title={Designing Network Design Strategies Through Gradient Path Analysis},\n  author={Wang, Chien-Yao and Liao, Hong-Yuan Mark and Yeh, I-Hau},\n  journal={Journal of Information Science and Engineering},\n  year={2023}\n}\n```\n\n\n## Teaser\n\nYOLOv7-semantic & YOLOv7-panoptic & YOLOv7-caption\n\n<div align=\"center\">\n    <a href=\"./\">\n        <img src=\"./figure/tennis.jpg\" width=\"24%\"/>\n    </a>\n    <a href=\"./\">\n        <img src=\"./figure/tennis_semantic.jpg\" width=\"24%\"/>\n    </a>\n    <a href=\"./\">\n        <img src=\"./figure/tennis_panoptic.png\" width=\"24%\"/>\n    </a>\n    <a href=\"./\">\n        <img src=\"./figure/tennis_caption.png\" width=\"24%\"/>\n    </a>\n</div>\n\nYOLOv7-semantic & YOLOv7-detection & YOLOv7-depth (with NTUT)\n\n<div align=\"center\">\n    <a href=\"./\">\n        <img src=\"./figure/yolov7_city.jpg\" width=\"80%\"/>\n    </a>\n</div>\n\nYOLOv7-3d-detection & YOLOv7-lidar & YOLOv7-road (with NTUT)\n\n<div align=\"center\">\n    <a href=\"./\">\n        <img src=\"./figure/yolov7_3d.jpg\" width=\"30%\"/>\n    </a>\n    <a href=\"./\">\n        <img src=\"./figure/yolov7_lidar.jpg\" width=\"30%\"/>\n    </a>\n    <a href=\"./\">\n        <img src=\"./figure/yolov7_road.jpg\" width=\"30%\"/>\n    </a>\n</div>\n\n\n## Acknowledgements\n\n<details><summary> <b>Expand</b> </summary>\n\n* [https://github.com/AlexeyAB/darknet](https://github.com/AlexeyAB/darknet)\n* [https://github.com/WongKinYiu/yolor](https://github.com/WongKinYiu/yolor)\n* [https://github.com/WongKinYiu/PyTorch_YOLOv4](https://github.com/WongKinYiu/PyTorch_YOLOv4)\n* [https://github.com/WongKinYiu/ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4)\n* [https://github.com/Megvii-BaseDetection/YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)\n* [https://github.com/ultralytics/yolov3](https://github.com/ultralytics/yolov3)\n* [https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5)\n* [https://github.com/DingXiaoH/RepVGG](https://github.com/DingXiaoH/RepVGG)\n* [https://github.com/JUGGHM/OREPA_CVPR2022](https://github.com/JUGGHM/OREPA_CVPR2022)\n* [https://github.com/TexasInstruments/edgeai-yolov5/tree/yolo-pose](https://github.com/TexasInstruments/edgeai-yolov5/tree/yolo-pose)\n\n</details>\n",
        "releases": [
            {
                "name": "YOLOv7",
                "date": "2022-07-07T00:25:35Z"
            }
        ]
    }
}