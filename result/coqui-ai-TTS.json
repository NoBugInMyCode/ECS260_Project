{
    "https://api.github.com/repos/coqui-ai/TTS": {
        "forks": 4593,
        "watchers": 37047,
        "stars": 37047,
        "languages": {
            "Python": 2992534,
            "Jupyter Notebook": 244647,
            "HTML": 8448,
            "Shell": 4288,
            "Makefile": 2246,
            "Cython": 1236,
            "Dockerfile": 1015
        },
        "commits": [
            "2024-02-10T14:20:58Z",
            "2023-12-27T16:23:43Z",
            "2023-12-14T17:00:30Z",
            "2023-12-08T07:37:53Z",
            "2023-12-13T07:54:57Z",
            "2023-12-13T07:53:43Z",
            "2023-12-12T15:10:46Z",
            "2023-12-12T15:10:09Z",
            "2023-12-12T15:09:57Z",
            "2023-12-08T07:37:28Z",
            "2023-12-06T20:47:33Z",
            "2023-12-12T12:51:26Z",
            "2023-12-12T12:50:59Z",
            "2023-12-12T12:50:27Z",
            "2023-12-12T12:50:01Z",
            "2023-12-12T12:30:21Z",
            "2023-12-12T12:28:16Z",
            "2023-12-12T12:22:07Z",
            "2023-12-12T12:19:56Z",
            "2023-12-11T22:58:52Z",
            "2023-12-11T22:35:27Z",
            "2023-12-11T22:35:07Z",
            "2023-12-11T22:17:54Z",
            "2023-12-11T22:01:30Z",
            "2023-12-11T21:31:53Z",
            "2023-12-11T21:11:46Z",
            "2023-12-11T19:21:53Z",
            "2023-12-11T17:49:18Z",
            "2023-12-11T17:48:49Z",
            "2023-12-11T17:48:31Z"
        ],
        "creation_date": "2020-05-20T15:45:28Z",
        "contributors": 30,
        "topics": [
            "deep-learning",
            "glow-tts",
            "hifigan",
            "melgan",
            "multi-speaker-tts",
            "python",
            "pytorch",
            "speaker-encoder",
            "speaker-encodings",
            "speech",
            "speech-synthesis",
            "tacotron",
            "text-to-speech",
            "tts",
            "tts-model",
            "vocoder",
            "voice-cloning",
            "voice-conversion",
            "voice-synthesis"
        ],
        "subscribers": 296,
        "readme": "\n## \ud83d\udc38Coqui.ai News\n- \ud83d\udce3 \u24cdTTSv2 is here with 16 languages and better performance across the board.\n- \ud83d\udce3 \u24cdTTS fine-tuning code is out. Check the [example recipes](https://github.com/coqui-ai/TTS/tree/dev/recipes/ljspeech).\n- \ud83d\udce3 \u24cdTTS can now stream with <200ms latency.\n- \ud83d\udce3 \u24cdTTS, our production TTS model that can speak 13 languages, is released [Blog Post](https://coqui.ai/blog/tts/open_xtts), [Demo](https://huggingface.co/spaces/coqui/xtts), [Docs](https://tts.readthedocs.io/en/dev/models/xtts.html)\n- \ud83d\udce3 [\ud83d\udc36Bark](https://github.com/suno-ai/bark) is now available for inference with unconstrained voice cloning. [Docs](https://tts.readthedocs.io/en/dev/models/bark.html)\n- \ud83d\udce3 You can use [~1100 Fairseq models](https://github.com/facebookresearch/fairseq/tree/main/examples/mms) with \ud83d\udc38TTS.\n- \ud83d\udce3 \ud83d\udc38TTS now supports \ud83d\udc22Tortoise with faster inference. [Docs](https://tts.readthedocs.io/en/dev/models/tortoise.html)\n\n<div align=\"center\">\n<img src=\"https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2\" />\n\n## <img src=\"https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png\" height=\"56\"/>\n\n\n**\ud83d\udc38TTS is a library for advanced Text-to-Speech generation.**\n\n\ud83d\ude80 Pretrained models in +1100 languages.\n\n\ud83d\udee0\ufe0f Tools for training new models and fine-tuning existing models in any language.\n\n\ud83d\udcda Utilities for dataset analysis and curation.\n______________________________________________________________________\n\n[![Discord](https://img.shields.io/discord/1037326658807533628?color=%239B59B6&label=chat%20on%20discord)](https://discord.gg/5eXr5seRrv)\n[![License](<https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg>)](https://opensource.org/licenses/MPL-2.0)\n[![PyPI version](https://badge.fury.io/py/TTS.svg)](https://badge.fury.io/py/TTS)\n[![Covenant](https://camo.githubusercontent.com/7d620efaa3eac1c5b060ece5d6aacfcc8b81a74a04d05cd0398689c01c4463bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76322e3025323061646f707465642d6666363962342e737667)](https://github.com/coqui-ai/TTS/blob/master/CODE_OF_CONDUCT.md)\n[![Downloads](https://pepy.tech/badge/tts)](https://pepy.tech/project/tts)\n[![DOI](https://zenodo.org/badge/265612440.svg)](https://zenodo.org/badge/latestdoi/265612440)\n\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/aux_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/style_check.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/text_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/tts_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/vocoder_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests0.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests1.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests2.yml/badge.svg)\n[![Docs](<https://readthedocs.org/projects/tts/badge/?version=latest&style=plastic>)](https://tts.readthedocs.io/en/latest/)\n\n</div>\n\n______________________________________________________________________\n\n## \ud83d\udcac Where to ask questions\nPlease use our dedicated channels for questions and discussion. Help is much more valuable if it's shared publicly so that more people can benefit from it.\n\n| Type                            | Platforms                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udea8 **Bug Reports**              | [GitHub Issue Tracker]                  |\n| \ud83c\udf81 **Feature Requests & Ideas** | [GitHub Issue Tracker]                  |\n| \ud83d\udc69\u200d\ud83d\udcbb **Usage Questions**          | [GitHub Discussions]                    |\n| \ud83d\uddef **General Discussion**       | [GitHub Discussions] or [Discord]   |\n\n[github issue tracker]: https://github.com/coqui-ai/tts/issues\n[github discussions]: https://github.com/coqui-ai/TTS/discussions\n[discord]: https://discord.gg/5eXr5seRrv\n[Tutorials and Examples]: https://github.com/coqui-ai/TTS/wiki/TTS-Notebooks-and-Tutorials\n\n\n## \ud83d\udd17 Links and Resources\n| Type                            | Links                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udcbc **Documentation**              | [ReadTheDocs](https://tts.readthedocs.io/en/latest/)\n| \ud83d\udcbe **Installation**               | [TTS/README.md](https://github.com/coqui-ai/TTS/tree/dev#installation)|\n| \ud83d\udc69\u200d\ud83d\udcbb **Contributing**               | [CONTRIBUTING.md](https://github.com/coqui-ai/TTS/blob/main/CONTRIBUTING.md)|\n| \ud83d\udccc **Road Map**                   | [Main Development Plans](https://github.com/coqui-ai/TTS/issues/378)\n| \ud83d\ude80 **Released Models**            | [TTS Releases](https://github.com/coqui-ai/TTS/releases) and [Experimental Models](https://github.com/coqui-ai/TTS/wiki/Experimental-Released-Models)|\n| \ud83d\udcf0 **Papers**                    | [TTS Papers](https://github.com/erogol/TTS-papers)|\n\n\n## \ud83e\udd47 TTS Performance\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png\" width=\"800\" /></p>\n\nUnderlined \"TTS*\" and \"Judy*\" are **internal** \ud83d\udc38TTS models that are not released open-source. They are here to show the potential. Models prefixed with a dot (.Jofish .Abe and .Janice) are real human voices.\n\n## Features\n- High-performance Deep Learning models for Text2Speech tasks.\n    - Text2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech).\n    - Speaker Encoder to compute speaker embeddings efficiently.\n    - Vocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN)\n- Fast and efficient model training.\n- Detailed training logs on the terminal and Tensorboard.\n- Support for Multi-speaker TTS.\n- Efficient, flexible, lightweight but feature complete `Trainer API`.\n- Released and ready-to-use models.\n- Tools to curate Text2Speech datasets under```dataset_analysis```.\n- Utilities to use and test your models.\n- Modular (but not too much) code base enabling easy implementation of new ideas.\n\n## Model Implementations\n### Spectrogram models\n- Tacotron: [paper](https://arxiv.org/abs/1703.10135)\n- Tacotron2: [paper](https://arxiv.org/abs/1712.05884)\n- Glow-TTS: [paper](https://arxiv.org/abs/2005.11129)\n- Speedy-Speech: [paper](https://arxiv.org/abs/2008.03802)\n- Align-TTS: [paper](https://arxiv.org/abs/2003.01950)\n- FastPitch: [paper](https://arxiv.org/pdf/2006.06873.pdf)\n- FastSpeech: [paper](https://arxiv.org/abs/1905.09263)\n- FastSpeech2: [paper](https://arxiv.org/abs/2006.04558)\n- SC-GlowTTS: [paper](https://arxiv.org/abs/2104.05557)\n- Capacitron: [paper](https://arxiv.org/abs/1906.03402)\n- OverFlow: [paper](https://arxiv.org/abs/2211.06892)\n- Neural HMM TTS: [paper](https://arxiv.org/abs/2108.13320)\n- Delightful TTS: [paper](https://arxiv.org/abs/2110.12612)\n\n### End-to-End Models\n- \u24cdTTS: [blog](https://coqui.ai/blog/tts/open_xtts)\n- VITS: [paper](https://arxiv.org/pdf/2106.06103)\n- \ud83d\udc38 YourTTS: [paper](https://arxiv.org/abs/2112.02418)\n- \ud83d\udc22 Tortoise: [orig. repo](https://github.com/neonbjb/tortoise-tts)\n- \ud83d\udc36 Bark: [orig. repo](https://github.com/suno-ai/bark)\n\n### Attention Methods\n- Guided Attention: [paper](https://arxiv.org/abs/1710.08969)\n- Forward Backward Decoding: [paper](https://arxiv.org/abs/1907.09006)\n- Graves Attention: [paper](https://arxiv.org/abs/1910.10288)\n- Double Decoder Consistency: [blog](https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/)\n- Dynamic Convolutional Attention: [paper](https://arxiv.org/pdf/1910.10288.pdf)\n- Alignment Network: [paper](https://arxiv.org/abs/2108.10447)\n\n### Speaker Encoder\n- GE2E: [paper](https://arxiv.org/abs/1710.10467)\n- Angular Loss: [paper](https://arxiv.org/pdf/2003.11982.pdf)\n\n### Vocoders\n- MelGAN: [paper](https://arxiv.org/abs/1910.06711)\n- MultiBandMelGAN: [paper](https://arxiv.org/abs/2005.05106)\n- ParallelWaveGAN: [paper](https://arxiv.org/abs/1910.11480)\n- GAN-TTS discriminators: [paper](https://arxiv.org/abs/1909.11646)\n- WaveRNN: [origin](https://github.com/fatchord/WaveRNN/)\n- WaveGrad: [paper](https://arxiv.org/abs/2009.00713)\n- HiFiGAN: [paper](https://arxiv.org/abs/2010.05646)\n- UnivNet: [paper](https://arxiv.org/abs/2106.07889)\n\n### Voice Conversion\n- FreeVC: [paper](https://arxiv.org/abs/2210.15418)\n\nYou can also help us implement more models.\n\n## Installation\n\ud83d\udc38TTS is tested on Ubuntu 18.04 with **python >= 3.9, < 3.12.**.\n\nIf you are only interested in [synthesizing speech](https://tts.readthedocs.io/en/latest/inference.html) with the released \ud83d\udc38TTS models, installing from PyPI is the easiest option.\n\n```bash\npip install TTS\n```\n\nIf you plan to code or train models, clone \ud83d\udc38TTS and install it locally.\n\n```bash\ngit clone https://github.com/coqui-ai/TTS\npip install -e .[all,dev,notebooks]  # Select the relevant extras\n```\n\nIf you are on Ubuntu (Debian), you can also run following commands for installation.\n\n```bash\n$ make system-deps  # intended to be used on Ubuntu (Debian). Let us know if you have a different OS.\n$ make install\n```\n\nIf you are on Windows, \ud83d\udc51@GuyPaddock wrote installation instructions [here](https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system).\n\n\n## Docker Image\nYou can also try TTS without install with the docker image.\nSimply run the following command and you will be able to run TTS without installing it.\n\n```bash\ndocker run --rm -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu\npython3 TTS/server/server.py --list_models #To get the list of available models\npython3 TTS/server/server.py --model_name tts_models/en/vctk/vits # To start a server\n```\n\nYou can then enjoy the TTS server [here](http://[::1]:5002/)\nMore details about the docker images (like GPU support) can be found [here](https://tts.readthedocs.io/en/latest/docker_images.html)\n\n\n## Synthesizing speech by \ud83d\udc38TTS\n\n### \ud83d\udc0d Python API\n\n#### Running a multi-speaker and multi-lingual model\n\n```python\nimport torch\nfrom TTS.api import TTS\n\n# Get device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# List available \ud83d\udc38TTS models\nprint(TTS().list_models())\n\n# Init TTS\ntts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n\n# Run TTS\n# \u2757 Since this model is multi-lingual voice cloning model, we must set the target speaker_wav and language\n# Text to speech list of amplitude values as output\nwav = tts.tts(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\")\n# Text to speech to a file\ntts.tts_to_file(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\n```\n\n#### Running a single speaker model\n\n```python\n# Init TTS with the target model name\ntts = TTS(model_name=\"tts_models/de/thorsten/tacotron2-DDC\", progress_bar=False).to(device)\n\n# Run TTS\ntts.tts_to_file(text=\"Ich bin eine Testnachricht.\", file_path=OUTPUT_PATH)\n\n# Example voice cloning with YourTTS in English, French and Portuguese\ntts = TTS(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=False).to(device)\ntts.tts_to_file(\"This is voice cloning.\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\ntts.tts_to_file(\"C'est le clonage de la voix.\", speaker_wav=\"my/cloning/audio.wav\", language=\"fr-fr\", file_path=\"output.wav\")\ntts.tts_to_file(\"Isso \u00e9 clonagem de voz.\", speaker_wav=\"my/cloning/audio.wav\", language=\"pt-br\", file_path=\"output.wav\")\n```\n\n#### Example voice conversion\n\nConverting the voice in `source_wav` to the voice of `target_wav`\n\n```python\ntts = TTS(model_name=\"voice_conversion_models/multilingual/vctk/freevc24\", progress_bar=False).to(\"cuda\")\ntts.voice_conversion_to_file(source_wav=\"my/source.wav\", target_wav=\"my/target.wav\", file_path=\"output.wav\")\n```\n\n#### Example voice cloning together with the voice conversion model.\nThis way, you can clone voices by using any model in \ud83d\udc38TTS.\n\n```python\n\ntts = TTS(\"tts_models/de/thorsten/tacotron2-DDC\")\ntts.tts_with_vc_to_file(\n    \"Wie sage ich auf Italienisch, dass ich dich liebe?\",\n    speaker_wav=\"target/speaker.wav\",\n    file_path=\"output.wav\"\n)\n```\n\n#### Example text to speech using **Fairseq models in ~1100 languages** \ud83e\udd2f.\nFor Fairseq models, use the following name format: `tts_models/<lang-iso_code>/fairseq/vits`.\nYou can find the language ISO codes [here](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)\nand learn about the Fairseq models [here](https://github.com/facebookresearch/fairseq/tree/main/examples/mms).\n\n```python\n# TTS with on the fly voice conversion\napi = TTS(\"tts_models/deu/fairseq/vits\")\napi.tts_with_vc_to_file(\n    \"Wie sage ich auf Italienisch, dass ich dich liebe?\",\n    speaker_wav=\"target/speaker.wav\",\n    file_path=\"output.wav\"\n)\n```\n\n### Command-line `tts`\n\n<!-- begin-tts-readme -->\n\nSynthesize speech on command line.\n\nYou can either use your trained model or choose a model from the provided list.\n\nIf you don't specify any models, then it uses LJSpeech based English model.\n\n#### Single Speaker Models\n\n- List provided models:\n\n  ```\n  $ tts --list_models\n  ```\n\n- Get model info (for both tts_models and vocoder_models):\n\n  - Query by type/name:\n    The model_info_by_name uses the name as it from the --list_models.\n    ```\n    $ tts --model_info_by_name \"<model_type>/<language>/<dataset>/<model_name>\"\n    ```\n    For example:\n    ```\n    $ tts --model_info_by_name tts_models/tr/common-voice/glow-tts\n    $ tts --model_info_by_name vocoder_models/en/ljspeech/hifigan_v2\n    ```\n  - Query by type/idx:\n    The model_query_idx uses the corresponding idx from --list_models.\n\n    ```\n    $ tts --model_info_by_idx \"<model_type>/<model_query_idx>\"\n    ```\n\n    For example:\n\n    ```\n    $ tts --model_info_by_idx tts_models/3\n    ```\n\n  - Query info for model info by full name:\n    ```\n    $ tts --model_info_by_name \"<model_type>/<language>/<dataset>/<model_name>\"\n    ```\n\n- Run TTS with default models:\n\n  ```\n  $ tts --text \"Text for TTS\" --out_path output/path/speech.wav\n  ```\n\n- Run TTS and pipe out the generated TTS wav file data:\n\n  ```\n  $ tts --text \"Text for TTS\" --pipe_out --out_path output/path/speech.wav | aplay\n  ```\n\n- Run a TTS model with its default vocoder model:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"<model_type>/<language>/<dataset>/<model_name>\" --out_path output/path/speech.wav\n  ```\n\n  For example:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"tts_models/en/ljspeech/glow-tts\" --out_path output/path/speech.wav\n  ```\n\n- Run with specific TTS and vocoder models from the list:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"<model_type>/<language>/<dataset>/<model_name>\" --vocoder_name \"<model_type>/<language>/<dataset>/<model_name>\" --out_path output/path/speech.wav\n  ```\n\n  For example:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"tts_models/en/ljspeech/glow-tts\" --vocoder_name \"vocoder_models/en/ljspeech/univnet\" --out_path output/path/speech.wav\n  ```\n\n- Run your own TTS model (Using Griffin-Lim Vocoder):\n\n  ```\n  $ tts --text \"Text for TTS\" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n  ```\n\n- Run your own TTS and Vocoder models:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n      --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json\n  ```\n\n#### Multi-speaker Models\n\n- List the available speakers and choose a <speaker_id> among them:\n\n  ```\n  $ tts --model_name \"<language>/<dataset>/<model_name>\"  --list_speaker_idxs\n  ```\n\n- Run the multi-speaker TTS model with the target speaker ID:\n\n  ```\n  $ tts --text \"Text for TTS.\" --out_path output/path/speech.wav --model_name \"<language>/<dataset>/<model_name>\"  --speaker_idx <speaker_id>\n  ```\n\n- Run your own multi-speaker TTS model:\n\n  ```\n  $ tts --text \"Text for TTS\" --out_path output/path/speech.wav --model_path path/to/model.pth --config_path path/to/config.json --speakers_file_path path/to/speaker.json --speaker_idx <speaker_id>\n  ```\n\n### Voice Conversion Models\n\n```\n$ tts --out_path output/path/speech.wav --model_name \"<language>/<dataset>/<model_name>\" --source_wav <path/to/speaker/wav> --target_wav <path/to/reference/wav>\n```\n\n<!-- end-tts-readme -->\n\n## Directory Structure\n```\n|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)\n|- utils/           (common utilities.)\n|- TTS\n    |- bin/             (folder for all the executables.)\n      |- train*.py                  (train your target model.)\n      |- ...\n    |- tts/             (text to speech models)\n        |- layers/          (model layer definitions)\n        |- models/          (model definitions)\n        |- utils/           (model specific utilities.)\n    |- speaker_encoder/ (Speaker Encoder models.)\n        |- (same)\n    |- vocoder/         (Vocoder models.)\n        |- (same)\n```\n",
        "releases": [
            {
                "name": "v0.22.0",
                "date": "2023-12-12T15:11:16Z"
            },
            {
                "name": "v0.21.3",
                "date": "2023-12-01T22:57:47Z"
            },
            {
                "name": "v0.21.2",
                "date": "2023-11-30T12:05:43Z"
            },
            {
                "name": "v0.21.1",
                "date": "2023-11-24T14:17:44Z"
            },
            {
                "name": "v0.21.0",
                "date": "2023-11-24T13:38:01Z"
            },
            {
                "name": "v0.20.6",
                "date": "2023-11-17T14:47:43Z"
            },
            {
                "name": "v0.20.5",
                "date": "2023-11-15T14:34:56Z"
            },
            {
                "name": "v0.20.4",
                "date": "2023-11-13T18:33:11Z"
            },
            {
                "name": "v0.20.3",
                "date": "2023-11-10T11:00:39Z"
            },
            {
                "name": "v0.20.2",
                "date": "2023-11-08T15:08:59Z"
            },
            {
                "name": "v0.20.1",
                "date": "2023-11-07T13:18:27Z"
            },
            {
                "name": "v0.20.0",
                "date": "2023-11-06T14:53:07Z"
            },
            {
                "name": "v0.19.1",
                "date": "2023-10-30T09:40:13Z"
            },
            {
                "name": "v0.19.0",
                "date": "2023-10-25T12:28:24Z"
            },
            {
                "name": "v0.18.2",
                "date": "2023-10-21T15:30:26Z"
            },
            {
                "name": "v0.18.1",
                "date": "2023-10-20T20:37:26Z"
            },
            {
                "name": "v0.18.0",
                "date": "2023-10-20T14:05:19Z"
            },
            {
                "name": "v0.17.10",
                "date": "2023-10-19T10:01:17Z"
            },
            {
                "name": "v0.17.9",
                "date": "2023-10-19T09:23:18Z"
            },
            {
                "name": "v0.17.8",
                "date": "2023-10-06T22:19:31Z"
            },
            {
                "name": "v0.17.7",
                "date": "2023-10-06T16:37:28Z"
            },
            {
                "name": "v0.17.6",
                "date": "2023-09-29T21:44:45Z"
            },
            {
                "name": "v0.17.5",
                "date": "2023-09-25T16:19:36Z"
            },
            {
                "name": "v0.17.4",
                "date": "2023-09-15T14:41:49Z"
            },
            {
                "name": "v0.17.3",
                "date": "2023-09-14T15:54:21Z"
            },
            {
                "name": "v0.17.2",
                "date": "2023-09-14T13:25:08Z"
            },
            {
                "name": "v0.17.1",
                "date": "2023-09-13T16:24:36Z"
            },
            {
                "name": "\ud83d\udc51v0.17.0",
                "date": "2023-09-13T16:06:55Z"
            },
            {
                "name": "v0.16.6",
                "date": "2023-09-04T10:54:02Z"
            },
            {
                "name": "v0.16.5",
                "date": "2023-08-26T10:01:34Z"
            },
            {
                "name": "v0.16.4",
                "date": "2023-08-26T08:41:38Z"
            },
            {
                "name": "v0.16.3",
                "date": "2023-08-13T10:22:48Z"
            },
            {
                "name": "v0.16.2",
                "date": "2023-08-07T11:21:39Z"
            },
            {
                "name": "v0.16.1",
                "date": "2023-07-31T13:55:20Z"
            },
            {
                "name": "v0.16.0",
                "date": "2023-07-24T14:04:48Z"
            },
            {
                "name": "\ud83d\udee0\ufe0fv0.15.6",
                "date": "2023-07-08T08:33:52Z"
            },
            {
                "name": "\ud83d\udee0\ufe0f v0.15.5",
                "date": "2023-07-03T09:18:36Z"
            },
            {
                "name": "\ud83d\udee0\ufe0fv0.15.4",
                "date": "2023-06-30T14:11:14Z"
            },
            {
                "name": "\ud83d\udee0\ufe0fv0.15.2",
                "date": "2023-06-30T12:18:20Z"
            },
            {
                "name": "\ud83d\udee0\ufe0fv0.15.1",
                "date": "2023-06-29T15:56:03Z"
            },
            {
                "name": "\ud83d\ude04 v0.15.0",
                "date": "2023-06-28T22:46:51Z"
            },
            {
                "name": "\ud83d\udc49 v0.14.3",
                "date": "2023-06-06T07:43:09Z"
            },
            {
                "name": "\u26c8\ufe0f v0.14.2",
                "date": "2023-06-05T20:41:37Z"
            },
            {
                "name": "\ud83d\ude97 v0.14.1",
                "date": "2023-06-05T09:30:45Z"
            },
            {
                "name": "v0.14.0",
                "date": "2023-05-16T08:09:27Z"
            },
            {
                "name": "v0.14.1_models",
                "date": "2023-04-27T14:39:07Z"
            },
            {
                "name": "v0.14.0_models",
                "date": "2023-05-08T10:02:07Z"
            },
            {
                "name": "\ud83d\udc36 v0.13.3",
                "date": "2023-04-17T14:15:53Z"
            },
            {
                "name": "v0.13.3_models",
                "date": "2023-04-17T11:47:47Z"
            },
            {
                "name": "\ud83c\udf08v0.13.2",
                "date": "2023-04-14T08:48:02Z"
            },
            {
                "name": "v0.13.1",
                "date": "2023-04-12T14:59:46Z"
            },
            {
                "name": "v0.13.0",
                "date": "2023-04-05T14:05:42Z"
            },
            {
                "name": "FreeVC Models",
                "date": "2023-03-20T11:48:28Z"
            },
            {
                "name": "v0.12.0",
                "date": "2023-03-17T12:42:49Z"
            },
            {
                "name": "v0.11.1",
                "date": "2023-02-10T16:42:48Z"
            },
            {
                "name": "v0.11.0",
                "date": "2023-02-10T09:28:44Z"
            },
            {
                "name": "v0.11.0_models",
                "date": "2023-01-30T12:57:29Z"
            },
            {
                "name": "v0.10.2",
                "date": "2023-01-11T00:14:22Z"
            },
            {
                "name": "v0.10.1",
                "date": "2022-12-26T14:48:39Z"
            },
            {
                "name": "v0.10.1_models",
                "date": "2022-12-22T12:57:37Z"
            },
            {
                "name": "v0.10.0",
                "date": "2022-12-15T11:03:28Z"
            },
            {
                "name": "v0.10.0_models",
                "date": "2022-12-13T08:24:26Z"
            },
            {
                "name": "v0.9.0",
                "date": "2022-11-16T15:50:13Z"
            },
            {
                "name": "v0.8.0 models",
                "date": "2022-08-22T09:15:28Z"
            },
            {
                "name": "v0.8.0",
                "date": "2022-08-22T12:55:32Z"
            },
            {
                "name": "v0.7.1 models",
                "date": "2022-07-21T09:09:01Z"
            },
            {
                "name": "v0.7.1",
                "date": "2022-06-21T12:12:14Z"
            },
            {
                "name": "v0.7.0",
                "date": "2022-06-20T21:56:17Z"
            },
            {
                "name": "Speaker Encoder Model",
                "date": "2022-05-30T12:56:13Z"
            },
            {
                "name": "v0.7.0 models",
                "date": "2022-05-19T16:32:05Z"
            },
            {
                "name": "v0.6.2",
                "date": "2022-04-20T09:48:23Z"
            },
            {
                "name": "v0.6.2 models",
                "date": "2022-04-19T12:03:35Z"
            },
            {
                "name": "v0.6.1 models",
                "date": "2022-03-30T08:35:09Z"
            },
            {
                "name": "v0.6.1",
                "date": "2022-03-07T15:06:48Z"
            },
            {
                "name": "v0.6.0",
                "date": "2022-03-07T14:44:06Z"
            },
            {
                "name": "v0.6.0 models",
                "date": "2022-03-02T17:03:40Z"
            },
            {
                "name": "v0.5.0",
                "date": "2022-01-03T17:28:41Z"
            },
            {
                "name": "v0.5.0_models",
                "date": "2022-01-01T15:39:35Z"
            },
            {
                "name": "v0.4.2",
                "date": "2021-12-08T15:42:51Z"
            },
            {
                "name": "v0.4.1",
                "date": "2021-10-26T17:55:11Z"
            },
            {
                "name": "v0.4.0",
                "date": "2021-10-26T16:34:16Z"
            },
            {
                "name": "v0.3.1",
                "date": "2021-09-17T23:55:13Z"
            },
            {
                "name": "v0.3.0",
                "date": "2021-09-13T12:13:49Z"
            },
            {
                "name": "v0.2.2",
                "date": "2021-09-06T17:25:58Z"
            },
            {
                "name": "v0.2.1",
                "date": "2021-08-31T10:39:04Z"
            },
            {
                "name": "v0.2.0",
                "date": "2021-08-11T08:43:19Z"
            },
            {
                "name": "v0.1.3",
                "date": "2021-07-26T16:37:32Z"
            },
            {
                "name": "v0.1.2",
                "date": "2021-07-06T13:26:09Z"
            },
            {
                "name": "v0.1.1",
                "date": "2021-07-06T08:32:01Z"
            },
            {
                "name": "v0.1.0",
                "date": "2021-07-03T12:18:49Z"
            },
            {
                "name": "v0.0.15.1 ",
                "date": "2021-06-08T08:57:06Z"
            },
            {
                "name": "v0.0.15",
                "date": "2021-06-04T12:06:00Z"
            },
            {
                "name": "v0.0.14",
                "date": "2021-05-19T12:09:30Z"
            },
            {
                "name": "v0.0.13",
                "date": "2021-04-29T07:17:24Z"
            },
            {
                "name": "v0.0.12",
                "date": "2021-04-15T15:02:29Z"
            },
            {
                "name": "v0.0.11",
                "date": "2021-04-02T14:43:22Z"
            },
            {
                "name": "v0.0.10",
                "date": "2021-03-10T17:11:11Z"
            },
            {
                "name": "v0.0.9 ",
                "date": "2021-03-09T15:51:32Z"
            }
        ]
    }
}