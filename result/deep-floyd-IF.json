{
    "https://api.github.com/repos/deep-floyd/IF": {
        "forks": 505,
        "watchers": 7722,
        "stars": 7722,
        "languages": {
            "Python": 137305
        },
        "commits": [
            "2023-06-02T19:06:46Z",
            "2023-06-02T17:45:18Z",
            "2023-04-29T11:11:18Z",
            "2023-04-28T23:59:00Z",
            "2023-04-28T23:06:29Z",
            "2023-04-28T13:33:53Z",
            "2023-04-28T13:00:34Z",
            "2023-04-28T13:00:03Z",
            "2023-04-28T12:57:56Z",
            "2023-04-28T12:52:12Z",
            "2023-04-28T12:15:57Z",
            "2023-04-28T12:14:07Z",
            "2023-04-28T12:01:32Z",
            "2023-04-28T11:37:30Z",
            "2023-04-28T11:13:30Z",
            "2023-04-28T10:32:17Z",
            "2023-04-28T10:30:19Z",
            "2023-04-28T10:14:40Z",
            "2023-04-28T10:13:25Z",
            "2023-04-27T19:20:32Z",
            "2023-04-27T19:19:53Z",
            "2023-04-27T14:00:59Z",
            "2023-04-27T13:52:19Z",
            "2023-04-27T13:36:55Z",
            "2023-04-27T09:32:53Z",
            "2023-04-27T03:39:18Z",
            "2023-04-26T23:35:01Z",
            "2023-04-26T23:34:23Z",
            "2023-04-26T23:20:00Z",
            "2023-04-26T23:09:24Z"
        ],
        "creation_date": "2023-01-20T16:49:53Z",
        "contributors": 8,
        "topics": [],
        "subscribers": 83,
        "readme": "[![License](https://img.shields.io/badge/Code_License-Modified_MIT-blue.svg)](LICENSE)\n[![License](https://img.shields.io/badge/Weights_License-DeepFloyd_IF-orange.svg)](LICENSE-MODEL)\n[![Downloads](https://pepy.tech/badge/deepfloyd_if)](https://pepy.tech/project/deepfloyd_if)\n[![Discord](https://img.shields.io/badge/Discord-%237289DA.svg?logo=discord&logoColor=white)](https://discord.gg/umz62Mgr)\n[![Twitter](https://img.shields.io/badge/Twitter-%231DA1F2.svg?logo=twitter&logoColor=white)](https://twitter.com/deepfloydai)\n[![Linktree](https://img.shields.io/badge/Linktree-%2339E09B.svg?logo=linktree&logoColor=white)](http://linktr.ee/deepfloyd)\n\n# IF by [DeepFloyd Lab](https://deepfloyd.ai) at [StabilityAI](https://stability.ai/)\n\n<p align=\"center\">\n  <img src=\"./pics/nabla.jpg\" width=\"100%\">\n</p>\n\nWe introduce DeepFloyd IF, a novel state-of-the-art open-source text-to-image model with a high degree of photorealism and language understanding. DeepFloyd IF is a modular composed of a frozen text encoder and three cascaded pixel diffusion modules: a base model that generates 64x64 px image based on text prompt and two super-resolution models, each designed to generate images of increasing resolution: 256x256 px and 1024x1024 px. All stages of the model utilize a frozen text encoder based on the T5 transformer to extract text embeddings, which are then fed into a UNet architecture enhanced with cross-attention and attention pooling. The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID score of 6.66 on the COCO dataset. Our work underscores the potential of larger UNet architectures in the first stage of cascaded diffusion models and depicts a promising future for text-to-image synthesis.\n\n<p align=\"center\">\n  <img src=\"./pics/deepfloyd_if_scheme.jpg\" width=\"100%\">\n</p>\n\n*Inspired by* [*Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding*](https://arxiv.org/pdf/2205.11487.pdf)\n\n## Minimum requirements to use all IF models:\n- 16GB vRAM for IF-I-XL (4.3B text to 64x64 base module) & IF-II-L (1.2B to 256x256 upscaler module)\n- 24GB vRAM for IF-I-XL (4.3B text to 64x64 base module) & IF-II-L (1.2B to 256x256 upscaler module) & Stable x4 (to 1024x1024 upscaler)\n- `xformers` and set env variable `FORCE_MEM_EFFICIENT_ATTN=1`\n\n\n## Quick Start\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/deepfloyd_if_free_tier_google_colab.ipynb)\n[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/DeepFloyd/IF)\n\n```shell\npip install deepfloyd_if==1.0.2rc0\npip install xformers==0.0.16\npip install git+https://github.com/openai/CLIP.git --no-deps\n```\n\n## Local notebooks\n[![Jupyter Notebook](https://img.shields.io/badge/jupyter_notebook-%23FF7A01.svg?logo=jupyter&logoColor=white)](https://huggingface.co/DeepFloyd/IF-notebooks/blob/main/pipes-DeepFloyd-IF-v1.0.ipynb)\n[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/shonenkov/deepfloyd-if-4-3b-generator-of-pictures)\n\nThe Dream, Style Transfer, Super Resolution or Inpainting modes are avaliable in a Jupyter Notebook [here](https://huggingface.co/DeepFloyd/IF-notebooks/blob/main/pipes-DeepFloyd-IF-v1.0.ipynb).\n\n\n\n## Integration with \ud83e\udd17 Diffusers\n\nIF is also integrated with the \ud83e\udd17 Hugging Face [Diffusers library](https://github.com/huggingface/diffusers/).\n\nDiffusers runs each stage individually allowing the user to customize the image generation process as well as allowing to inspect intermediate results easily.\n\n### Example\n\nBefore you can use IF, you need to accept its usage conditions. To do so:\n1. Make sure to have a [Hugging Face account](https://huggingface.co/join) and be loggin in\n2. Accept the license on the model card of [DeepFloyd/IF-I-XL-v1.0](https://huggingface.co/DeepFloyd/IF-I-XL-v1.0)\n3. Make sure to login locally. Install `huggingface_hub`\n```sh\npip install huggingface_hub --upgrade\n```\n\nrun the login function in a Python shell\n\n```py\nfrom huggingface_hub import login\n\nlogin()\n```\n\nand enter your [Hugging Face Hub access token](https://huggingface.co/docs/hub/security-tokens#what-are-user-access-tokens).\n\nNext we install `diffusers` and dependencies:\n\n```sh\npip install diffusers accelerate transformers safetensors\n```\n\nAnd we can now run the model locally.\n\nBy default `diffusers` makes use of [model cpu offloading](https://huggingface.co/docs/diffusers/optimization/fp16#model-offloading-for-fast-inference-and-memory-savings) to run the whole IF pipeline with as little as 14 GB of VRAM.\n\nIf you are using `torch>=2.0.0`, make sure to **delete all** `enable_xformers_memory_efficient_attention()`\nfunctions.\n\n```py\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import pt_to_pil\nimport torch\n\n# stage 1\nstage_1 = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", variant=\"fp16\", torch_dtype=torch.float16)\nstage_1.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ >= 2.0.0\nstage_1.enable_model_cpu_offload()\n\n# stage 2\nstage_2 = DiffusionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16\n)\nstage_2.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ >= 2.0.0\nstage_2.enable_model_cpu_offload()\n\n# stage 3\nsafety_modules = {\"feature_extractor\": stage_1.feature_extractor, \"safety_checker\": stage_1.safety_checker, \"watermarker\": stage_1.watermarker}\nstage_3 = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-x4-upscaler\", **safety_modules, torch_dtype=torch.float16)\nstage_3.enable_xformers_memory_efficient_attention()  # remove line if torch.__version__ >= 2.0.0\nstage_3.enable_model_cpu_offload()\n\nprompt = 'a photo of a kangaroo wearing an orange hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says \"very deep learning\"'\n\n# text embeds\nprompt_embeds, negative_embeds = stage_1.encode_prompt(prompt)\n\ngenerator = torch.manual_seed(0)\n\n# stage 1\nimage = stage_1(prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds, generator=generator, output_type=\"pt\").images\npt_to_pil(image)[0].save(\"./if_stage_I.png\")\n\n# stage 2\nimage = stage_2(\n    image=image, prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds, generator=generator, output_type=\"pt\"\n).images\npt_to_pil(image)[0].save(\"./if_stage_II.png\")\n\n# stage 3\nimage = stage_3(prompt=prompt, image=image, generator=generator, noise_level=100).images\nimage[0].save(\"./if_stage_III.png\")\n```\n\n There are multiple ways to speed up the inference time and lower the memory consumption even more with `diffusers`. To do so, please have a look at the Diffusers docs:\n\n- \ud83d\ude80 [Optimizing for inference time](https://huggingface.co/docs/diffusers/api/pipelines/if#optimizing-for-speed)\n- \u2699\ufe0f [Optimizing for low memory during inference](https://huggingface.co/docs/diffusers/api/pipelines/if#optimizing-for-memory)\n\nFor more in-detail information about how to use IF, please have a look at [the IF blog post](https://huggingface.co/blog/if) and [the documentation](https://huggingface.co/docs/diffusers/main/en/api/pipelines/if) \ud83d\udcd6.\n\nDiffusers dreambooth scripts also supports fine-tuning \ud83c\udfa8 [IF](https://huggingface.co/docs/diffusers/main/en/training/dreambooth#if).\nWith parameter efficient finetuning, you can add new concepts to IF with a single GPU and ~28 GB VRAM.\n\n## Run the code locally\n\n### Loading the models into VRAM\n\n```python\nfrom deepfloyd_if.modules import IFStageI, IFStageII, StableStageIII\nfrom deepfloyd_if.modules.t5 import T5Embedder\n\ndevice = 'cuda:0'\nif_I = IFStageI('IF-I-XL-v1.0', device=device)\nif_II = IFStageII('IF-II-L-v1.0', device=device)\nif_III = StableStageIII('stable-diffusion-x4-upscaler', device=device)\nt5 = T5Embedder(device=\"cpu\")\n```\n\n### I. Dream\nDream is the text-to-image mode of the IF model\n\n```python\nfrom deepfloyd_if.pipelines import dream\n\nprompt = 'ultra close-up color photo portrait of rainbow owl with deer horns in the woods'\ncount = 4\n\nresult = dream(\n    t5=t5, if_I=if_I, if_II=if_II, if_III=if_III,\n    prompt=[prompt]*count,\n    seed=42,\n    if_I_kwargs={\n        \"guidance_scale\": 7.0,\n        \"sample_timestep_respacing\": \"smart100\",\n    },\n    if_II_kwargs={\n        \"guidance_scale\": 4.0,\n        \"sample_timestep_respacing\": \"smart50\",\n    },\n    if_III_kwargs={\n        \"guidance_scale\": 9.0,\n        \"noise_level\": 20,\n        \"sample_timestep_respacing\": \"75\",\n    },\n)\n\nif_III.show(result['III'], size=14)\n```\n![](./pics/dream-III.jpg)\n\n## II. Zero-shot Image-to-Image Translation\n\n![](./pics/img_to_img_scheme.jpeg)\n\nIn Style Transfer mode, the output of your prompt comes out at the style of the `support_pil_img`\n```python\nfrom deepfloyd_if.pipelines import style_transfer\n\nresult = style_transfer(\n    t5=t5, if_I=if_I, if_II=if_II,\n    support_pil_img=raw_pil_image,\n    style_prompt=[\n        'in style of professional origami',\n        'in style of oil art, Tate modern',\n        'in style of plastic building bricks',\n        'in style of classic anime from 1990',\n    ],\n    seed=42,\n    if_I_kwargs={\n        \"guidance_scale\": 10.0,\n        \"sample_timestep_respacing\": \"10,10,10,10,10,10,10,10,0,0\",\n        'support_noise_less_qsample_steps': 5,\n    },\n    if_II_kwargs={\n        \"guidance_scale\": 4.0,\n        \"sample_timestep_respacing\": 'smart50',\n        \"support_noise_less_qsample_steps\": 5,\n    },\n)\nif_I.show(result['II'], 1, 20)\n```\n\n![Alternative Text](./pics/deep_floyd_if_image_2_image.gif)\n\n\n## III. Super Resolution\nFor super-resolution, users can run `IF-II` and `IF-III` or 'Stable x4' on an image that was not necessarely generated by IF (two cascades):\n\n```python\nfrom deepfloyd_if.pipelines import super_resolution\n\nmiddle_res = super_resolution(\n    t5,\n    if_III=if_II,\n    prompt=['woman with a blue headscarf and a blue sweaterp, detailed picture, 4k dslr, best quality'],\n    support_pil_img=raw_pil_image,\n    img_scale=4.,\n    img_size=64,\n    if_III_kwargs={\n        'sample_timestep_respacing': 'smart100',\n        'aug_level': 0.5,\n        'guidance_scale': 6.0,\n    },\n)\nhigh_res = super_resolution(\n    t5,\n    if_III=if_III,\n    prompt=[''],\n    support_pil_img=middle_res['III'][0],\n    img_scale=4.,\n    img_size=256,\n    if_III_kwargs={\n        \"guidance_scale\": 9.0,\n        \"noise_level\": 20,\n        \"sample_timestep_respacing\": \"75\",\n    },\n)\nshow_superres(raw_pil_image, high_res['III'][0])\n```\n\n![](./pics/if_as_upscaler.jpg)\n\n\n### IV. Zero-shot Inpainting\n\n```python\nfrom deepfloyd_if.pipelines import inpainting\n\nresult = inpainting(\n    t5=t5, if_I=if_I,\n    if_II=if_II,\n    if_III=if_III,\n    support_pil_img=raw_pil_image,\n    inpainting_mask=inpainting_mask,\n    prompt=[\n        'oil art, a man in a hat',\n    ],\n    seed=42,\n    if_I_kwargs={\n        \"guidance_scale\": 7.0,\n        \"sample_timestep_respacing\": \"10,10,10,10,10,0,0,0,0,0\",\n        'support_noise_less_qsample_steps': 0,\n    },\n    if_II_kwargs={\n        \"guidance_scale\": 4.0,\n        'aug_level': 0.0,\n        \"sample_timestep_respacing\": '100',\n    },\n    if_III_kwargs={\n        \"guidance_scale\": 9.0,\n        \"noise_level\": 20,\n        \"sample_timestep_respacing\": \"75\",\n    },\n)\nif_I.show(result['I'], 2, 3)\nif_I.show(result['II'], 2, 6)\nif_I.show(result['III'], 2, 14)\n```\n![](./pics/deep_floyd_if_inpainting.gif)\n\n### \ud83e\udd17 Model Zoo \ud83e\udd17\nThe link to download the weights as well as the model cards will be available soon on each model of the model zoo\n\n#### Original\n\n| Name                                                      | Cascade | Params | FID  | Batch size | Steps |\n|:----------------------------------------------------------|:-------:|:------:|:----:|:----------:|:-----:|\n| [IF-I-M](https://huggingface.co/DeepFloyd/IF-I-M-v1.0)    |    I    |  400M  | 8.86 |    3072    | 2.5M  |\n| [IF-I-L](https://huggingface.co/DeepFloyd/IF-I-L-v1.0)    |    I    |  900M  | 8.06 |    3200    | 3.0M  |\n| [IF-I-XL](https://huggingface.co/DeepFloyd/IF-I-XL-v1.0)* |    I    |  4.3B  | 6.66 |    3072    | 2.42M |\n| [IF-II-M](https://huggingface.co/DeepFloyd/IF-II-M-v1.0)  |   II    |  450M  |  -   |    1536    | 2.5M  |\n| [IF-II-L](https://huggingface.co/DeepFloyd/IF-II-L-v1.0)* |   II    |  1.2B  |  -   |    1536    | 2.5M  |\n| IF-III-L* _(soon)_                                        |   III   |  700M  |  -   |    3072    | 1.25M |\n\n *best modules\n\n### Quantitative Evaluation\n\n`FID = 6.66`\n\n![](./pics/fid30k_if.jpg)\n\n## License\n\nThe code in this repository is released under the bespoke license (see added [point two](https://github.com/deep-floyd/IF/blob/main/LICENSE#L13)).\n\nThe weights will be available soon via [the DeepFloyd organization at Hugging Face](https://huggingface.co/DeepFloyd) and have their own LICENSE.\n\n**Disclaimer:** *The initial release of the IF model is under a restricted research-purposes-only license temporarily to gather feedback, and after that we intend to release a fully open-source model in line with other Stability AI models.*\n\n## Limitations and Biases\n\nThe models available in this codebase have known limitations and biases. Please refer to [the model card](https://huggingface.co/DeepFloyd/IF-I-L-v1.0) for more information.\n\n\n## \ud83c\udf93 DeepFloyd IF creators:\n\n- Alex Shonenkov [GitHub](https://github.com/shonenkov) | [Linktr](https://linktr.ee/shonenkovAI)\n- Misha Konstantinov [GitHub](https://github.com/zeroshot-ai) | [Twitter](https://twitter.com/_bra_ket)\n- Daria Bakshandaeva [GitHub](https://github.com/Gugutse) | [Twitter](https://twitter.com/_gugutse_)\n- Christoph Schuhmann [GitHub](https://github.com/christophschuhmann) | [Twitter](https://twitter.com/laion_ai)\n- Ksenia Ivanova [GitHub](https://github.com/ivksu) | [Twitter](https://twitter.com/susiaiv)\n- Nadiia Klokova [GitHub](https://github.com/vauimpuls) | [Twitter](https://twitter.com/vauimpuls)\n\n\n## \ud83d\udcc4 Research Paper (Soon)\n\n## Acknowledgements\n\nSpecial thanks to [StabilityAI](http://stability.ai) and its CEO [Emad Mostaque](https://twitter.com/emostaque) for invaluable support, providing GPU compute and infrastructure to train the models (our gratitude goes to [Richard Vencu](https://github.com/rvencu)); thanks to [LAION](https://laion.ai) and [Christoph Schuhmann](https://github.com/christophschuhmann) in particular for contribution to the project and well-prepared datasets; thanks to [Huggingface](https://huggingface.co) teams for optimizing models' speed and memory consumption during inference, creating demos and giving cool advice!\n\n## \ud83d\ude80 External Contributors \ud83d\ude80\n- The Biggest Thanks [@Apolin\u00e1rio](https://github.com/apolinario), for ideas, consultations, help and support on all stages to make IF available in open-source; for writing a lot of documentation and instructions; for creating a friendly atmosphere in difficult moments \ud83e\udd89;\n- Thanks, [@patrickvonplaten](https://github.com/patrickvonplaten), for improving loading time of unet models by 80%;\nfor integration Stable-Diffusion-x4 as native pipeline \ud83d\udcaa;\n- Thanks, [@williamberman](https://github.com/williamberman) and [@patrickvonplaten](https://github.com/patrickvonplaten) for diffusers integration \ud83d\ude4c;\n- Thanks, [@hysts](https://github.com/hysts) and [@Apolin\u00e1rio](https://github.com/apolinario) for creating [the best gradio demo with IF](https://huggingface.co/spaces/DeepFloyd/IF) \ud83d\ude80;\n- Thanks, [@Dango233](https://github.com/Dango233), for adapting IF with xformers memory efficient attention \ud83d\udcaa;\n",
        "releases": [
            {
                "name": "v1.0.1",
                "date": "2023-04-28T12:17:09Z"
            },
            {
                "name": "v1.0.0",
                "date": "2023-04-27T13:04:38Z"
            }
        ]
    }
}