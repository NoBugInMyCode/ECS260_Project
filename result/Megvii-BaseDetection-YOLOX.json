{
    "https://api.github.com/repos/Megvii-BaseDetection/YOLOX": {
        "forks": 2243,
        "watchers": 9585,
        "stars": 9585,
        "languages": {
            "Python": 309414,
            "C++": 24802
        },
        "commits": [
            "2024-11-20T07:46:56Z",
            "2024-07-30T03:58:00Z",
            "2024-07-11T05:07:55Z",
            "2024-07-10T11:49:13Z",
            "2023-05-23T03:50:21Z",
            "2023-05-15T11:00:06Z",
            "2023-03-24T11:05:23Z",
            "2023-03-03T04:02:09Z",
            "2023-03-01T03:56:20Z",
            "2023-02-22T03:41:18Z",
            "2023-02-16T05:18:49Z",
            "2023-02-02T08:38:08Z",
            "2023-01-10T02:53:09Z",
            "2023-01-09T12:31:23Z",
            "2022-12-27T10:27:21Z",
            "2022-12-26T12:42:01Z",
            "2022-12-26T02:21:32Z",
            "2022-12-24T01:50:23Z",
            "2022-12-16T03:30:35Z",
            "2022-12-10T02:51:03Z",
            "2022-11-30T11:23:22Z",
            "2022-11-29T08:05:38Z",
            "2022-11-03T03:36:02Z",
            "2022-11-03T03:14:45Z",
            "2022-08-12T03:12:50Z",
            "2022-08-01T09:28:27Z",
            "2022-07-20T08:34:43Z",
            "2022-07-08T02:53:38Z",
            "2022-07-05T03:24:54Z",
            "2022-06-29T06:49:15Z"
        ],
        "creation_date": "2021-07-17T02:01:45Z",
        "contributors": 30,
        "topics": [
            "deep-learning",
            "megengine",
            "ncnn",
            "object-detection",
            "onnx",
            "openvino",
            "pytorch",
            "tensorrt",
            "yolo",
            "yolov3",
            "yolox"
        ],
        "subscribers": 77,
        "readme": "<div align=\"center\"><img src=\"assets/logo.png\" width=\"350\"></div>\n<img src=\"assets/demo.png\" >\n\n## Introduction\nYOLOX is an anchor-free version of YOLO, with a simpler design but better performance! It aims to bridge the gap between research and industrial communities.\nFor more details, please refer to our [report on Arxiv](https://arxiv.org/abs/2107.08430).\n\nThis repo is an implementation of PyTorch version YOLOX, there is also a [MegEngine implementation](https://github.com/MegEngine/YOLOX).\n\n<img src=\"assets/git_fig.png\" width=\"1000\" >\n\n## Updates!!\n* \u30102023/02/28\u3011 We support assignment visualization tool, see doc [here](./docs/assignment_visualization.md).\n* \u30102022/04/14\u3011 We support jit compile op.\n* \u30102021/08/19\u3011 We optimize the training process with **2x** faster training and **~1%** higher performance! See [notes](docs/updates_note.md) for more details.\n* \u30102021/08/05\u3011 We release [MegEngine version YOLOX](https://github.com/MegEngine/YOLOX).\n* \u30102021/07/28\u3011 We fix the fatal error of [memory leak](https://github.com/Megvii-BaseDetection/YOLOX/issues/103)\n* \u30102021/07/26\u3011 We now support [MegEngine](https://github.com/Megvii-BaseDetection/YOLOX/tree/main/demo/MegEngine) deployment.\n* \u30102021/07/20\u3011 We have released our technical report on [Arxiv](https://arxiv.org/abs/2107.08430).\n\n## Benchmark\n\n#### Standard Models.\n\n|Model |size |mAP<sup>val<br>0.5:0.95 |mAP<sup>test<br>0.5:0.95 | Speed V100<br>(ms) | Params<br>(M) |FLOPs<br>(G)| weights |\n| ------        |:---: | :---:    | :---:       |:---:     |:---:  | :---: | :----: |\n|[YOLOX-s](./exps/default/yolox_s.py)    |640  |40.5 |40.5      |9.8      |9.0 | 26.8 | [github](https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_s.pth) |\n|[YOLOX-m](./exps/default/yolox_m.py)    |640  |46.9 |47.2      |12.3     |25.3 |73.8| [github](https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_m.pth) |\n|[YOLOX-l](./exps/default/yolox_l.py)    |640  |49.7 |50.1      |14.5     |54.2| 155.6 | [github](https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_l.pth) |\n|[YOLOX-x](./exps/default/yolox_x.py)   |640   |51.1 |**51.5**  | 17.3    |99.1 |281.9 | [github](https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_x.pth) |\n|[YOLOX-Darknet53](./exps/default/yolov3.py)   |640  | 47.7 | 48.0 | 11.1 |63.7 | 185.3 | [github](https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_darknet.pth) |\n\n<details>\n<summary>Legacy models</summary>\n\n|Model |size |mAP<sup>test<br>0.5:0.95 | Speed V100<br>(ms) | Params<br>(M) |FLOPs<br>(G)| weights |\n| ------        |:---: | :---:       |:---:     |:---:  | :---: | :----: |\n|[YOLOX-s](./exps/default/yolox_s.py)    |640  |39.6      |9.8     |9.0 | 26.8 | [onedrive](https://megvii-my.sharepoint.cn/:u:/g/personal/gezheng_megvii_com/EW62gmO2vnNNs5npxjzunVwB9p307qqygaCkXdTO88BLUg?e=NMTQYw)/[github](https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_s.pth) |\n|[YOLOX-m](./exps/default/yolox_m.py)    |640  |46.4      |12.3     |25.3 |73.8| [onedrive](https://megvii-my.sharepoint.cn/:u:/g/personal/gezheng_megvii_com/ERMTP7VFqrVBrXKMU7Vl4TcBQs0SUeCT7kvc-JdIbej4tQ?e=1MDo9y)/[github](https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_m.pth) |\n|[YOLOX-l](./exps/default/yolox_l.py)    |640  |50.0  |14.5 |54.2| 155.6 | [onedrive](https://megvii-my.sharepoint.cn/:u:/g/personal/gezheng_megvii_com/EWA8w_IEOzBKvuueBqfaZh0BeoG5sVzR-XYbOJO4YlOkRw?e=wHWOBE)/[github](https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_l.pth) |\n|[YOLOX-x](./exps/default/yolox_x.py)   |640  |**51.2**      | 17.3 |99.1 |281.9 | [onedrive](https://megvii-my.sharepoint.cn/:u:/g/personal/gezheng_megvii_com/EdgVPHBziOVBtGAXHfeHI5kBza0q9yyueMGdT0wXZfI1rQ?e=tABO5u)/[github](https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_x.pth) |\n|[YOLOX-Darknet53](./exps/default/yolov3.py)   |640  | 47.4      | 11.1 |63.7 | 185.3 | [onedrive](https://megvii-my.sharepoint.cn/:u:/g/personal/gezheng_megvii_com/EZ-MV1r_fMFPkPrNjvbJEMoBLOLAnXH-XKEB77w8LhXL6Q?e=mf6wOc)/[github](https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_darknet53.pth) |\n\n</details>\n\n#### Light Models.\n\n|Model |size |mAP<sup>val<br>0.5:0.95 | Params<br>(M) |FLOPs<br>(G)| weights |\n| ------        |:---:  |  :---:       |:---:     |:---:  | :---: |\n|[YOLOX-Nano](./exps/default/yolox_nano.py) |416  |25.8  | 0.91 |1.08 | [github](https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_nano.pth) |\n|[YOLOX-Tiny](./exps/default/yolox_tiny.py) |416  |32.8 | 5.06 |6.45 | [github](https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_tiny.pth) |\n\n\n<details>\n<summary>Legacy models</summary>\n\n|Model |size |mAP<sup>val<br>0.5:0.95 | Params<br>(M) |FLOPs<br>(G)| weights |\n| ------        |:---:  |  :---:       |:---:     |:---:  | :---: |\n|[YOLOX-Nano](./exps/default/yolox_nano.py) |416  |25.3  | 0.91 |1.08 | [github](https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_nano.pth) |\n|[YOLOX-Tiny](./exps/default/yolox_tiny.py) |416  |32.8 | 5.06 |6.45 | [github](https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_tiny_32dot8.pth) |\n\n</details>\n\n## Quick Start\n\n<details>\n<summary>Installation</summary>\n\nStep1. Install YOLOX from source.\n```shell\ngit clone git@github.com:Megvii-BaseDetection/YOLOX.git\ncd YOLOX\npip3 install -v -e .  # or  python3 setup.py develop\n```\n\n</details>\n\n<details>\n<summary>Demo</summary>\n\nStep1. Download a pretrained model from the benchmark table.\n\nStep2. Use either -n or -f to specify your detector's config. For example:\n\n```shell\npython tools/demo.py image -n yolox-s -c /path/to/your/yolox_s.pth --path assets/dog.jpg --conf 0.25 --nms 0.45 --tsize 640 --save_result --device [cpu/gpu]\n```\nor\n```shell\npython tools/demo.py image -f exps/default/yolox_s.py -c /path/to/your/yolox_s.pth --path assets/dog.jpg --conf 0.25 --nms 0.45 --tsize 640 --save_result --device [cpu/gpu]\n```\nDemo for video:\n```shell\npython tools/demo.py video -n yolox-s -c /path/to/your/yolox_s.pth --path /path/to/your/video --conf 0.25 --nms 0.45 --tsize 640 --save_result --device [cpu/gpu]\n```\n\n\n</details>\n\n<details>\n<summary>Reproduce our results on COCO</summary>\n\nStep1. Prepare COCO dataset\n```shell\ncd <YOLOX_HOME>\nln -s /path/to/your/COCO ./datasets/COCO\n```\n\nStep2. Reproduce our results on COCO by specifying -n:\n\n```shell\npython -m yolox.tools.train -n yolox-s -d 8 -b 64 --fp16 -o [--cache]\n                               yolox-m\n                               yolox-l\n                               yolox-x\n```\n* -d: number of gpu devices\n* -b: total batch size, the recommended number for -b is num-gpu * 8\n* --fp16: mixed precision training\n* --cache: caching imgs into RAM to accelarate training, which need large system RAM.\n\n\n\nWhen using -f, the above commands are equivalent to:\n```shell\npython -m yolox.tools.train -f exps/default/yolox_s.py -d 8 -b 64 --fp16 -o [--cache]\n                               exps/default/yolox_m.py\n                               exps/default/yolox_l.py\n                               exps/default/yolox_x.py\n```\n\n**Multi Machine Training**\n\nWe also support multi-nodes training. Just add the following args:\n* --num\\_machines: num of your total training nodes\n* --machine\\_rank: specify the rank of each node\n\nSuppose you want to train YOLOX on 2 machines, and your master machines's IP is 123.123.123.123, use port 12312 and TCP.\n\nOn master machine, run\n```shell\npython tools/train.py -n yolox-s -b 128 --dist-url tcp://123.123.123.123:12312 --num_machines 2 --machine_rank 0\n```\nOn the second machine, run\n```shell\npython tools/train.py -n yolox-s -b 128 --dist-url tcp://123.123.123.123:12312 --num_machines 2 --machine_rank 1\n```\n\n**Logging to Weights & Biases**\n\nTo log metrics, predictions and model checkpoints to [W&B](https://docs.wandb.ai/guides/integrations/other/yolox) use the command line argument `--logger wandb` and use the prefix \"wandb-\" to specify arguments for initializing the wandb run.\n\n```shell\npython tools/train.py -n yolox-s -d 8 -b 64 --fp16 -o [--cache] --logger wandb wandb-project <project name>\n                         yolox-m\n                         yolox-l\n                         yolox-x\n```\n\nAn example wandb dashboard is available [here](https://wandb.ai/manan-goel/yolox-nano/runs/3pzfeom0)\n\n**Others**\n\nSee more information with the following command:\n```shell\npython -m yolox.tools.train --help\n```\n\n</details>\n\n\n<details>\n<summary>Evaluation</summary>\n\nWe support batch testing for fast evaluation:\n\n```shell\npython -m yolox.tools.eval -n  yolox-s -c yolox_s.pth -b 64 -d 8 --conf 0.001 [--fp16] [--fuse]\n                               yolox-m\n                               yolox-l\n                               yolox-x\n```\n* --fuse: fuse conv and bn\n* -d: number of GPUs used for evaluation. DEFAULT: All GPUs available will be used.\n* -b: total batch size across on all GPUs\n\nTo reproduce speed test, we use the following command:\n```shell\npython -m yolox.tools.eval -n  yolox-s -c yolox_s.pth -b 1 -d 1 --conf 0.001 --fp16 --fuse\n                               yolox-m\n                               yolox-l\n                               yolox-x\n```\n\n</details>\n\n\n<details>\n<summary>Tutorials</summary>\n\n*  [Training on custom data](docs/train_custom_data.md)\n*  [Caching for custom data](docs/cache.md)\n*  [Manipulating training image size](docs/manipulate_training_image_size.md)\n*  [Assignment visualization](docs/assignment_visualization.md)\n*  [Freezing model](docs/freeze_module.md)\n\n</details>\n\n## Deployment\n\n\n1. [MegEngine in C++ and Python](./demo/MegEngine)\n2. [ONNX export and an ONNXRuntime](./demo/ONNXRuntime)\n3. [TensorRT in C++ and Python](./demo/TensorRT)\n4. [ncnn in C++ and Java](./demo/ncnn)\n5. [OpenVINO in C++ and Python](./demo/OpenVINO)\n6. [Accelerate YOLOX inference with nebullvm in Python](./demo/nebullvm)\n\n## Third-party resources\n* YOLOX for streaming perception: [StreamYOLO (CVPR 2022 Oral)](https://github.com/yancie-yjr/StreamYOLO)\n* The YOLOX-s and YOLOX-nano are Integrated into [ModelScope](https://www.modelscope.cn/home). Try out the Online Demo at [YOLOX-s](https://www.modelscope.cn/models/damo/cv_cspnet_image-object-detection_yolox/summary) and [YOLOX-Nano](https://www.modelscope.cn/models/damo/cv_cspnet_image-object-detection_yolox_nano_coco/summary) respectively \ud83d\ude80.\n* Integrated into [Huggingface Spaces \ud83e\udd17](https://huggingface.co/spaces) using [Gradio](https://github.com/gradio-app/gradio). Try out the Web Demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/Sultannn/YOLOX-Demo)\n* The ncnn android app with video support: [ncnn-android-yolox](https://github.com/FeiGeChuanShu/ncnn-android-yolox) from [FeiGeChuanShu](https://github.com/FeiGeChuanShu)\n* YOLOX with Tengine support: [Tengine](https://github.com/OAID/Tengine/blob/tengine-lite/examples/tm_yolox.cpp) from [BUG1989](https://github.com/BUG1989)\n* YOLOX + ROS2 Foxy: [YOLOX-ROS](https://github.com/Ar-Ray-code/YOLOX-ROS) from [Ar-Ray](https://github.com/Ar-Ray-code)\n* YOLOX Deploy DeepStream: [YOLOX-deepstream](https://github.com/nanmi/YOLOX-deepstream) from [nanmi](https://github.com/nanmi)\n* YOLOX MNN/TNN/ONNXRuntime: [YOLOX-MNN](https://github.com/DefTruth/lite.ai.toolkit/blob/main/lite/mnn/cv/mnn_yolox.cpp)\u3001[YOLOX-TNN](https://github.com/DefTruth/lite.ai.toolkit/blob/main/lite/tnn/cv/tnn_yolox.cpp) and [YOLOX-ONNXRuntime C++](https://github.com/DefTruth/lite.ai.toolkit/blob/main/lite/ort/cv/yolox.cpp) from [DefTruth](https://github.com/DefTruth)\n* Converting darknet or yolov5 datasets to COCO format for YOLOX: [YOLO2COCO](https://github.com/RapidAI/YOLO2COCO) from [Daniel](https://github.com/znsoftm)\n\n## Cite YOLOX\nIf you use YOLOX in your research, please cite our work by using the following BibTeX entry:\n\n```latex\n @article{yolox2021,\n  title={YOLOX: Exceeding YOLO Series in 2021},\n  author={Ge, Zheng and Liu, Songtao and Wang, Feng and Li, Zeming and Sun, Jian},\n  journal={arXiv preprint arXiv:2107.08430},\n  year={2021}\n}\n```\n## In memory of Dr. Jian Sun\nWithout the guidance of [Dr. Jian Sun](https://scholar.google.com/citations?user=ALVSZAYAAAAJ), YOLOX would not have been released and open sourced to the community.\nThe passing away of Dr. Sun is a huge loss to the Computer Vision field. We add this section here to express our remembrance and condolences to our captain Dr. Sun.\nIt is hoped that every AI practitioner in the world will stick to the belief of \"continuous innovation to expand cognitive boundaries, and extraordinary technology to achieve product value\" and move forward all the way.\n\n<div align=\"center\"><img src=\"assets/sunjian.png\" width=\"200\"></div>\n\u6ca1\u6709\u5b59\u5251\u535a\u58eb\u7684\u6307\u5bfc\uff0cYOLOX\u4e5f\u4e0d\u4f1a\u95ee\u4e16\u5e76\u5f00\u6e90\u7ed9\u793e\u533a\u4f7f\u7528\u3002\n\u5b59\u5251\u535a\u58eb\u7684\u79bb\u53bb\u662fCV\u9886\u57df\u7684\u4e00\u5927\u635f\u5931\uff0c\u6211\u4eec\u5728\u6b64\u7279\u522b\u6dfb\u52a0\u4e86\u8fd9\u4e2a\u90e8\u5206\u6765\u8868\u8fbe\u5bf9\u6211\u4eec\u7684\u201c\u8239\u957f\u201d\u5b59\u8001\u5e08\u7684\u7eaa\u5ff5\u548c\u54c0\u601d\u3002\n\u5e0c\u671b\u4e16\u754c\u4e0a\u7684\u6bcf\u4e2aAI\u4ece\u4e1a\u8005\u79c9\u6301\u7740\u201c\u6301\u7eed\u521b\u65b0\u62d3\u5c55\u8ba4\u77e5\u8fb9\u754c\uff0c\u975e\u51e1\u79d1\u6280\u6210\u5c31\u4ea7\u54c1\u4ef7\u503c\u201d\u7684\u89c2\u5ff5\uff0c\u4e00\u8def\u5411\u524d\u3002\n",
        "releases": [
            {
                "name": "YOLOX 0.3.0",
                "date": "2022-04-22T07:20:43Z"
            },
            {
                "name": "0.2.0",
                "date": "2022-01-18T07:05:34Z"
            },
            {
                "name": "0.1.1 pre release",
                "date": "2021-08-18T12:41:41Z"
            },
            {
                "name": "YOLOX 0.1.0 version",
                "date": "2021-08-18T03:55:44Z"
            }
        ]
    }
}