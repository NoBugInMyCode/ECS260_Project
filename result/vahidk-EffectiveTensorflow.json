{
    "https://api.github.com/repos/vahidk/EffectiveTensorflow": {
        "forks": 905,
        "watchers": 8622,
        "stars": 8622,
        "languages": {},
        "commits": [
            "2020-10-22T05:25:22Z",
            "2020-10-22T05:23:20Z",
            "2019-04-23T06:02:36Z",
            "2019-04-22T14:59:23Z",
            "2019-04-22T14:31:42Z",
            "2019-03-17T00:41:51Z",
            "2019-03-16T23:56:15Z",
            "2019-03-16T23:47:45Z",
            "2019-03-16T23:43:30Z",
            "2019-03-16T23:42:24Z",
            "2019-03-16T23:39:08Z",
            "2019-03-16T18:29:01Z",
            "2019-02-20T05:50:08Z",
            "2019-02-20T05:34:25Z",
            "2019-02-14T11:32:37Z",
            "2018-09-28T23:23:14Z",
            "2018-09-28T07:26:50Z",
            "2018-07-11T20:58:08Z",
            "2018-07-11T05:10:54Z",
            "2018-07-11T04:05:13Z",
            "2018-03-05T08:31:48Z",
            "2018-03-03T07:21:15Z",
            "2018-01-20T21:23:33Z",
            "2018-01-20T19:13:13Z",
            "2017-10-26T23:18:31Z",
            "2017-09-17T04:46:14Z",
            "2017-09-16T10:13:57Z",
            "2017-09-16T10:08:31Z",
            "2017-09-16T10:02:00Z",
            "2017-09-13T08:24:13Z"
        ],
        "creation_date": "2017-08-01T06:00:56Z",
        "contributors": 10,
        "topics": [
            "deep-learning",
            "ebook",
            "machine-learning",
            "neural-network",
            "tensorflow"
        ],
        "subscribers": 344,
        "readme": "# Effective TensorFlow 2\n\nTable of Contents\n=================\n## Part I: TensorFlow 2 Fundamentals\n1.  [TensorFlow 2 Basics](#basics)\n2.  [Broadcasting the good and the ugly](#broadcast)\n3.  [Take advantage of the overloaded operators](#overloaded_ops)\n4.  [Control flow operations: conditionals and loops](#control_flow)\n5.  [Prototyping kernels and advanced visualization with Python ops](#python_ops)\n6.  [Numerical stability in TensorFlow](#stable)\n---\n\n_We updated the guide to follow the newly released TensorFlow 2.x API. If you want the original guide for TensorFlow 1.x see the [v1 branch](https://github.com/vahidk/EffectiveTensorflow/tree/v1)._\n\n_To install TensorFlow 2.0 (alpha) follow the [instructions on the official website](https://www.tensorflow.org/install/pip):_\n```\npip install tensorflow==2.0.0-alpha0\n```\n\n_We aim to gradually expand this series by adding new articles and keep the content up to date with the latest releases of TensorFlow API. If you have suggestions on how to improve this series or find the explanations ambiguous, feel free to create an issue, send patches, or reach out by email._\n\n# Part I: TensorFlow 2.0 Fundamentals\n<a name=\"fundamentals\"></a>\n\n## TensorFlow Basics\n<a name=\"basics\"></a>\nTensorFlow 2 went under a massive redesign to make the API more accessible and easier to use. If you are familiar with numpy you will find yourself right at home when using TensorFlow 2. Unlike TensorFlow 1 which was purely symbolic, TensorFlow 2 hides its symbolic nature behind the hood to look like any other imperative library like NumPy. It's important to note the change is mostly an interface change, and TensorFlow 2 is still able to take advantage of its symbolic machinery to do everything that TensorFlow 1.x can do (e.g. automatic-differentiation and massively parallel computation on TPUs/GPUs).\n\nLet's start with a simple example, we want to multiply two random matrices. First we look at an implementation done in NumPy:\n```python\nimport numpy as np\n\nx = np.random.normal(size=[10, 10])\ny = np.random.normal(size=[10, 10])\nz = np.dot(x, y)\n\nprint(z)\n```\n\nNow we perform the exact same computation this time in TensorFlow 2.0:\n```python\nimport tensorflow as tf\n\nx = tf.random.normal([10, 10])\ny = tf.random.normal([10, 10])\nz = tf.matmul(x, y)\n\nprint(z)\n```\nSimilar to NumPy TensorFlow 2 also immediately performs the computation and produces the result. The only difference is that TensorFlow uses tf.Tensor type to store the results which can be easily converted to NumPy, by calling tf.Tensor.numpy() member function: \n\n```python\nprint(z.numpy())\n```\n\nTo understand how powerful symbolic computation can be let's have a look at another example. Assume that we have samples from a curve (say f(x) = 5x^2 + 3) and we want to estimate f(x) based on these samples. We define a parametric function g(x, w) = w0 x^2 + w1 x + w2, which is a function of the input x and latent parameters w, our goal is then to find the latent parameters such that g(x, w) \u2248 f(x). This can be done by minimizing the following loss function: L(w) = &sum; (f(x) - g(x, w))^2. Although there's a closed form solution for this simple problem, we opt to use a more general approach that can be applied to any arbitrary differentiable function, and that is using stochastic gradient descent. We simply compute the average gradient of L(w) with respect to w over a set of sample points and move in the opposite direction.\n\nHere's how it can be done in TensorFlow:\n\n```python\nimport numpy as np\nimport tensorflow as tf\n\n# Assuming we know that the desired function is a polynomial of 2nd degree, we\n# allocate a vector of size 3 to hold the coefficients and initialize it with\n# random noise.\nw = tf.Variable(tf.random.normal([3, 1]))\n\n# We use the Adam optimizer with learning rate set to 0.1 to minimize the loss.\nopt = tf.optimizers.Adam(0.1)\n\ndef model(x):\n    # We define yhat to be our estimate of y.\n    f = tf.stack([tf.square(x), x, tf.ones_like(x)], 1)\n    yhat = tf.squeeze(tf.matmul(f, w), 1)\n    return yhat\n\ndef compute_loss(y, yhat):\n    # The loss is defined to be the l2 distance between our estimate of y and its\n    # true value. We also added a shrinkage term, to ensure the resulting weights\n    # would be small.\n    loss = tf.nn.l2_loss(yhat - y) + 0.1 * tf.nn.l2_loss(w)\n    return loss\n\ndef generate_data():\n    # Generate some training data based on the true function\n    x = np.random.uniform(-10.0, 10.0, size=100).astype(np.float32)\n    y = 5 * np.square(x) + 3\n    return x, y\n\ndef train_step():\n    x, y = generate_data()\n\n    def _loss_fn():\n        yhat = model(x)\n        loss = compute_loss(y, yhat)\n        return loss\n    \n    opt.minimize(_loss_fn, [w])\n\nfor _ in range(1000):\n    train_step()\n\nprint(w.numpy())\n```\nBy running this piece of code you should see a result close to this:\n```python\n[4.9924135, 0.00040895029, 3.4504161]\n```\nWhich is a relatively close approximation to our parameters.\n\nNote that in the above code we are running Tensorflow in imperative mode (i.e. operations get instantly executed), which is not very efficient. TensorFlow 2.0 can also turn a given piece of python code into a graph which can then optimized and efficiently parallelized on GPUs and TPUs. To get all those benefits we simply need to decorate the train_step function with tf.function decorator:\n\n```python\n@tf.function\ndef train_step():\n    x, y = generate_data()\n\n    def _loss_fn():\n        yhat = model(x)\n        loss = compute_loss(y, yhat)\n        return loss\n    \n    opt.minimize(_loss_fn, [w])\n```\n\nWhat's cool about tf.function is that it's also able to convert basic python statements like while, for and if into native TensorFlow functions. We will get to that later.\n\nThis is just tip of the iceberg for what TensorFlow can do. Many problems such as optimizing large neural networks with millions of parameters can be implemented efficiently in TensorFlow in just a few lines of code. TensorFlow takes care of scaling across multiple devices, and threads, and supports a variety of platforms.\n\n## Broadcasting the good and the ugly\n<a name=\"broadcast\"></a>\nTensorFlow supports broadcasting elementwise operations. Normally when you want to perform operations like addition and multiplication, you need to make sure that shapes of the operands match, e.g. you can\u2019t add a tensor of shape [3, 2] to a tensor of shape [3, 4]. But there\u2019s a special case and that\u2019s when you have a singular dimension. TensorFlow implicitly tiles the tensor across its singular dimensions to match the shape of the other operand. So it\u2019s valid to add a tensor of shape [3, 2] to a tensor of shape [3, 1]\n\n```python\nimport tensorflow as tf\n\na = tf.constant([[1., 2.], [3., 4.]])\nb = tf.constant([[1.], [2.]])\n# c = a + tf.tile(b, [1, 2])\nc = a + b\n\nprint(c)\n```\n\nBroadcasting allows us to perform implicit tiling which makes the code shorter, and more memory efficient, since we don\u2019t need to store the result of the tiling operation. One neat place that this can be used is when combining features of varying length. In order to concatenate features of varying length we commonly tile the input tensors, concatenate the result and apply some nonlinearity. This is a common pattern across a variety of neural network architectures:\n\n```python\na = tf.random.uniform([5, 3, 5])\nb = tf.random.uniform([5, 1, 6])\n\n# concat a and b and apply nonlinearity\ntiled_b = tf.tile(b, [1, 3, 1])\nc = tf.concat([a, tiled_b], 2)\nd = tf.keras.layers.Dense(10, activation=tf.nn.relu).apply(c)\n\nprint(d)\n```\n\nBut this can be done more efficiently with broadcasting. We use the fact that f(m(x + y)) is equal to f(mx + my). So we can do the linear operations separately and use broadcasting to do implicit concatenation:\n\n```python\npa = tf.keras.layers.Dense(10).apply(a)\npb = tf.keras.layers.Dense(10).apply(b)\nd = tf.nn.relu(pa + pb)\n\nprint(d)\n```\n\nIn fact this piece of code is pretty general and can be applied to tensors of arbitrary shape as long as broadcasting between tensors is possible:\n\n```python\ndef merge(a, b, units, activation=None):\n    pa = tf.keras.layers.Dense(units).apply(a)\n    pb = tf.keras.layers.Dense(units).apply(b)\n    c = pa + pb\n    if activation is not None:\n        c = activation(c)\n    return c\n```\n\nSo far we discussed the good part of broadcasting. But what\u2019s the ugly part you may ask? Implicit assumptions almost always make debugging harder to do. Consider the following example:\n\n```python\na = tf.constant([[1.], [2.]])\nb = tf.constant([1., 2.])\nc = tf.reduce_sum(a + b)\n\nprint(c)\n```\n\nWhat do you think the value of c would be after evaluation? If you guessed 6, that\u2019s wrong. It\u2019s going to be 12. This is because when rank of two tensors don\u2019t match, TensorFlow automatically expands the first dimension of the tensor with lower rank before the elementwise operation, so the result of addition would be [[2, 3], [3, 4]], and the reducing over all parameters would give us 12.\n\nThe way to avoid this problem is to be as explicit as possible. Had we specified which dimension we would want to reduce across, catching this bug would have been much easier:\n\n```python\na = tf.constant([[1.], [2.]])\nb = tf.constant([1., 2.])\nc = tf.reduce_sum(a + b, 0)\n\nprint(c)\n```\n\nHere the value of c would be [5, 7], and we immediately would guess based on the shape of the result that there\u2019s something wrong. A general rule of thumb is to always specify the dimensions in reduction operations and when using tf.squeeze.\n\n## Take advantage of the overloaded operators\n<a name=\"overloaded_ops\"></a>\nJust like NumPy, TensorFlow overloads a number of python operators to make building graphs easier and the code more readable.\n\nThe slicing op is one of the overloaded operators that can make indexing tensors very easy:\n```python\nz = x[begin:end]  # z = tf.slice(x, [begin], [end-begin])\n```\nBe very careful when using this op though. The slicing op is very inefficient and often better avoided, especially when the number of slices is high. To understand how inefficient this op can be let's look at an example. We want to manually perform reduction across the rows of a matrix:\n```python\nimport tensorflow as tf\nimport time\n\nx = tf.random.uniform([500, 10])\n\nz = tf.zeros([10])\n\nstart = time.time()\nfor i in range(500):\n    z += x[i]\nprint(\"Took %f seconds.\" % (time.time() - start))\n```\nOn my MacBook Pro, this took 0.045 seconds to run which is quite slow. The reason is that we are calling the slice op 500 times, which is going to be very slow to run. A better choice would have been to use tf.unstack op to slice the matrix into a list of vectors all at once:\n```python\nz = tf.zeros([10])\nfor x_i in tf.unstack(x):\n    z += x_i\n```\nThis took 0.01 seconds. Of course, the right way to do this simple reduction is to use tf.reduce_sum op:\n```python\nz = tf.reduce_sum(x, axis=0)\n```\nThis took 0.0001 seconds, which is 100x faster than the original implementation.\n\nTensorFlow also overloads a range of arithmetic and logical operators:\n```python\nz = -x  # z = tf.negative(x)\nz = x + y  # z = tf.add(x, y)\nz = x - y  # z = tf.subtract(x, y)\nz = x * y  # z = tf.mul(x, y)\nz = x / y  # z = tf.div(x, y)\nz = x // y  # z = tf.floordiv(x, y)\nz = x % y  # z = tf.mod(x, y)\nz = x ** y  # z = tf.pow(x, y)\nz = x @ y  # z = tf.matmul(x, y)\nz = x > y  # z = tf.greater(x, y)\nz = x >= y  # z = tf.greater_equal(x, y)\nz = x < y  # z = tf.less(x, y)\nz = x <= y  # z = tf.less_equal(x, y)\nz = abs(x)  # z = tf.abs(x)\nz = x & y  # z = tf.logical_and(x, y)\nz = x | y  # z = tf.logical_or(x, y)\nz = x ^ y  # z = tf.logical_xor(x, y)\nz = ~x  # z = tf.logical_not(x)\n```\n\nYou can also use the augmented version of these ops. For example `x += y` and `x **= 2` are also valid.\n\nNote that Python doesn't allow overloading \"and\", \"or\", and \"not\" keywords.\n\nOther operators that aren't supported are equal (==) and not equal (!=) operators which are overloaded in NumPy but not in TensorFlow. Use the function versions instead which are `tf.equal` and `tf.not_equal`.\n\n## Control flow operations: conditionals and loops\n<a name=\"control_flow\"></a>\nWhen building complex models such as recurrent neural networks you may need to control the flow of operations through conditionals and loops. In this section we introduce a number of commonly used control flow ops.\n\nLet's assume you want to decide whether to multiply to or add two given tensors based on a predicate. This can be simply implemented with either python's built-in if statement or using tf.cond function:\n```python\na = tf.constant(1)\nb = tf.constant(2)\n\np = tf.constant(True)\n\n# Alternatively:\n# x = tf.cond(p, lambda: a + b, lambda: a * b)\nx = a + b if p else a * b\n\nprint(x.numpy())\n```\nSince the predicate is True in this case, the output would be the result of the addition, which is 3.\n\nMost of the times when using TensorFlow you are using large tensors and want to perform operations in batch. A related conditional operation is tf.where, which like tf.cond takes a predicate, but selects the output based on the condition in batch.\n```python\na = tf.constant([1, 1])\nb = tf.constant([2, 2])\n\np = tf.constant([True, False])\n\nx = tf.where(p, a + b, a * b)\n\nprint(x.numpy())\n```\nThis will return [3, 2].\n\nAnother widely used control flow operation is tf.while_loop. It allows building dynamic loops in TensorFlow that operate on sequences of variable length. Let's see how we can generate Fibonacci sequence with tf.while_loops:\n\n```python\n@tf.function\ndef fibonacci(n):\n    a = tf.constant(1)\n    b = tf.constant(1)\n\n    for i in range(2, n):\n        a, b = b, a + b\n    \n    return b\n    \nn = tf.constant(5)\nb = fibonacci(n)\n    \nprint(b.numpy())\n```\nThis will print 5. Note that tf.function automatically converts the given python code to use tf.while_loop so we don't need to directly interact with the TF API.\n\nNow imagine we want to keep the whole series of Fibonacci sequence. We may update our body to keep a record of the history of current values:\n```python\n@tf.function\ndef fibonacci(n):\n    a = tf.constant(1)\n    b = tf.constant(1)\n    c = tf.constant([1, 1])\n\n    for i in range(2, n):\n        a, b = b, a + b\n        c = tf.concat([c, [b]], 0)\n    \n    return c\n    \nn = tf.constant(5)\nb = fibonacci(n)\n    \nprint(b.numpy())\n```\n\nNow if you try running this, TensorFlow will complain that the shape of the the one of the loop variables is changing. \nOne way to fix this is is to use \"shape invariants\", but this functionality is only available when using the low-level tf.while_loop API:\n\n\n```python\nn = tf.constant(5)\n\ndef cond(i, a, b, c):\n    return i < n\n\ndef body(i, a, b, c):\n    a, b = b, a + b\n    c = tf.concat([c, [b]], 0)\n    return i + 1, a, b, c\n\ni, a, b, c = tf.while_loop(\n    cond, body, (2, 1, 1, tf.constant([1, 1])),\n    shape_invariants=(tf.TensorShape([]),\n                      tf.TensorShape([]),\n                      tf.TensorShape([]),\n                      tf.TensorShape([None])))\n\nprint(c.numpy())\n```\n\nThis is not only getting ugly, but is also pretty inefficient. Note that we are building a lot of intermediary tensors that we don't use. TensorFlow has a better solution for this kind of growing arrays. Meet tf.TensorArray. Let's do the same thing this time with tensor arrays:\n```python\n@tf.function\ndef fibonacci(n):\n    a = tf.constant(1)\n    b = tf.constant(1)\n\n    c = tf.TensorArray(tf.int32, n)\n    c = c.write(0, a)\n    c = c.write(1, b)\n\n    for i in range(2, n):\n        a, b = b, a + b\n        c = c.write(i, b)\n    \n    return c.stack()\n\nn = tf.constant(5)\nc = fibonacci(n)\n    \nprint(c.numpy())\n```\nTensorFlow while loops and tensor arrays are essential tools for building complex recurrent neural networks. As an exercise try implementing [beam search](https://en.wikipedia.org/wiki/Beam_search) using tf.while_loops. Can you make it more efficient with tensor arrays?\n\n## Prototyping kernels and advanced visualization with Python ops\n<a name=\"python_ops\"></a>\nOperation kernels in TensorFlow are entirely written in C++ for efficiency. But writing a TensorFlow kernel in C++ can be quite a pain. So, before spending hours implementing your kernel you may want to prototype something quickly, however inefficient. With tf.py_function() you can turn any piece of python code to a TensorFlow operation.\n\nFor example this is how you can implement a simple ReLU nonlinearity kernel in TensorFlow as a python op:\n```python\nimport numpy as np\nimport tensorflow as tf\nimport uuid\n\ndef relu(inputs):\n    # Define the op in python\n    def _py_relu(x):\n        return np.maximum(x, 0.)\n\n    # Define the op's gradient in python\n    def _py_relu_grad(x):\n        return np.float32(x > 0)\n    \n    @tf.custom_gradient\n    def _relu(x):\n        y = tf.py_function(_py_relu, [x], tf.float32)\n        \n        def _relu_grad(dy):\n            return dy * tf.py_function(_py_relu_grad, [x], tf.float32)\n\n        return y, _relu_grad\n\n    return _relu(inputs)\n```\n\nTo verify that the gradients are correct you can compare the numerical and analytical gradients and compare the vlaues.\n```python\n# Compute analytical gradient\nx = tf.random.normal([10], dtype=np.float32)\nwith tf.GradientTape() as tape:\n    tape.watch(x)\n    y = relu(x)\ng = tape.gradient(y, x)\nprint(g)\n\n# Compute numerical gradient\ndx_n = 1e-5\ndy_n = relu(x + dx_n) - relu(x)\ng_n = dy_n / dx_n\nprint(g_n)\n```\nThe numbers should be very close.\n\nNote that this implementation is pretty inefficient, and is only useful for prototyping, since the python code is not parallelizable and won't run on GPU. Once you verified your idea, you definitely would want to write it as a C++ kernel.\n\nIn practice we commonly use python ops to do visualization on Tensorboard. Consider the case that you are building an image classification model and want to visualize your model predictions during training. TensorFlow allows visualizing images with tf.summary.image() function:\n```python\nimage = tf.placeholder(tf.float32)\ntf.summary.image(\"image\", image)\n```\nBut this only visualizes the input image. In order to visualize the predictions you have to find a way to add annotations to the image which may be almost impossible with existing ops. An easier way to do this is to do the drawing in python, and wrap it in a python op:\n```python\ndef visualize_labeled_images(images, labels, max_outputs=3, name=\"image\"):\n    def _visualize_image(image, label):\n        # Do the actual drawing in python\n        fig = plt.figure(figsize=(3, 3), dpi=80)\n        ax = fig.add_subplot(111)\n        ax.imshow(image[::-1,...])\n        ax.text(0, 0, str(label),\n          horizontalalignment=\"left\",\n          verticalalignment=\"top\")\n        fig.canvas.draw()\n\n        # Write the plot as a memory file.\n        buf = io.BytesIO()\n        data = fig.savefig(buf, format=\"png\")\n        buf.seek(0)\n\n        # Read the image and convert to numpy array\n        img = PIL.Image.open(buf)\n        return np.array(img.getdata()).reshape(img.size[0], img.size[1], -1)\n\n    def _visualize_images(images, labels):\n        # Only display the given number of examples in the batch\n        outputs = []\n        for i in range(max_outputs):\n            output = _visualize_image(images[i], labels[i])\n            outputs.append(output)\n        return np.array(outputs, dtype=np.uint8)\n\n    # Run the python op.\n    figs = tf.py_function(_visualize_images, [images, labels], tf.uint8)\n    return tf.summary.image(name, figs)\n```\n\nNote that since summaries are usually only evaluated once in a while (not per step), this implementation may be used in practice without worrying about efficiency.\n\n## Numerical stability in TensorFlow\n<a name=\"stable\"></a>\nWhen using any numerical computation library such as NumPy or TensorFlow, it's important to note that writing mathematically correct code doesn't necessarily lead to correct results. You also need to make sure that the computations are stable.\n\nLet's start with a simple example. From primary school we know that x * y / y is equal to x for any non zero value of x. But let's see if that's always true in practice:\n```python\nimport numpy as np\n\nx = np.float32(1)\n\ny = np.float32(1e-50)  # y would be stored as zero\nz = x * y / y\n\nprint(z)  # prints nan\n```\n\nThe reason for the incorrect result is that y is simply too small for float32 type. A similar problem occurs when y is too large:\n\n```python\ny = np.float32(1e39)  # y would be stored as inf\nz = x * y / y\n\nprint(z)  # prints nan\n```\n\nThe smallest positive value that float32 type can represent is 1.4013e-45 and anything below that would be stored as zero. Also, any number beyond 3.40282e+38, would be stored as inf.\n\n```python\nprint(np.nextafter(np.float32(0), np.float32(1)))  # prints 1.4013e-45\nprint(np.finfo(np.float32).max)  # print 3.40282e+38\n```\n\nTo make sure that your computations are stable, you want to avoid values with small or very large absolute value. This may sound very obvious, but these kind of problems can become extremely hard to debug especially when doing gradient descent in TensorFlow. This is because you not only need to make sure that all the values in the forward pass are within the valid range of your data types, but also you need to make sure of the same for the backward pass (during gradient computation).\n\nLet's look at a real example. We want to compute the softmax over a vector of logits. A naive implementation would look something like this:\n```python\nimport tensorflow as tf\n\ndef unstable_softmax(logits):\n    exp = tf.exp(logits)\n    return exp / tf.reduce_sum(exp)\n\nprint(unstable_softmax([1000., 0.]).numpy())  # prints [ nan, 0.]\n```\nNote that computing the exponential of logits for relatively small numbers results to gigantic results that are out of float32 range. The largest valid logit for our naive softmax implementation is ln(3.40282e+38) = 88.7, anything beyond that leads to a nan outcome.\n\nBut how can we make this more stable? The solution is rather simple. It's easy to see that exp(x - c) / &sum; exp(x - c) = exp(x) / &sum; exp(x). Therefore we can subtract any constant from the logits and the result would remain the same. We choose this constant to be the maximum of logits. This way the domain of the exponential function would be limited to [-inf, 0], and consequently its range would be [0.0, 1.0] which is desirable:\n\n```python\nimport tensorflow as tf\n\ndef softmax(logits):\n    exp = tf.exp(logits - tf.reduce_max(logits))\n    return exp / tf.reduce_sum(exp)\n\nprint(softmax([1000., 0.]).numpy())  # prints [ 1., 0.]\n```\n\nLet's look at a more complicated case. Consider we have a classification problem. We use the softmax function to produce probabilities from our logits. We then define our loss function to be the cross entropy between our predictions and the labels. Recall that cross entropy for a categorical distribution can be simply defined as xe(p, q) = -&sum; p_i log(q_i). So a naive implementation of the cross entropy would look like this:\n\n```python\ndef unstable_softmax_cross_entropy(labels, logits):\n    logits = tf.math.log(softmax(logits))\n    return -tf.reduce_sum(labels * logits)\n\nlabels = tf.constant([0.5, 0.5])\nlogits = tf.constant([1000., 0.])\n\nxe = unstable_softmax_cross_entropy(labels, logits)\n\nprint(xe.numpy())  # prints inf\n```\n\nNote that in this implementation as the softmax output approaches zero, the log's output approaches infinity which causes instability in our computation. We can rewrite this by expanding the softmax and doing some simplifications:\n\n```python\ndef softmax_cross_entropy(labels, logits):\n    scaled_logits = logits - tf.reduce_max(logits)\n    normalized_logits = scaled_logits - tf.reduce_logsumexp(scaled_logits)\n    return -tf.reduce_sum(labels * normalized_logits)\n\nlabels = tf.constant([0.5, 0.5])\nlogits = tf.constant([1000., 0.])\n\nxe = softmax_cross_entropy(labels, logits)\n\nprint(xe.numpy())  # prints 500.0\n```\n\nWe can also verify that the gradients are also computed correctly:\n```python\nwith tf.GradientTape() as tape:\n    tape.watch(logits)\n    xe = softmax_cross_entropy(labels, logits)\n    \ng = tape.gradient(xe, logits)\nprint(g.numpy())  # prints [0.5, -0.5]\n```\nwhich is correct.\n\nLet me remind again that extra care must be taken when doing gradient descent to make sure that the range of your functions as well as the gradients for each layer are within a valid range. Exponential and logarithmic functions when used naively are especially problematic because they can map small numbers to enormous ones and the other way around.\n\n",
        "releases": []
    }
}