{
    "https://api.github.com/repos/EpistasisLab/tpot": {
        "forks": 1574,
        "watchers": 9814,
        "stars": 9814,
        "languages": {
            "Python": 448453,
            "Jupyter Notebook": 139415,
            "Shell": 3355
        },
        "commits": [
            "2024-02-23T19:03:07Z",
            "2024-02-23T19:00:14Z",
            "2024-02-22T18:49:32Z",
            "2024-02-22T18:10:34Z",
            "2024-02-22T17:45:37Z",
            "2024-02-20T22:39:42Z",
            "2023-12-12T17:00:27Z",
            "2023-12-08T19:12:48Z",
            "2023-12-08T19:07:07Z",
            "2023-12-08T18:23:22Z",
            "2023-12-08T18:22:13Z",
            "2023-12-08T18:21:45Z",
            "2023-12-08T18:17:32Z",
            "2023-12-08T17:04:25Z",
            "2023-11-28T19:10:27Z",
            "2023-09-06T21:46:11Z",
            "2023-09-06T21:14:25Z",
            "2023-08-15T18:20:37Z",
            "2023-08-14T21:48:43Z",
            "2023-08-11T23:50:39Z",
            "2023-08-11T23:48:24Z",
            "2023-06-01T22:41:40Z",
            "2023-06-01T22:41:19Z",
            "2023-05-25T22:11:55Z",
            "2023-05-18T21:01:56Z",
            "2023-05-18T21:01:42Z",
            "2023-05-09T01:00:09Z",
            "2023-05-08T23:59:31Z",
            "2023-05-08T23:54:03Z",
            "2023-05-08T23:48:12Z"
        ],
        "creation_date": "2015-11-03T21:08:40Z",
        "contributors": 30,
        "topics": [
            "adsp",
            "ag066833",
            "aiml",
            "alzheimer",
            "alzheimers",
            "automated-machine-learning",
            "automation",
            "automl",
            "data-science",
            "feature-engineering",
            "gradient-boosting",
            "hyperparameter-optimization",
            "machine-learning",
            "model-selection",
            "nia",
            "parameter-tuning",
            "python",
            "random-forest",
            "scikit-learn",
            "u01ag066833"
        ],
        "subscribers": 290,
        "readme": "Master status: [![Master Build Status - Mac/Linux](https://travis-ci.com/EpistasisLab/tpot.svg?branch=master)](https://travis-ci.com/EpistasisLab/tpot)\n[![Master Build Status - Windows](https://ci.appveyor.com/api/projects/status/b7bmpwpkjhifrm7v/branch/master?svg=true)](https://ci.appveyor.com/project/weixuanfu/tpot?branch=master)\n[![Master Coverage Status](https://coveralls.io/repos/github/EpistasisLab/tpot/badge.svg?branch=master)](https://coveralls.io/github/EpistasisLab/tpot?branch=master)\n\nDevelopment status: [![Development Build Status - Mac/Linux](https://travis-ci.com/EpistasisLab/tpot.svg?branch=development)](https://travis-ci.com/EpistasisLab/tpot/branches)\n[![Development Build Status - Windows](https://ci.appveyor.com/api/projects/status/b7bmpwpkjhifrm7v/branch/development?svg=true)](https://ci.appveyor.com/project/weixuanfu/tpot?branch=development)\n[![Development Coverage Status](https://coveralls.io/repos/github/EpistasisLab/tpot/badge.svg?branch=development)](https://coveralls.io/github/EpistasisLab/tpot?branch=development)\n\nPackage information: [![Python 3.7](https://img.shields.io/badge/python-3.7-blue.svg)](https://www.python.org/downloads/release/python-370/)\n[![License: LGPL v3](https://img.shields.io/badge/license-LGPL%20v3-blue.svg)](http://www.gnu.org/licenses/lgpl-3.0)\n[![PyPI version](https://badge.fury.io/py/TPOT.svg)](https://badge.fury.io/py/TPOT)\n\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/EpistasisLab/tpot/master/images/tpot-logo.jpg\" width=300 />\n</p>\n\n---\nTo try the ![NEW!](https://raw.githubusercontent.com/EpistasisLab/tpot/master/images/NEW-small.gif \"NEW!\") TPOT2 (*alpha*) please go [here](https://github.com/EpistasisLab/tpot2)!\n\n- - - -\n\n**TPOT** stands for **T**ree-based **P**ipeline **O**ptimization **T**ool. Consider TPOT your **Data Science Assistant**. TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.\n\n![TPOT Demo](https://github.com/EpistasisLab/tpot/blob/master/images/tpot-demo.gif \"TPOT Demo\")\n\nTPOT will automate the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one for your data.\n\n![An example Machine Learning pipeline](https://github.com/EpistasisLab/tpot/blob/master/images/tpot-ml-pipeline.png \"An example Machine Learning pipeline\")\n\n<p align=\"center\"><strong>An example Machine Learning pipeline</strong></p>\n\nOnce TPOT is finished searching (or you get tired of waiting), it provides you with the Python code for the best pipeline it found so you can tinker with the pipeline from there.\n\n![An example TPOT pipeline](https://github.com/EpistasisLab/tpot/blob/master/images/tpot-pipeline-example.png \"An example TPOT pipeline\")\n\nTPOT is built on top of scikit-learn, so all of the code it generates should look familiar... if you're familiar with scikit-learn, anyway.\n\n**TPOT is still under active development** and we encourage you to check back on this repository regularly for updates.\n\nFor further information about TPOT, please see the [project documentation](http://epistasislab.github.io/tpot/).\n\n## License\n\nPlease see the [repository license](https://github.com/EpistasisLab/tpot/blob/master/LICENSE) for the licensing and usage information for TPOT.\n\nGenerally, we have licensed TPOT to make it as widely usable as possible.\n\n## Installation\n\nWe maintain the [TPOT installation instructions](http://epistasislab.github.io/tpot/installing/) in the documentation. TPOT requires a working installation of Python.\n\n## Usage\n\nTPOT can be used [on the command line](http://epistasislab.github.io/tpot/using/#tpot-on-the-command-line) or [with Python code](http://epistasislab.github.io/tpot/using/#tpot-with-code).\n\nClick on the corresponding links to find more information on TPOT usage in the documentation.\n\n## Examples\n\n### Classification\n\nBelow is a minimal working example with the optical recognition of handwritten digits dataset.\n\n```python\nfrom tpot import TPOTClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75, test_size=0.25, random_state=42)\n\ntpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_digits_pipeline.py')\n```\n\nRunning this code should discover a pipeline that achieves about 98% testing accuracy, and the corresponding Python code should be exported to the `tpot_digits_pipeline.py` file and look similar to the following:\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom tpot.builtins import StackingEstimator\nfrom tpot.export_utils import set_param_recursive\n\n# NOTE: Make sure that the outcome column is labeled 'target' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\nfeatures = tpot_data.drop('target', axis=1)\ntraining_features, testing_features, training_target, testing_target = \\\n            train_test_split(features, tpot_data['target'], random_state=42)\n\n# Average CV score on the training set was: 0.9799428471757372\nexported_pipeline = make_pipeline(\n    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),\n    StackingEstimator(estimator=LogisticRegression(C=0.1, dual=False, penalty=\"l1\")),\n    RandomForestClassifier(bootstrap=True, criterion=\"entropy\", max_features=0.35000000000000003, min_samples_leaf=20, min_samples_split=19, n_estimators=100)\n)\n# Fix random state for all the steps in exported pipeline\nset_param_recursive(exported_pipeline.steps, 'random_state', 42)\n\nexported_pipeline.fit(training_features, training_target)\nresults = exported_pipeline.predict(testing_features)\n```\n\n### Regression\n\nSimilarly, TPOT can optimize pipelines for regression problems. Below is a minimal working example with the practice Boston housing prices data set.\n\n```python\nfrom tpot import TPOTRegressor\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\n\nhousing = load_boston()\nX_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,\n                                                    train_size=0.75, test_size=0.25, random_state=42)\n\ntpot = TPOTRegressor(generations=5, population_size=50, verbosity=2, random_state=42)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_boston_pipeline.py')\n```\n\nwhich should result in a pipeline that achieves about 12.77 mean squared error (MSE), and the Python code in `tpot_boston_pipeline.py` should look similar to:\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom tpot.export_utils import set_param_recursive\n\n# NOTE: Make sure that the outcome column is labeled 'target' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\nfeatures = tpot_data.drop('target', axis=1)\ntraining_features, testing_features, training_target, testing_target = \\\n            train_test_split(features, tpot_data['target'], random_state=42)\n\n# Average CV score on the training set was: -10.812040755234403\nexported_pipeline = make_pipeline(\n    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),\n    ExtraTreesRegressor(bootstrap=False, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=100)\n)\n# Fix random state for all the steps in exported pipeline\nset_param_recursive(exported_pipeline.steps, 'random_state', 42)\n\nexported_pipeline.fit(training_features, training_target)\nresults = exported_pipeline.predict(testing_features)\n```\n\nCheck the documentation for [more examples and tutorials](http://epistasislab.github.io/tpot/examples/).\n\n## Contributing to TPOT\n\nWe welcome you to [check the existing issues](https://github.com/EpistasisLab/tpot/issues/) for bugs or enhancements to work on. If you have an idea for an extension to TPOT, please [file a new issue](https://github.com/EpistasisLab/tpot/issues/new) so we can discuss it.\n\nBefore submitting any contributions, please review our [contribution guidelines](http://epistasislab.github.io/tpot/contributing/).\n\n## Having problems or have questions about TPOT?\n\nPlease [check the existing open and closed issues](https://github.com/EpistasisLab/tpot/issues?utf8=%E2%9C%93&q=is%3Aissue) to see if your issue has already been attended to. If it hasn't, [file a new issue](https://github.com/EpistasisLab/tpot/issues/new) on this repository so we can review your issue.\n\n## Citing TPOT\n\nIf you use TPOT in a scientific publication, please consider citing at least one of the following papers:\n\nTrang T. Le, Weixuan Fu and Jason H. Moore (2020). [Scaling tree-based automated machine learning to biomedical big data with a feature set selector](https://academic.oup.com/bioinformatics/article/36/1/250/5511404). *Bioinformatics*.36(1): 250-256.\n\nBibTeX entry:\n\n```bibtex\n@article{le2020scaling,\n  title={Scaling tree-based automated machine learning to biomedical big data with a feature set selector},\n  author={Le, Trang T and Fu, Weixuan and Moore, Jason H},\n  journal={Bioinformatics},\n  volume={36},\n  number={1},\n  pages={250--256},\n  year={2020},\n  publisher={Oxford University Press}\n}\n```\n\n\nRandal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender, La Creis Kidd, and Jason H. Moore (2016). [Automating biomedical data science through tree-based pipeline optimization](http://link.springer.com/chapter/10.1007/978-3-319-31204-0_9). *Applications of Evolutionary Computation*, pages 123-137.\n\nBibTeX entry:\n\n```bibtex\n@inbook{Olson2016EvoBio,\n    author={Olson, Randal S. and Urbanowicz, Ryan J. and Andrews, Peter C. and Lavender, Nicole A. and Kidd, La Creis and Moore, Jason H.},\n    editor={Squillero, Giovanni and Burelli, Paolo},\n    chapter={Automating Biomedical Data Science Through Tree-Based Pipeline Optimization},\n    title={Applications of Evolutionary Computation: 19th European Conference, EvoApplications 2016, Porto, Portugal, March 30 -- April 1, 2016, Proceedings, Part I},\n    year={2016},\n    publisher={Springer International Publishing},\n    pages={123--137},\n    isbn={978-3-319-31204-0},\n    doi={10.1007/978-3-319-31204-0_9},\n    url={http://dx.doi.org/10.1007/978-3-319-31204-0_9}\n}\n```\n\nRandal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, and Jason H. Moore (2016). [Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science](http://dl.acm.org/citation.cfm?id=2908918). *Proceedings of GECCO 2016*, pages 485-492.\n\nBibTeX entry:\n\n```bibtex\n@inproceedings{OlsonGECCO2016,\n    author = {Olson, Randal S. and Bartley, Nathan and Urbanowicz, Ryan J. and Moore, Jason H.},\n    title = {Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science},\n    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},\n    series = {GECCO '16},\n    year = {2016},\n    isbn = {978-1-4503-4206-3},\n    location = {Denver, Colorado, USA},\n    pages = {485--492},\n    numpages = {8},\n    url = {http://doi.acm.org/10.1145/2908812.2908918},\n    doi = {10.1145/2908812.2908918},\n    acmid = {2908918},\n    publisher = {ACM},\n    address = {New York, NY, USA},\n}\n```\n\nAlternatively, you can cite the repository directly with the following DOI:\n\n[![DOI](https://zenodo.org/badge/20747/rhiever/tpot.svg)](https://zenodo.org/badge/latestdoi/20747/rhiever/tpot)\n\n## Support for TPOT\n\nTPOT was developed in the [Computational Genetics Lab](http://epistasis.org/) at the [University of Pennsylvania](https://www.upenn.edu/) with funding from the [NIH](http://www.nih.gov/) under grant R01 AI117694. We are incredibly grateful for the support of the NIH and the University of Pennsylvania during the development of this project.\n\nThe TPOT logo was designed by Todd Newmuis, who generously donated his time to the project.\n",
        "releases": [
            {
                "name": "v0.12.2",
                "date": "2024-02-23T19:05:48Z"
            },
            {
                "name": "v0.12.1",
                "date": "2023-08-15T18:21:39Z"
            },
            {
                "name": "v0.12.0 release",
                "date": "2023-05-25T22:44:19Z"
            },
            {
                "name": "v0.11.7 minor release ",
                "date": "2021-01-06T15:19:33Z"
            },
            {
                "name": "0.11.6.post3",
                "date": "2020-12-14T15:07:26Z"
            },
            {
                "name": "v0.11.6.post2",
                "date": "2020-11-30T16:31:53Z"
            },
            {
                "name": "v0.11.6.post1",
                "date": "2020-11-05T15:52:25Z"
            },
            {
                "name": "Version 0.11.6",
                "date": "2020-10-26T15:09:32Z"
            },
            {
                "name": "Covariate adjustments branch",
                "date": "2020-09-02T20:35:20Z"
            },
            {
                "name": "v0.11.5",
                "date": "2020-06-01T22:10:38Z"
            },
            {
                "name": "TPOT v0.11.4 minor release",
                "date": "2020-05-29T16:12:58Z"
            },
            {
                "name": "TPOT v0.11.3 minor release",
                "date": "2020-05-14T13:16:28Z"
            },
            {
                "name": "TPOT v0.11.2 Minor Release",
                "date": "2020-05-13T14:49:49Z"
            },
            {
                "name": "TPOT v0.11.1 Minor Release",
                "date": "2020-01-03T18:04:20Z"
            },
            {
                "name": "Version 0.11.0",
                "date": "2019-11-05T21:04:49Z"
            },
            {
                "name": "TPOT v0.10.2 minor release",
                "date": "2019-07-16T17:29:50Z"
            },
            {
                "name": "TPOT v0.10.1 minor release",
                "date": "2019-04-19T15:19:09Z"
            },
            {
                "name": "TPOT 0.10.0 Release",
                "date": "2019-04-12T14:48:07Z"
            },
            {
                "name": "TPOT 0.9.6 Minor Release",
                "date": "2019-03-01T18:30:35Z"
            },
            {
                "name": "TPOT now supports integration with Dask for parallelization",
                "date": "2018-09-04T16:41:09Z"
            },
            {
                "name": "Sparse matrix support, early stopping, and checkpointing",
                "date": "2017-09-27T17:56:34Z"
            },
            {
                "name": "More built-in configurations, missing data support, and detailed API documentation",
                "date": "2017-06-01T22:16:40Z"
            },
            {
                "name": "Multiprocessing support and custom operator configurations ",
                "date": "2017-03-22T20:49:21Z"
            },
            {
                "name": "Support for regression problems",
                "date": "2016-09-02T19:52:19Z"
            },
            {
                "name": "Full support for scikit-learn Pipelines",
                "date": "2016-08-20T03:06:30Z"
            },
            {
                "name": "Major upgrade",
                "date": "2016-06-23T13:01:58Z"
            },
            {
                "name": "Zenodo release",
                "date": "2016-03-06T17:02:56Z"
            },
            {
                "name": "GECCO 2016 paper release",
                "date": "2016-02-03T13:35:23Z"
            },
            {
                "name": "Export functionality and more ML models",
                "date": "2015-12-07T18:52:50Z"
            },
            {
                "name": "EvoBIO paper release",
                "date": "2015-11-18T14:33:38Z"
            }
        ]
    }
}