{
    "https://api.github.com/repos/unifyai/ivy": {
        "forks": 5697,
        "watchers": 14016,
        "stars": 14016,
        "languages": {
            "Python": 14514675,
            "Rust": 150088,
            "C++": 61615,
            "Shell": 17681,
            "Starlark": 7591,
            "Jupyter Notebook": 3527,
            "Dockerfile": 2904
        },
        "commits": [
            "2025-01-21T16:28:15Z",
            "2025-01-20T13:34:11Z",
            "2025-01-20T12:50:01Z",
            "2025-01-20T11:55:00Z",
            "2025-01-18T01:13:06Z",
            "2025-01-17T20:43:41Z",
            "2025-01-17T20:41:20Z",
            "2025-01-17T16:34:00Z",
            "2025-01-17T16:30:28Z",
            "2025-01-17T16:24:19Z",
            "2025-01-17T16:21:39Z",
            "2025-01-17T16:15:09Z",
            "2025-01-17T16:12:29Z",
            "2025-01-17T12:30:58Z",
            "2025-01-17T10:43:38Z",
            "2025-01-16T21:13:32Z",
            "2025-01-16T20:57:28Z",
            "2025-01-16T20:12:34Z",
            "2025-01-16T19:24:01Z",
            "2025-01-16T13:31:23Z",
            "2025-01-16T13:28:13Z",
            "2025-01-16T13:12:50Z",
            "2025-01-16T12:36:03Z",
            "2025-01-16T12:32:41Z",
            "2025-01-09T13:07:30Z",
            "2025-01-04T02:56:36Z",
            "2025-01-04T02:26:25Z",
            "2025-01-04T01:51:55Z",
            "2025-01-04T01:47:36Z",
            "2025-01-04T01:44:09Z"
        ],
        "creation_date": "2021-01-19T08:37:25Z",
        "contributors": 30,
        "topics": [
            "converter",
            "deep-learning",
            "ivy",
            "jax",
            "machine-learning",
            "neural-network",
            "numpy",
            "python",
            "pytorch",
            "tensorflow",
            "translation",
            "transpilation"
        ],
        "subscribers": 70,
        "readme": "<div style=\"display: block;\" align=\"center\">\r\n    <a href=\"https://ivy.dev/\">\r\n        <img class=\"dark-light\" width=\"50%\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/logos/ivy-long.svg\"/>\r\n    </a>\r\n</div>\r\n\r\n------------------------------------------------------------------------\r\n\r\n<table align=\"center\">\r\n  <tr>\r\n    <td align=\"center\">\r\n      <a href=\"https://ivy.dev/\">\r\n          <img class=\"dark-light\" width=\"75\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/website.svg\" alt=\"Website\">\r\n      </a>\r\n      <br>\r\n      <a href=\"https://ivy.dev/\" style=\"text-decoration: none;\">Website</a>\r\n    </td>\r\n    <td align=\"center\">\r\n      <a href=\"https://docs.ivy.dev/\">\r\n          <img class=\"dark-light\" width=\"70\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/docs.svg\" alt=\"Docs\">\r\n      </a>\r\n      <br>\r\n      <a href=\"https://docs.ivy.dev/\" style=\"text-decoration: none;\">Docs</a>\r\n    </td>\r\n    <td align=\"center\">\r\n      <a href=\"https://www.docs.ivy.dev/demos/examples_and_demos.html\">\r\n          <img class=\"dark-light\" width=\"75\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/demos.svg\" alt=\"Demos\">\r\n      </a>\r\n      <br>\r\n      <a href=\"https://www.docs.ivy.dev/demos/examples_and_demos.html\" style=\"text-decoration: none;\">Demos</a>\r\n    </td>\r\n    <td align=\"center\">\r\n      <a href=\"https://docs.ivy.dev/overview/design.html\">\r\n          <img class=\"dark-light\" width=\"75\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/design.svg\" alt=\"Design\">\r\n      </a>\r\n      <br>\r\n      <a href=\"https://docs.ivy.dev/overview/design.html\" style=\"text-decoration: none;\">Design</a>\r\n    </td>\r\n    <td align=\"center\">\r\n      <a href=\"https://docs.ivy.dev/overview/faq.html\">\r\n          <img class=\"dark-light\" width=\"75\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/faq.svg\" alt=\"FAQ\">\r\n      </a>\r\n      <br>\r\n      <a href=\"https://docs.ivy.dev/overview/faq.html\" style=\"text-decoration: none;\">FAQ</a>\r\n    </td>\r\n  </tr>\r\n</table>\r\n<br>\r\n\r\n<div style=\"margin-top: 10px; margin-bottom: 10px; display: block;\" align=\"center\">\r\n    <a href=\"https://github.com/ivy-llc/ivy/issues\">\r\n        <img class=\"dark-light\" style=\"padding-right: 4px; padding-bottom: 4px;\" src=\"https://img.shields.io/github/issues/ivy-llc/ivy\">\r\n    </a>\r\n    <a href=\"https://github.com/ivy-llc/ivy/network/members\">\r\n        <img class=\"dark-light\" style=\"padding-right: 4px; padding-bottom: 4px;\" src=\"https://img.shields.io/github/forks/ivy-llc/ivy\">\r\n    </a>\r\n    <a href=\"https://github.com/ivy-llc/ivy/stargazers\">\r\n        <img class=\"dark-light\" style=\"padding-right: 4px; padding-bottom: 4px;\" src=\"https://img.shields.io/github/stars/ivy-llc/ivy\">\r\n    </a>\r\n    <a href=\"https://github.com/ivy-llc/ivy/pulls\">\r\n        <img class=\"dark-light\" style=\"padding-right: 4px; padding-bottom: 4px;\" src=\"https://img.shields.io/badge/PRs-welcome-brightgreen.svg\">\r\n    </a>\r\n    <a href=\"https://pypi.org/project/ivy\">\r\n        <img class=\"dark-light\" style=\"padding-right: 4px; padding-bottom: 4px;\" src=\"https://badge.fury.io/py/ivy.svg\">\r\n    </a>\r\n    <a href=\"https://github.com/ivy-llc/ivy/actions?query=workflow%3Adocs\">\r\n        <img class=\"dark-light\" style=\"padding-right: 4px; padding-bottom: 4px;\" src=\"https://github.com/ivy-llc/ivy/actions/workflows/docs.yml/badge.svg\">\r\n    </a>\r\n    <a href=\"https://discord.gg/uYRmyPxMQq\">\r\n        <img class=\"dark-light\" style=\"padding-right: 4px; padding-bottom: 4px;\" src=\"https://img.shields.io/discord/1220325004013604945?color=blue&label=%20&logo=discord&logoColor=white\">\r\n    </a>\r\n</div>\r\n<br clear=\"all\" />\r\n\r\n\r\n# Convert Machine Learning Code Between Frameworks\r\n\r\nIvy enables you to:\r\n\r\n- Convert ML models, tools and libraries between frameworks while maintaining complete functionality using `ivy.transpile`\r\n- Create optimized graph-based models and functions in any native framework (PyTorch, TensorFlow, etc..) with `ivy.trace_graph`\r\n\r\n<div style=\"display: block;\" align=\"center\">\r\n    <div>\r\n    <a href=\"https://jax.readthedocs.io\">\r\n        <img class=\"dark-light\" width=\"100\" height=\"100\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/logos/jax.svg\">\r\n    </a>\r\n    <img class=\"dark-light\" width=\"5%\" src=\"https://github.com/ivy-llc/assets/blob/main/assets/empty.png?raw=true\">\r\n    <img class=\"dark-light\" width=\"5%\" src=\"https://github.com/ivy-llc/assets/blob/main/assets/empty.png?raw=true\">\r\n    <a href=\"https://www.tensorflow.org\">\r\n        <img class=\"dark-light\" width=\"100\" height=\"100\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/logos/tensorflow.svg\">\r\n    </a>\r\n    <img class=\"dark-light\" width=\"5%\" src=\"https://github.com/ivy-llc/assets/blob/main/assets/empty.png?raw=true\">\r\n    <img class=\"dark-light\" width=\"5%\" src=\"https://github.com/ivy-llc/assets/blob/main/assets/empty.png?raw=true\">\r\n    <a href=\"https://pytorch.org\">\r\n        <img class=\"dark-light\" width=\"100\" height=\"100\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/logos/pytorch.svg\">\r\n    </a>\r\n    <img class=\"dark-light\" width=\"5%\" src=\"https://github.com/ivy-llc/assets/blob/main/assets/empty.png?raw=true\">\r\n    <img class=\"dark-light\" width=\"5%\" src=\"https://github.com/ivy-llc/assets/blob/main/assets/empty.png?raw=true\">\r\n    <a href=\"https://numpy.org\">\r\n        <img class=\"dark-light\" width=\"100\" height=\"100\" src=\"https://raw.githubusercontent.com/ivy-llc/assets/refs/heads/main/assets/logos/numpy.svg\">\r\n    </a>\r\n    </div>\r\n</div>\r\n\r\n<br clear=\"all\" />\r\n\r\n# Installing ivy\r\n\r\nThe easiest way to set up Ivy is to install it using **pip**:\r\n\r\n``` bash\r\npip install ivy\r\n```\r\n\r\n<details>\r\n<summary><b>Docker Image</b></summary>\r\n\r\nYou can pull the Docker image for Ivy from:\r\n\r\n``` bash\r\ndocker pull ivyllc/ivy:latest\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n<summary><b>From Source</b></summary>\r\n\r\nYou can also install Ivy from source if you want to take advantage of\r\nthe latest changes, but we can\\'t ensure everything will work as\r\nexpected \ud83d\ude05\r\n\r\n``` bash\r\ngit clone https://github.com/ivy-llc/ivy.git\r\ncd ivy\r\npip install --user -e .\r\n```\r\n\r\nIf you want to set up testing and various frameworks it\\'s probably best\r\nto check out the [Setting Up](https://docs.ivy.dev/overview/contributing/setting_up.html)\r\npage, where OS-specific and IDE-specific instructions are available!\r\n\r\n</details>\r\n\r\n<br>\r\n\r\n# Supported Frameworks\r\n\r\nThese are the frameworks that `ivy.transpile` currently supports conversions from and to.\r\nWe're working hard on adding support for more frameworks, let us know on [Discord](https://discord.gg/uYRmyPxMQq) if there are source/target frameworks that would be useful for you!\r\n\r\n| Framework  | Source | Target |\r\n|------------|:------:|:------:|\r\n| PyTorch    |   \u2705   |   \ud83d\udea7   |\r\n| TensorFlow |   \ud83d\udea7   |   \u2705   |\r\n| JAX        |   \ud83d\udea7   |   \u2705   |\r\n| NumPy      |   \ud83d\udea7   |   \u2705   |\r\n\r\n<br>\r\n\r\n# Getting started\r\n\r\n- [Docs](https://docs.ivy.dev/)\r\n- [Demos](https://www.docs.ivy.dev/demos/examples_and_demos.html)\r\n- [FAQ](https://docs.ivy.dev/overview/faq.html)\r\n\r\n[Ivy's transpiler](https://docs.ivy.dev/overview/design/ivy_as_a_transpiler.html) allows you convert code between different ML frameworks. Have a look at our [Quickstart](https://docs.ivy.dev/demos/quickstart.html) notebook to get a brief idea of the features!\r\n\r\nBeyond that, based on the frameworks you want to convert code between, there are a few more [examples](#using-ivy) further down this page \ud83d\udc47 which contain a number of models and libraries transpiled between PyTorch, JAX, TensorFlow and NumPy.\r\n\r\n<br>\r\n\r\n# Using ivy\r\n\r\nHere's some examples, to help you get started using Ivy! The [examples page](https://www.docs.ivy.dev/demos/examples_and_demos.html) also features a wide range of\r\ndemos and tutorials showcasing some more use cases for Ivy.\r\n\r\n  <details>\r\n   <summary><b>Transpiling any code from one framework to another</b></summary>\r\n\r\n   ``` python\r\n   import ivy\r\n   import torch\r\n   import tensorflow as tf\r\n\r\n   def torch_fn(x):\r\n       a = torch.mul(x, x)\r\n       b = torch.mean(x)\r\n       return x * a + b\r\n\r\n   tf_fn = ivy.transpile(torch_fn, source=\"torch\", target=\"tensorflow\")\r\n\r\n   tf_x = tf.convert_to_tensor([1., 2., 3.])\r\n   ret = tf_fn(tf_x)\r\n   ```\r\n\r\n   </details>\r\n\r\n  <details>\r\n   <summary><b>Tracing a computational graph of any code</b></summary>\r\n\r\n   ``` python\r\n   import ivy\r\n   import torch\r\n\r\n   def torch_fn(x):\r\n       a = torch.mul(x, x)\r\n       b = torch.mean(x)\r\n       return x * a + b\r\n\r\n   torch_x = torch.tensor([1., 2., 3.])\r\n   graph = ivy.trace_graph(jax_fn, to=\"torch\", args=(torch_x,))\r\n   ret = graph(torch_x)\r\n   ```\r\n\r\n   </details>\r\n\r\n<!-- <details>\r\n<summary><b>I'm using PyTorch&ensp;<img class=\"dark-light\" src=\"https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/torch_small_logo.png\"></b></summary>\r\n   <blockquote>You can use Ivy to get PyTorch code from:\r\n      <details>\r\n         <summary>Any model</summary>\r\n         <blockquote>\r\n            <details>\r\n               <summary>From TensorFlow</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport torch\r\nimport tensorflow as tf\r\n\r\n# Get a pretrained keras model\r\neff_encoder = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(\r\n    include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3)\r\n)\r\n\r\n# Transpile it into a torch.nn.Module with the corresponding parameters\r\nnoise = tf.random.normal(shape=(1, 224, 224, 3))\r\ntorch_eff_encoder = ivy.transpile(eff_encoder, source=\"tensorflow\", to=\"torch\", args=(noise,))\r\n\r\n# Build a classifier using the transpiled encoder\r\nclass Classifier(torch.nn.Module):\r\n    def __init__(self, num_classes=20):\r\n        super().__init__()\r\n        self.encoder = torch_eff_encoder\r\n        self.fc = torch.nn.Linear(1280, num_classes)\r\n\r\n    def forward(self, x):\r\n        x = self.encoder(x)\r\n        return self.fc(x)\r\n\r\n# Initialize a trainable, customizable, torch.nn.Module\r\nclassifier = Classifier()\r\nret = classifier(torch.rand((1, 244, 244, 3)))\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From JAX</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport jax\r\nimport torch\r\n\r\n# Get a pretrained haiku model\r\n# https://github.com/unifyai/demos/blob/15c235f/scripts/deepmind_perceiver_io.py\r\nfrom deepmind_perceiver_io import key, perceiver_backbone\r\n\r\n# Transpile it into a torch.nn.Module with the corresponding parameters\r\ndummy_input = jax.random.uniform(key, shape=(1, 3, 224, 224))\r\nparams = perceiver_backbone.init(rng=key, images=dummy_input)\r\nivy.set_backend(\"jax\")\r\nbackbone = ivy.transpile(\r\n    perceiver_backbone, source=\"jax\", to=\"torch\", params_v=params, kwargs={\"images\": dummy_input}\r\n)\r\n\r\n# Build a classifier using the transpiled backbone\r\nclass PerceiverIOClassifier(torch.nn.Module):\r\n    def __init__(self, num_classes=20):\r\n        super().__init__()\r\n        self.backbone = backbone\r\n        self.max_pool = torch.nn.MaxPool2d((512, 1))\r\n        self.flatten = torch.nn.Flatten()\r\n        self.fc = torch.nn.Linear(1024, num_classes)\r\n\r\n    def forward(self, x):\r\n        x = self.backbone(images=x)\r\n        x = self.flatten(self.max_pool(x))\r\n        return self.fc(x)\r\n\r\n# Initialize a trainable, customizable, torch.nn.Module\r\nclassifier = PerceiverIOClassifier()\r\nret = classifier(torch.rand((1, 3, 224, 224)))\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary>Any library</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From Tensorflow</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport torch\r\nimport os\r\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\r\nimport segmentation_models as sm\r\n\r\n# transpile sm from tensorflow to torch\r\ntorch_sm = ivy.transpile(sm, source=\"tensorflow\", to=\"torch\")\r\n\r\n# get some image-like arrays\r\noutput = torch.rand((1, 3, 512, 512))\r\ntarget = torch.rand((1, 3, 512, 512))\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = torch_sm.metrics.iou_score(output, target)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From JAX</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport rax\r\nimport torch\r\n\r\n# transpile rax from jax to torch\r\ntorch_rax = ivy.transpile(rax, source=\"jax\", to=\"torch\")\r\n\r\n# get some arrays\r\nscores = torch.tensor([2.2, 1.3, 5.4])\r\nlabels = torch.tensor([1.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = torch_rax.poly1_softmax_loss(scores, labels)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From NumPy</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport torch\r\nimport madmom\r\n\r\n# transpile madmon from numpy to torch\r\ntorch_madmom = ivy.transpile(madmom, source=\"numpy\", to=\"torch\")\r\n\r\n# get some arrays\r\nfreqs = torch.arange(20) * 10\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = torch_madmom.audio.filters.hz2midi(freqs)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary>Any function</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From Tensorflow</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport tensorflow as tf\r\nimport torch\r\n\r\ndef loss(predictions, targets):\r\n    return tf.sqrt(tf.reduce_mean(tf.square(predictions - targets)))\r\n\r\n# transpile any function from tf to torch\r\ntorch_loss = ivy.transpile(loss, source=\"tensorflow\", to=\"torch\")\r\n\r\n# get some arrays\r\np = torch.tensor([3.0, 2.0, 1.0])\r\nt = torch.tensor([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = torch_loss(p, t)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From JAX</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport jax.numpy as jnp\r\nimport torch\r\n\r\ndef loss(predictions, targets):\r\n    return jnp.sqrt(jnp.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from jax to torch\r\ntorch_loss = ivy.transpile(loss, source=\"jax\", to=\"torch\")\r\n\r\n# get some arrays\r\np = torch.tensor([3.0, 2.0, 1.0])\r\nt = torch.tensor([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = torch_loss(p, t)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From NumPy</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport numpy as np\r\nimport torch\r\n\r\ndef loss(predictions, targets):\r\n    return np.sqrt(np.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from numpy to torch\r\ntorch_loss = ivy.transpile(loss, source=\"numpy\", to=\"torch\")\r\n\r\n# get some arrays\r\np = torch.tensor([3.0, 2.0, 1.0])\r\nt = torch.tensor([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = torch_loss(p, t)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary><b>I'm using TensorFlow&ensp;<img class=\"dark-light\" src=\"https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/tf_small_logo.png\"></b></summary>\r\n<blockquote>You can use Ivy to get TensorFlow code from:\r\n<details>\r\n<summary>Any model</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From PyTorch</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport torch\r\nimport timm\r\nimport tensorflow as tf\r\n\r\n# Get a pretrained pytorch model\r\nmlp_encoder = timm.create_model(\"mixer_b16_224\", pretrained=True, num_classes=0)\r\n\r\n# Transpile it into a keras.Model with the corresponding parameters\r\nnoise = torch.randn(1, 3, 224, 224)\r\nmlp_encoder = ivy.transpile(mlp_encoder, to=\"tensorflow\", args=(noise,))\r\n\r\n# Build a classifier using the transpiled encoder\r\nclass Classifier(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.encoder = mlp_encoder\r\n        self.output_dense = tf.keras.layers.Dense(units=1000, activation=\"softmax\")\r\n\r\n    def call(self, x):\r\n        x = self.encoder(x)\r\n        return self.output_dense(x)\r\n\r\n# Transform the classifier and use it as a standard keras.Model\r\nx = tf.random.normal(shape=(1, 3, 224, 224))\r\nmodel = Classifier()\r\nret = model(x)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From JAX</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport jax\r\nimport tensorflow as tf\r\n\r\n# Get a pretrained haiku model\r\n# https://ivy.dev/demos/scripts/deepmind_perceiver_io.py\r\nfrom deepmind_perceiver_io import key, perceiver_backbone\r\n\r\n# Transpile it into a tf.keras.Model with the corresponding parameters\r\ndummy_input = jax.random.uniform(key, shape=(1, 3, 224, 224))\r\nparams = perceiver_backbone.init(rng=key, images=dummy_input)\r\nbackbone = ivy.transpile(\r\n    perceiver_backbone, to=\"tensorflow\", params_v=params, args=(dummy_input,)\r\n)\r\n\r\n# Build a classifier using the transpiled backbone\r\nclass PerceiverIOClassifier(tf.keras.Model):\r\n    def __init__(self, num_classes=20):\r\n        super().__init__()\r\n        self.backbone = backbone\r\n        self.max_pool = tf.keras.layers.MaxPooling1D(pool_size=512)\r\n        self.flatten = tf.keras.layers.Flatten()\r\n        self.fc = tf.keras.layers.Dense(num_classes)\r\n\r\n    def call(self, x):\r\n        x = self.backbone(x)\r\n        x = self.flatten(self.max_pool(x))\r\n        return self.fc(x)\r\n\r\n# Initialize a trainable, customizable, tf.keras.Model\r\nx = tf.random.normal(shape=(1, 3, 224, 224))\r\nclassifier = PerceiverIOClassifier()\r\nret = classifier(x)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary>Any library</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From PyTorch</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport kornia\r\nimport requests\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom PIL import Image\r\n\r\n# transpile kornia from torch to tensorflow\r\ntf_kornia = ivy.transpile(kornia, source=\"torch\", to=\"tensorflow\")\r\n\r\n# get an image\r\nurl = \"http://images.cocodataset.org/train2017/000000000034.jpg\"\r\nraw_img = Image.open(requests.get(url, stream=True).raw)\r\n\r\n# convert it to the format expected by kornia\r\nimg = np.array(raw_img)\r\nimg = tf.transpose(tf.constant(img), (2, 0, 1))\r\nimg = tf.expand_dims(img, 0) / 255\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = tf_kornia.enhance.sharpness(img, 5)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From JAX</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport rax\r\nimport tensorflow as tf\r\n\r\n# transpile rax from jax to tensorflow\r\ntf_rax = ivy.transpile(rax, source=\"jax\", to=\"tensorflow\")\r\n\r\n# get some arrays\r\nscores = tf.constant([2.2, 1.3, 5.4])\r\nlabels = tf.constant([1.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = tf_rax.poly1_softmax_loss(scores, labels)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From NumPy</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport madmom\r\nimport tensorflow as tf\r\n\r\n# transpile madmom from numpy to tensorflow\r\ntf_madmom = ivy.transpile(madmom, source=\"numpy\", to=\"tensorflow\")\r\n\r\n# get some arrays\r\nfreqs = tf.range(20) * 10\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = tf_madmom.audio.filters.hz2midi(freqs)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary>Any function</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From PyTorch</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport torch\r\nimport tensorflow as tf\r\n\r\ndef loss(predictions, targets):\r\n    return torch.sqrt(torch.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from torch to tensorflow\r\ntf_loss = ivy.transpile(loss, source=\"torch\", to=\"tensorflow\")\r\n\r\n# get some arrays\r\np = tf.constant([3.0, 2.0, 1.0])\r\nt = tf.constant([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = tf_loss(p, t)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From JAX</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport jax.numpy as jnp\r\nimport tensorflow as tf\r\n\r\ndef loss(predictions, targets):\r\n    return jnp.sqrt(jnp.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from jax to tensorflow\r\ntf_loss = ivy.transpile(loss, source=\"jax\", to=\"tensorflow\")\r\n\r\n# get some arrays\r\np = tf.constant([3.0, 2.0, 1.0])\r\nt = tf.constant([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = tf_loss(p, t)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From NumPy</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef loss(predictions, targets):\r\n    return np.sqrt(np.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from numpy to tensorflow\r\ntf_loss = ivy.transpile(loss, source=\"numpy\", to=\"tensorflow\")\r\n\r\n# get some arrays\r\np = tf.constant([3.0, 2.0, 1.0])\r\nt = tf.constant([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = tf_loss(p, t)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary><b>I'm using Jax&ensp;<img class=\"dark-light\" src=\"https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/jax_small_logo.png\"></b></summary>\r\n<blockquote>You can use Ivy to get JAX code from:\r\n<details>\r\n<summary>Any model</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From PyTorch</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport timm\r\nimport torch\r\nimport jax\r\nimport haiku as hk\r\n\r\n# Get a pretrained pytorch model\r\nmlp_encoder = timm.create_model(\"mixer_b16_224\", pretrained=True, num_classes=0)\r\n\r\n# Transpile it into a hk.Module with the corresponding parameters\r\nnoise = torch.randn(1, 3, 224, 224)\r\nmlp_encoder = ivy.transpile(mlp_encoder, source=\"torch\", to=\"haiku\", args=(noise,))\r\n\r\n# Build a classifier using the transpiled encoder\r\nclass Classifier(hk.Module):\r\n    def __init__(self, num_classes=1000):\r\n        super().__init__()\r\n        self.encoder = mlp_encoder()\r\n        self.fc = hk.Linear(output_size=num_classes, with_bias=True)\r\n\r\n    def __call__(self, x):\r\n        x = self.encoder(x)\r\n        x = self.fc(x)\r\n        return x\r\n\r\ndef _forward_classifier(x):\r\n    module = Classifier()\r\n    return module(x)\r\n\r\n# Transform the classifier and use it as a standard hk.Module\r\nrng_key = jax.random.PRNGKey(42)\r\nx = jax.random.uniform(key=rng_key, shape=(1, 3, 224, 224), dtype=jax.numpy.float32)\r\nforward_classifier = hk.transform(_forward_classifier)\r\nparams = forward_classifier.init(rng=rng_key, x=x)\r\n\r\nret = forward_classifier.apply(params, None, x)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From TensorFlow</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport jax\r\nimport haiku as hk\r\nimport tensorflow as tf\r\njax.config.update(\"jax_enable_x64\", True)\r\n\r\n# Get a pretrained keras model\r\neff_encoder = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(\r\n    include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3)\r\n)\r\n\r\n# Transpile it into a hk.Module with the corresponding parameters\r\nnoise = tf.random.normal(shape=(1, 224, 224, 3))\r\nhk_eff_encoder = ivy.transpile(eff_encoder, source=\"tensorflow\", to=\"haiku\", args=(noise,))\r\n\r\n# Build a classifier using the transpiled encoder\r\nclass Classifier(hk.Module):\r\n    def __init__(self, num_classes=1000):\r\n        super().__init__()\r\n        self.encoder = hk_eff_encoder()\r\n        self.fc = hk.Linear(output_size=num_classes, with_bias=True)\r\n\r\n    def __call__(self, x):\r\n        x = self.encoder(x)\r\n        x = self.fc(x)\r\n        return x\r\n\r\ndef _forward_classifier(x):\r\n    module = Classifier()\r\n    return module(x)\r\n\r\n# Transform the classifier and use it as a standard hk.Module\r\nrng_key = jax.random.PRNGKey(42)\r\ndummy_x = jax.random.uniform(key=rng_key, shape=(1, 224, 224, 3))\r\nforward_classifier = hk.transform(_forward_classifier)\r\nparams = forward_classifier.init(rng=rng_key, x=dummy_x)\r\n\r\nret = forward_classifier.apply(params, None, dummy_x)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary>Any library</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From PyTorch</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport kornia\r\nimport requests\r\nimport jax.numpy as jnp\r\nfrom PIL import Image\r\njax.config.update(\"jax_enable_x64\", True)\r\n\r\n# transpile kornia from torch to jax\r\njax_kornia = ivy.transpile(kornia, source=\"torch\", to=\"jax\")\r\n\r\n# get an image\r\nurl = \"http://images.cocodataset.org/train2017/000000000034.jpg\"\r\nraw_img = Image.open(requests.get(url, stream=True).raw)\r\n\r\n# convert it to the format expected by kornia\r\nimg = jnp.transpose(jnp.array(raw_img), (2, 0, 1))\r\nimg = jnp.expand_dims(img, 0) / 255\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = jax_kornia.enhance.sharpness(img, 5)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From TensorFlow</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport jax\r\nimport os\r\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\r\nimport segmentation_models as sm\r\n\r\n# transpile sm from tensorflow to jax\r\njax_sm = ivy.transpile(sm, source=\"tensorflow\", to=\"jax\")\r\n\r\n# get some image-like arrays\r\nkey = jax.random.PRNGKey(23)\r\nkey1, key2 = jax.random.split(key)\r\noutput = jax.random.uniform(key1, (1, 3, 512, 512))\r\ntarget = jax.random.uniform(key2, (1, 3, 512, 512))\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = jax_sm.metrics.iou_score(output, target)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From NumPy</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport madmom\r\nimport jax.numpy as jnp\r\n\r\n# transpile madmon from numpy to jax\r\njax_madmom = ivy.transpile(madmom, source=\"numpy\", to=\"jax\")\r\n\r\n# get some arrays\r\nfreqs = jnp.arange(20) * 10\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = jax_madmom.audio.filters.hz2midi(freqs)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary>Any function</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From PyTorch</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport torch\r\nimport jax.numpy as jnp\r\n\r\ndef loss(predictions, targets):\r\n    return torch.sqrt(torch.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from torch to jax\r\njax_loss = ivy.transpile(loss, source=\"torch\", to=\"jax\")\r\n\r\n# get some arrays\r\np = jnp.array([3.0, 2.0, 1.0])\r\nt = jnp.array([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = jax_loss(p, t)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From TensorFlow</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport tensorflow as tf\r\nimport jax.numpy as jnp\r\n\r\ndef loss(predictions, targets):\r\n    return tf.sqrt(tf.reduce_mean(tf.square(predictions - targets)))\r\n\r\n# transpile any function from tf to jax\r\njax_loss = ivy.transpile(loss, source=\"tensorflow\", to=\"jax\")\r\n\r\n# get some arrays\r\np = jnp.array([3.0, 2.0, 1.0])\r\nt = jnp.array([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = jax_loss(p, t)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From NumPy</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport numpy as np\r\nimport jax\r\nimport jax.numpy as jnp\r\njax.config.update('jax_enable_x64', True)\r\n\r\ndef loss(predictions, targets):\r\n    return np.sqrt(np.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from numpy to jax\r\njax_loss = ivy.transpile(loss, source=\"numpy\", to=\"jax\")\r\n\r\n# get some arrays\r\np = jnp.array([3.0, 2.0, 1.0])\r\nt = jnp.array([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = jax_loss(p, t)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary><b>I'm using NumPy&ensp;<img class=\"dark-light\" src=\"https://raw.githubusercontent.com/unifyai/unifyai.github.io/main/img/externally_linked/logos/supported/numpy_small_logo.png\"></b></summary>\r\n<blockquote>You can use Ivy to get NumPy code from:\r\n<details>\r\n<summary>Any library</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From PyTorch</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport kornia\r\nimport requests\r\nimport numpy as np\r\nfrom PIL import Image\r\n\r\n# transpile kornia from torch to np\r\nnp_kornia = ivy.transpile(kornia, source=\"torch\", to=\"numpy\")\r\n\r\n# get an image\r\nurl = \"http://images.cocodataset.org/train2017/000000000034.jpg\"\r\nraw_img = Image.open(requests.get(url, stream=True).raw)\r\n\r\n# convert it to the format expected by kornia\r\nimg = np.transpose(np.array(raw_img), (2, 0, 1))\r\nimg = np.expand_dims(img, 0) / 255\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = np_kornia.enhance.sharpness(img, 5)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From TensorFlow</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport numpy as np\r\nimport os\r\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\r\nimport segmentation_models as sm\r\n\r\n# transpile sm from tensorflow to numpy\r\nnp_sm = ivy.transpile(sm, source=\"tensorflow\", to=\"numpy\")\r\n\r\n# get some image-like arrays\r\noutput = np.random.rand(1, 3, 512, 512).astype(dtype=np.float32)\r\ntarget = np.random.rand(1, 3, 512, 512).astype(dtype=np.float32)\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = np_sm.metrics.iou_score(output, target)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From Jax</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport rax\r\nimport numpy as np\r\n\r\n# transpile rax from jax to numpy\r\nnp_rax = ivy.transpile(rax, source=\"jax\", to=\"numpy\")\r\n\r\n# get some arrays\r\nscores = np.array([2.2, 1.3, 5.4])\r\nlabels = np.array([1.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version of any function from the library!\r\nout = np_rax.poly1_softmax_loss(scores, labels)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n<details>\r\n<summary>Any function</summary>\r\n<blockquote>\r\n<details>\r\n   <summary>From PyTorch</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport torch\r\nimport numpy as np\r\n\r\ndef loss(predictions, targets):\r\n    return torch.sqrt(torch.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from torch to numpy\r\nnp_loss = ivy.transpile(loss, source=\"torch\", to=\"numpy\")\r\n\r\n# get some arrays\r\np = np.array([3.0, 2.0, 1.0])\r\nt = np.array([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = np_loss(p, t)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From TensorFlow</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef loss(predictions, targets):\r\n    return tf.sqrt(tf.reduce_mean(tf.square(predictions - targets)))\r\n\r\n# transpile any function from tf to numpy\r\nnp_loss = ivy.transpile(loss, source=\"tensorflow\", to=\"numpy\")\r\n\r\n# get some arrays\r\np = np.array([3.0, 2.0, 1.0])\r\nt = np.array([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = np_loss(p, t)\r\n```\r\n\r\n</details>\r\n<details>\r\n   <summary>From JAX</summary>\r\n\r\n``` python\r\nimport ivy\r\nimport jax.numpy as jnp\r\nimport numpy as np\r\n\r\ndef loss(predictions, targets):\r\n    return jnp.sqrt(jnp.mean((predictions - targets) ** 2))\r\n\r\n# transpile any function from jax to numpy\r\nnp_loss = ivy.transpile(loss, source=\"jax\", to=\"numpy\")\r\n\r\n# get some arrays\r\np = np.array([3.0, 2.0, 1.0])\r\nt = np.array([0.0, 0.0, 0.0])\r\n\r\n# and use the transpiled version!\r\nout = np_loss(p, t)\r\n```\r\n\r\n</details>\r\n</blockquote>\r\n</details>\r\n\r\n</blockquote>\r\n</details> -->\r\n\r\n<br>\r\n\r\n# How ivy works?\r\n\r\nLet's take a look at how Ivy works as a transpiler in more detail to get an idea of why and where to use it.\r\n\r\n<blockquote>\r\n<details>\r\n<summary>When is Ivy's transpiler useful?</summary>\r\n\r\nIf you want to use building blocks published in other frameworks (neural\r\nnetworks, layers, array computing libraries, training pipelines\\...),\r\nyou want to integrate code developed in various frameworks, or maybe\r\nstraight up migrate code from one framework to another or even between versions of the same framework, the transpiler is\r\ndefinitely the tool for the job! You can use the converted code just\r\nas if it was code originally developed in that framework, applying\r\nframework-specific optimizations or tools, instantly exposing your\r\nproject to all of the unique perks of a different framework.\r\n</details>\r\n</blockquote>\r\n\r\n\\\r\nIvy\\'s transpiler allows you to use code from any other framework (or\r\nfrom any other version of the same framework!) in your own code, by just\r\nadding one line of code.\r\n\r\nThis way, Ivy makes all ML-related projects available for you,\r\nindependently of the framework you want to use to research, develop, or\r\ndeploy systems. Feel free to head over to the docs for the full API\r\nreference, but the functions you\\'d most likely want to use are:\r\n\r\n``` python\r\n# Converts framework-specific code to a target framework of choice. See usage in the documentation\r\nivy.transpile()\r\n\r\n# Traces an efficient fully-functional graph from a function, removing all wrapping and redundant code. See usage in the documentation\r\nivy.trace_graph()\r\n```\r\n\r\n#### `ivy.transpile` will eagerly transpile if a class or function is provided\r\n\r\n``` python\r\nimport ivy\r\nimport torch\r\nimport tensorflow as tf\r\n\r\ndef torch_fn(x):\r\n    x = torch.abs(x)\r\n    return torch.sum(x)\r\n\r\nx1 = torch.tensor([1., 2.])\r\nx1 = tf.convert_to_tensor([1., 2.])\r\n\r\n# Transpilation happens eagerly\r\ntf_fn = ivy.transpile(test_fn, source=\"torch\", target=\"tensorflow\")\r\n\r\n# tf_fn is now tensorflow code and runs efficiently\r\nret = tf_fn(x1)\r\n```\r\n\r\n#### `ivy.transpile` will lazily transpile if a module (library) is provided\r\n\r\n``` python\r\nimport ivy\r\nimport kornia\r\nimport tensorflow as tf\r\n\r\nx2 = tf.random.normal((5, 3, 4, 4))\r\n\r\n# Module is provided -> transpilation happens lazily\r\ntf_kornia = ivy.transpile(kornia, source=\"torch\", target=\"tensorflow\")\r\n\r\n# The transpilation is initialized here, and this function is converted to tensorflwo\r\nret = tf_kornia.color.rgb_to_grayscale(x2)\r\n\r\n# Transpilation has already occurred, the tensorflow function runs efficiently\r\nret = tf_kornia.color.rgb_to_grayscale(x2)\r\n```\r\n\r\n#### `ivy.trace_graph` can be used eagerly or lazily\r\nIf you pass the necessary arguments for function tracing, the graph tracing step will\r\nhappen instantly (eagerly). Otherwise, the graph tracing\r\nwill happen only when the returned function is first invoked.\r\n\r\n``` python\r\nimport ivy\r\nimport jax\r\nivy.set_backend(\"jax\")\r\n\r\n# Simple JAX function to transpile\r\ndef test_fn(x):\r\n    return jax.numpy.sum(x)\r\n\r\nx1 = ivy.array([1., 2.])\r\n```\r\n\r\n``` python\r\n# Arguments are available -> tracing happens eagerly\r\neager_graph = ivy.trace_graph(test_fn, to=\"jax\", args=(x1,))\r\n\r\n# eager_graph now runs efficiently\r\nret = eager_graph(x1)\r\n```\r\n\r\n``` python\r\n# Arguments are not available -> tracing happens lazily\r\nlazy_graph = ivy.trace_graph(test_fn, to=\"jax\")\r\n\r\n# The traced graph is initialized, tracing will happen here\r\nret = lazy_graph(x1)\r\n\r\n# Tracing has already happend, traced graph runs efficiently\r\nret = lazy_graph(x1)\r\n```\r\n\r\nIf you want to learn more, you can find more information in the [Ivy as\r\na transpiler section of the\r\ndocs!](https://docs.ivy.dev/overview/design/ivy_as_a_transpiler.html)\r\n\r\n\r\n<br>\r\n\r\n# Documentation\r\n\r\nYou can find Ivy's documentation on the [Docs page](https://docs.ivy.dev/), which includes:\r\n- [Motivation](https://docs.ivy.dev/overview/motivation.html): This contextualizes the problem Ivy is trying to solve by going over\r\n    - The current [ML Explosion](https://docs.ivy.dev/overview/motivation/ml_explosion.html#ml-explosion).\r\n    - Explaining why it is important [to solve this problem](https://www.docs.ivy.dev/overview/motivation/why_transpile.html#why-transpile).\r\n- [Related Work](https://docs.ivy.dev/overview/related_work.html): Which paints a picture of the role Ivy plays in the ML stack, comparing it to other existing solutions in terms of functionalities and abstraction level.\r\n- [Design](https://docs.ivy.dev/overview/design.html): A user-focused guide about the design decision behind the architecture and the main building blocks of Ivy.\r\n- [Deep Dive](https://docs.ivy.dev/overview/deep_dive.html): Which delves deeper into the implementation details of Ivy and is oriented towards potential contributors to the code base.\r\n\r\n\r\n<br>\r\n\r\n# Contributing\r\n\r\nWe believe that everyone can contribute and make a difference. Whether\r\nit\\'s writing code, fixing bugs, or simply sharing feedback,\r\nyour contributions are definitely welcome and appreciated \ud83d\ude4c\r\n\r\nCheck out all of our [Open Tasks](https://docs.ivy.dev/overview/contributing/open_tasks.html),\r\nand find out more info in our [Contributing guide](https://docs.ivy.dev/overview/contributing.html)\r\nin the docs! Or to immediately dive into a useful task, look for any failing tests on our [Test Dashboard](https://github.com/ivy-llc/ivy-tests-dashboard/blob/main/DASHBOARD.md)!\r\n\r\n\r\n<br>\r\n\r\n# Community\r\n\r\n<a href=\"https://github.com/ivy-llc/ivy/graphs/contributors\">\r\n  <img class=\"dark-light\" src=\"https://contrib.rocks/image?repo=ivy-llc/ivy&anon=0&columns=20&max=100&r=true\" />\r\n</a>\r\n\r\n<br>\r\n<br>\r\n\r\nJoin our growing community on a mission to make conversions between frameworks simple and accessible to all!\r\nWhether you are a seasoned developer or just starting out, you\\'ll find a place here! Join the Ivy community on\r\nour [Discord](https://discord.gg/uYRmyPxMQq) \ud83d\udc7e server, which is the\r\nperfect place to ask questions, share ideas, and get help from both\r\nfellow developers and the Ivy Team directly.\r\n\r\n<b> See you there! </b>\r\n\r\n\r\n<br>\r\n\r\n# Citation\r\n\r\nIf you use Ivy for your work, please don\\'t forget to give proper credit\r\nby including the accompanying [paper](https://arxiv.org/abs/2102.02886)\r\n\ud83d\udcc4 in your references. It\\'s a small way to show appreciation and help\r\nto continue to support this and other open source projects \ud83d\ude4c\r\n\r\n\r\n    @article{lenton2021ivy,\r\n      title={Ivy: Templated deep learning for inter-framework portability},\r\n      author={Lenton, Daniel and Pardo, Fabio and Falck, Fabian and James, Stephen and Clark, Ronald},\r\n      journal={arXiv preprint arXiv:2102.02886},\r\n      year={2021}\r\n    }\r\n",
        "releases": [
            {
                "name": "Ivy v1.0.0.2",
                "date": "2025-01-21T16:32:24Z"
            },
            {
                "name": "Ivy v1.0.0.1",
                "date": "2024-11-03T19:38:51Z"
            },
            {
                "name": "Ivy v1.0.0.0",
                "date": "2024-10-25T15:54:26Z"
            },
            {
                "name": "Pre-Release v0.0.9.17",
                "date": "2024-10-25T09:06:43Z"
            },
            {
                "name": "Pre-Release v0.0.9.16",
                "date": "2024-10-24T10:15:18Z"
            },
            {
                "name": "Pre-Release v0.0.9.15",
                "date": "2024-10-22T11:36:47Z"
            },
            {
                "name": "Pre-Release v0.0.9.14",
                "date": "2024-10-22T05:03:48Z"
            },
            {
                "name": "Pre-Release v0.0.9.13",
                "date": "2024-10-20T16:43:00Z"
            },
            {
                "name": "Pre-Release v0.0.9.12",
                "date": "2024-10-20T12:51:52Z"
            },
            {
                "name": "Pre-Release v0.0.9.10",
                "date": "2024-10-15T03:59:40Z"
            },
            {
                "name": "Pre-Release v0.0.9.9",
                "date": "2024-10-14T04:09:51Z"
            },
            {
                "name": "Pre-Release v0.0.9.8",
                "date": "2024-09-25T02:47:40Z"
            },
            {
                "name": "Pre-Release v0.0.9.7",
                "date": "2024-08-09T04:28:32Z"
            },
            {
                "name": "Pre-Release v0.0.9.6",
                "date": "2024-08-02T11:46:29Z"
            },
            {
                "name": "Pre-Release v0.0.9.5",
                "date": "2024-07-26T03:07:31Z"
            },
            {
                "name": "Pre-Release v0.0.9.4",
                "date": "2024-07-08T15:02:46Z"
            },
            {
                "name": "Pre-Release v0.0.9.3",
                "date": "2024-07-05T00:31:31Z"
            },
            {
                "name": "Test Release v0.0.9.2",
                "date": "2024-06-24T17:56:54Z"
            },
            {
                "name": "Test Release v0.0.9.1",
                "date": "2024-06-05T10:26:04Z"
            },
            {
                "name": "Test Release v0.0.9.0",
                "date": "2024-03-15T11:43:59Z"
            },
            {
                "name": "Test Release v0.0.8.0",
                "date": "2024-03-07T07:32:30Z"
            },
            {
                "name": "Test Release v0.0.7.5",
                "date": "2024-03-01T08:09:26Z"
            },
            {
                "name": "Test Release v0.0.7.4",
                "date": "2024-02-28T14:47:39Z"
            },
            {
                "name": "Test Release v0.0.7.3",
                "date": "2024-02-28T13:09:51Z"
            },
            {
                "name": "Test Release v0.0.7.2",
                "date": "2024-02-21T22:02:02Z"
            },
            {
                "name": "Test Release v0.0.7.1",
                "date": "2024-02-20T12:59:23Z"
            },
            {
                "name": "Test Release v0.0.7.0",
                "date": "2024-02-20T08:05:04Z"
            },
            {
                "name": "Test Release v0.0.6.2",
                "date": "2024-01-22T13:15:57Z"
            },
            {
                "name": "Test Release v0.0.6.1",
                "date": "2024-01-22T13:01:01Z"
            },
            {
                "name": "Test Release v0.0.6.0",
                "date": "2024-01-22T12:43:48Z"
            },
            {
                "name": "Test Release v0.0.5.1",
                "date": "2024-01-04T08:06:19Z"
            },
            {
                "name": "Test Release v0.0.5.0",
                "date": "2024-01-03T17:12:42Z"
            },
            {
                "name": "Test Release v0.0.4.0",
                "date": "2023-10-11T17:02:57Z"
            },
            {
                "name": "Test Release v0.0.3.0",
                "date": "2023-09-04T10:22:03Z"
            },
            {
                "name": "Test Release v0.0.2.0",
                "date": "2023-08-21T10:13:58Z"
            }
        ]
    }
}