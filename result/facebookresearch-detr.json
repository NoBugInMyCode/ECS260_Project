{
    "https://api.github.com/repos/facebookresearch/detr": {
        "forks": 2500,
        "watchers": 13902,
        "stars": 13902,
        "languages": {
            "Python": 169188,
            "Dockerfile": 328
        },
        "commits": [
            "2024-03-12T15:58:25Z",
            "2023-02-07T10:12:31Z",
            "2022-03-07T12:59:53Z",
            "2021-10-18T10:06:31Z",
            "2021-10-08T08:37:51Z",
            "2021-09-15T12:56:07Z",
            "2021-06-30T10:31:23Z",
            "2020-11-15T16:03:45Z",
            "2020-09-28T12:18:45Z",
            "2020-09-23T15:19:30Z",
            "2020-09-21T09:55:14Z",
            "2020-09-21T09:30:17Z",
            "2020-09-19T19:52:12Z",
            "2020-08-19T09:46:29Z",
            "2020-08-11T17:45:34Z",
            "2020-08-11T10:42:26Z",
            "2020-08-07T11:52:07Z",
            "2020-08-05T14:01:02Z",
            "2020-08-03T12:03:09Z",
            "2020-07-10T17:34:11Z",
            "2020-07-10T16:49:35Z",
            "2020-07-10T12:38:34Z",
            "2020-07-09T13:32:27Z",
            "2020-06-30T09:47:05Z",
            "2020-06-29T15:00:30Z",
            "2020-06-29T14:35:13Z",
            "2020-06-28T16:39:54Z",
            "2020-06-28T13:59:10Z",
            "2020-06-22T20:28:47Z",
            "2020-06-15T13:45:32Z"
        ],
        "creation_date": "2020-05-26T23:54:52Z",
        "contributors": 24,
        "topics": [],
        "subscribers": 150,
        "readme": "**DE\u2af6TR**: End-to-End Object Detection with Transformers\n========\n\n[![Support Ukraine](https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB)](https://opensource.fb.com/support-ukraine)\n\nPyTorch training code and pretrained models for **DETR** (**DE**tection **TR**ansformer).\nWe replace the full complex hand-crafted object detection pipeline with a Transformer, and match Faster R-CNN with a ResNet-50, obtaining **42 AP** on COCO using half the computation power (FLOPs) and the same number of parameters. Inference in 50 lines of PyTorch.\n\n![DETR](.github/DETR.png)\n\n**What it is**. Unlike traditional computer vision techniques, DETR approaches object detection as a direct set prediction problem. It consists of a set-based global loss, which forces unique predictions via bipartite matching, and a Transformer encoder-decoder architecture. \nGiven a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. Due to this parallel nature, DETR is very fast and efficient.\n\n**About the code**. We believe that object detection should not be more difficult than classification,\nand should not require complex libraries for training and inference.\nDETR is very simple to implement and experiment with, and we provide a\n[standalone Colab Notebook](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb)\nshowing how to do inference with DETR in only a few lines of PyTorch code.\nTraining code follows this idea - it is not a library,\nbut simply a [main.py](main.py) importing model and criterion\ndefinitions with standard training loops.\n\nAdditionnally, we provide a Detectron2 wrapper in the d2/ folder. See the readme there for more information.\n\nFor details see [End-to-End Object Detection with Transformers](https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\n\nSee our [blog post](https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers/) to learn more about end to end object detection with transformers.\n# Model Zoo\nWe provide baseline DETR and DETR-DC5 models, and plan to include more in future.\nAP is computed on COCO 2017 val5k, and inference time is over the first 100 val5k COCO images,\nwith torchscript transformer.\n\n<table>\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>backbone</th>\n      <th>schedule</th>\n      <th>inf_time</th>\n      <th>box AP</th>\n      <th>url</th>\n      <th>size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>DETR</td>\n      <td>R50</td>\n      <td>500</td>\n      <td>0.036</td>\n      <td>42.0</td>\n      <td><a href=\"https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detr/logs/detr-r50_log.txt\">logs</a></td>\n      <td>159Mb</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DETR-DC5</td>\n      <td>R50</td>\n      <td>500</td>\n      <td>0.083</td>\n      <td>43.3</td>\n      <td><a href=\"https://dl.fbaipublicfiles.com/detr/detr-r50-dc5-f0fb7ef5.pth\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detr/logs/detr-r50-dc5_log.txt\">logs</a></td>\n      <td>159Mb</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DETR</td>\n      <td>R101</td>\n      <td>500</td>\n      <td>0.050</td>\n      <td>43.5</td>\n      <td><a href=\"https://dl.fbaipublicfiles.com/detr/detr-r101-2c7b67e5.pth\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detr/logs/detr-r101_log.txt\">logs</a></td>\n      <td>232Mb</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DETR-DC5</td>\n      <td>R101</td>\n      <td>500</td>\n      <td>0.097</td>\n      <td>44.9</td>\n      <td><a href=\"https://dl.fbaipublicfiles.com/detr/detr-r101-dc5-a2e86def.pth\">model</a>&nbsp;|&nbsp;<a href=\"https://dl.fbaipublicfiles.com/detr/logs/detr-r101-dc5_log.txt\">logs</a></td>\n      <td>232Mb</td>\n    </tr>\n  </tbody>\n</table>\n\nCOCO val5k evaluation results can be found in this [gist](https://gist.github.com/szagoruyko/9c9ebb8455610958f7deaa27845d7918).\n\nThe models are also available via torch hub,\nto load DETR R50 with pretrained weights simply do:\n```python\nmodel = torch.hub.load('facebookresearch/detr:main', 'detr_resnet50', pretrained=True)\n```\n\n\nCOCO panoptic val5k models:\n<table>\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>backbone</th>\n      <th>box AP</th>\n      <th>segm AP</th>\n      <th>PQ</th>\n      <th>url</th>\n      <th>size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>DETR</td>\n      <td>R50</td>\n      <td>38.8</td>\n      <td>31.1</td>\n      <td>43.4</td>\n      <td><a href=\"https://dl.fbaipublicfiles.com/detr/detr-r50-panoptic-00ce5173.pth\">download</a></td>\n      <td>165Mb</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DETR-DC5</td>\n      <td>R50</td>\n      <td>40.2</td>\n      <td>31.9</td>\n      <td>44.6</td>\n      <td><a href=\"https://dl.fbaipublicfiles.com/detr/detr-r50-dc5-panoptic-da08f1b1.pth\">download</a></td>\n      <td>165Mb</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DETR</td>\n      <td>R101</td>\n      <td>40.1</td>\n      <td>33</td>\n      <td>45.1</td>\n      <td><a href=\"https://dl.fbaipublicfiles.com/detr/detr-r101-panoptic-40021d53.pth\">download</a></td>\n      <td>237Mb</td>\n    </tr>\n  </tbody>\n</table>\n\nCheckout our [panoptic colab](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/DETR_panoptic.ipynb)\nto see how to use and visualize DETR's panoptic segmentation prediction.\n\n# Notebooks\n\nWe provide a few notebooks in colab to help you get a grasp on DETR:\n* [DETR's hands on Colab Notebook](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_attention.ipynb): Shows how to load a model from hub, generate predictions, then visualize the attention of the model (similar to the figures of the paper)\n* [Standalone Colab Notebook](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb): In this notebook, we demonstrate how to implement a simplified version of DETR from the grounds up in 50 lines of Python, then visualize the predictions. It is a good starting point if you want to gain better understanding the architecture and poke around before diving in the codebase.\n* [Panoptic Colab Notebook](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/DETR_panoptic.ipynb): Demonstrates how to use DETR for panoptic segmentation and plot the predictions.\n\n\n# Usage - Object detection\nThere are no extra compiled components in DETR and package dependencies are minimal,\nso the code is very simple to use. We provide instructions how to install dependencies via conda.\nFirst, clone the repository locally:\n```\ngit clone https://github.com/facebookresearch/detr.git\n```\nThen, install PyTorch 1.5+ and torchvision 0.6+:\n```\nconda install -c pytorch pytorch torchvision\n```\nInstall pycocotools (for evaluation on COCO) and scipy (for training):\n```\nconda install cython scipy\npip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n```\nThat's it, should be good to train and evaluate detection models.\n\n(optional) to work with panoptic install panopticapi:\n```\npip install git+https://github.com/cocodataset/panopticapi.git\n```\n\n## Data preparation\n\nDownload and extract COCO 2017 train and val images with annotations from\n[http://cocodataset.org](http://cocodataset.org/#download).\nWe expect the directory structure to be the following:\n```\npath/to/coco/\n  annotations/  # annotation json files\n  train2017/    # train images\n  val2017/      # val images\n```\n\n## Training\nTo train baseline DETR on a single node with 8 gpus for 300 epochs run:\n```\npython -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --coco_path /path/to/coco \n```\nA single epoch takes 28 minutes, so 300 epoch training\ntakes around 6 days on a single machine with 8 V100 cards.\nTo ease reproduction of our results we provide\n[results and training logs](https://gist.github.com/szagoruyko/b4c3b2c3627294fc369b899987385a3f)\nfor 150 epoch schedule (3 days on a single machine), achieving 39.5/60.3 AP/AP50.\n\nWe train DETR with AdamW setting learning rate in the transformer to 1e-4 and 1e-5 in the backbone.\nHorizontal flips, scales and crops are used for augmentation.\nImages are rescaled to have min size 800 and max size 1333.\nThe transformer is trained with dropout of 0.1, and the whole model is trained with grad clip of 0.1.\n\n\n## Evaluation\nTo evaluate DETR R50 on COCO val5k with a single GPU run:\n```\npython main.py --batch_size 2 --no_aux_loss --eval --resume https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth --coco_path /path/to/coco\n```\nWe provide results for all DETR detection models in this\n[gist](https://gist.github.com/szagoruyko/9c9ebb8455610958f7deaa27845d7918).\nNote that numbers vary depending on batch size (number of images) per GPU.\nNon-DC5 models were trained with batch size 2, and DC5 with 1,\nso DC5 models show a significant drop in AP if evaluated with more\nthan 1 image per GPU.\n\n## Multinode training\nDistributed training is available via Slurm and [submitit](https://github.com/facebookincubator/submitit):\n```\npip install submitit\n```\nTrain baseline DETR-6-6 model on 4 nodes for 300 epochs:\n```\npython run_with_submitit.py --timeout 3000 --coco_path /path/to/coco\n```\n\n# Usage - Segmentation\n\nWe show that it is relatively straightforward to extend DETR to predict segmentation masks. We mainly demonstrate strong panoptic segmentation results.\n\n## Data preparation\n\nFor panoptic segmentation, you need the panoptic annotations additionally to the coco dataset (see above for the coco dataset). You need to download and extract the [annotations](http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip).\nWe expect the directory structure to be the following:\n```\npath/to/coco_panoptic/\n  annotations/  # annotation json files\n  panoptic_train2017/    # train panoptic annotations\n  panoptic_val2017/      # val panoptic annotations\n```\n\n## Training\n\nWe recommend training segmentation in two stages: first train DETR to detect all the boxes, and then train the segmentation head.\nFor panoptic segmentation, DETR must learn to detect boxes for both stuff and things classes. You can train it on a single node with 8 gpus for 300 epochs with:\n```\npython -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --coco_path /path/to/coco  --coco_panoptic_path /path/to/coco_panoptic --dataset_file coco_panoptic --output_dir /output/path/box_model\n```\nFor instance segmentation, you can simply train a normal box model (or used a pre-trained one we provide).\n\nOnce you have a box model checkpoint, you need to freeze it, and train the segmentation head in isolation.\nFor panoptic segmentation you can train on a single node with 8 gpus for 25 epochs:\n```\npython -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --masks --epochs 25 --lr_drop 15 --coco_path /path/to/coco  --coco_panoptic_path /path/to/coco_panoptic  --dataset_file coco_panoptic --frozen_weights /output/path/box_model/checkpoint.pth --output_dir /output/path/segm_model\n```\nFor instance segmentation only, simply remove the `dataset_file` and `coco_panoptic_path` arguments from the above command line.\n\n# License\nDETR is released under the Apache 2.0 license. Please see the [LICENSE](LICENSE) file for more information.\n\n# Contributing\nWe actively welcome your pull requests! Please see [CONTRIBUTING.md](.github/CONTRIBUTING.md) and [CODE_OF_CONDUCT.md](.github/CODE_OF_CONDUCT.md) for more info.\n",
        "releases": [
            {
                "name": "Detectron2 and torchscript support, attention and panoptic notebooks, code improvements",
                "date": "2020-06-29T16:41:01Z"
            }
        ]
    }
}