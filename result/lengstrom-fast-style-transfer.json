{
    "https://api.github.com/repos/lengstrom/fast-style-transfer": {
        "forks": 2599,
        "watchers": 10934,
        "stars": 10934,
        "languages": {
            "Python": 28429,
            "Shell": 212
        },
        "commits": [
            "2021-09-18T21:01:16Z",
            "2021-07-27T21:47:23Z",
            "2021-07-27T21:44:53Z",
            "2021-03-23T18:08:30Z",
            "2020-11-13T17:04:30Z",
            "2020-05-14T18:55:53Z",
            "2020-05-05T01:29:33Z",
            "2020-05-05T01:29:00Z",
            "2020-04-11T09:59:11Z",
            "2020-04-10T23:52:41Z",
            "2019-12-10T14:22:02Z",
            "2019-12-10T05:09:45Z",
            "2019-12-10T05:03:38Z",
            "2019-08-14T17:23:19Z",
            "2019-08-03T04:47:42Z",
            "2018-05-04T03:56:31Z",
            "2018-03-28T04:13:08Z",
            "2018-01-22T22:31:51Z",
            "2018-01-04T00:55:09Z",
            "2018-01-04T00:51:23Z",
            "2017-12-16T20:56:45Z",
            "2017-11-18T14:25:53Z",
            "2017-06-18T05:32:23Z",
            "2017-01-11T15:42:40Z",
            "2017-01-09T15:49:18Z",
            "2017-01-09T15:02:18Z",
            "2017-01-09T14:36:07Z",
            "2017-04-20T18:39:22Z",
            "2017-04-20T18:23:58Z",
            "2017-02-23T23:26:04Z"
        ],
        "creation_date": "2016-07-21T02:59:11Z",
        "contributors": 15,
        "topics": [
            "deep-learning",
            "neural-networks",
            "neural-style",
            "style-transfer"
        ],
        "subscribers": 320,
        "readme": "## Fast Style Transfer in [TensorFlow](https://github.com/tensorflow/tensorflow)\n\nAdd styles from famous paintings to any photo in a fraction of a second! [You can even style videos!](#video-stylization)\n\n<p align = 'center'>\n<img src = 'examples/style/udnie.jpg' height = '246px'>\n<img src = 'examples/content/stata.jpg' height = '246px'>\n<a href = 'examples/results/stata_udnie.jpg'><img src = 'examples/results/stata_udnie_header.jpg' width = '627px'></a>\n</p>\n<p align = 'center'>\nIt takes 100ms on a 2015 Titan X to style the MIT Stata Center (1024\u00d7680) like Udnie, by Francis Picabia.\n</p>\n\nOur implementation is based off of a combination of Gatys' [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576), Johnson's [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](http://cs.stanford.edu/people/jcjohns/eccv16/), and Ulyanov's [Instance Normalization](https://arxiv.org/abs/1607.08022). \n\n### Sponsorship\nPlease consider sponsoring my work on this project!\n\n### License\nCopyright (c) 2016 Logan Engstrom. Contact me for commercial use (or rather any use that is not academic research) (email: engstrom at my university's domain dot edu). Free for research use, as long as proper attribution is given and this copyright notice is retained.\n\n## Video Stylization \nHere we transformed every frame in a video, then combined the results. [Click to go to the full demo on YouTube!](https://www.youtube.com/watch?v=xVJwwWQlQ1o) The style here is Udnie, as above.\n<div align = 'center'>\n     <a href = 'https://www.youtube.com/watch?v=xVJwwWQlQ1o'>\n        <img src = 'examples/results/fox_udnie.gif' alt = 'Stylized fox video. Click to go to YouTube!' width = '800px' height = '400px'>\n     </a>\n</div>\n\nSee how to generate these videos [here](#stylizing-video)!\n\n## Image Stylization\nWe added styles from various paintings to a photo of Chicago. Click on thumbnails to see full applied style images.\n<div align='center'>\n<img src = 'examples/content/chicago.jpg' height=\"200px\">\n</div>\n     \n<div align = 'center'>\n<a href = 'examples/style/wave.jpg'><img src = 'examples/thumbs/wave.jpg' height = '200px'></a>\n<img src = 'examples/results/chicago_wave.jpg' height = '200px'>\n<img src = 'examples/results/chicago_udnie.jpg' height = '200px'>\n<a href = 'examples/style/udnie.jpg'><img src = 'examples/thumbs/udnie.jpg' height = '200px'></a>\n<br>\n<a href = 'examples/style/rain_princess.jpg'><img src = 'examples/thumbs/rain_princess.jpg' height = '200px'></a>\n<img src = 'examples/results/chicago_rain_princess.jpg' height = '200px'>\n<img src = 'examples/results/chicago_la_muse.jpg' height = '200px'>\n<a href = 'examples/style/la_muse.jpg'><img src = 'examples/thumbs/la_muse.jpg' height = '200px'></a>\n\n<br>\n<a href = 'examples/style/the_shipwreck_of_the_minotaur.jpg'><img src = 'examples/thumbs/the_shipwreck_of_the_minotaur.jpg' height = '200px'></a>\n<img src = 'examples/results/chicago_wreck.jpg' height = '200px'>\n<img src = 'examples/results/chicago_the_scream.jpg' height = '200px'>\n<a href = 'examples/style/the_scream.jpg'><img src = 'examples/thumbs/the_scream.jpg' height = '200px'></a>\n</div>\n\n## Implementation Details\nOur implementation uses TensorFlow to train a fast style transfer network. We use roughly the same transformation network as described in Johnson, except that batch normalization is replaced with Ulyanov's instance normalization, and the scaling/offset of the output `tanh` layer is slightly different. We use a loss function close to the one described in Gatys, using VGG19 instead of VGG16 and typically using \"shallower\" layers than in Johnson's implementation (e.g. we use `relu1_1` rather than `relu1_2`). Empirically, this results in larger scale style features in transformations.\n## Virtual Environment Setup (Anaconda) - Windows/Linux\nTested on\n| Spec                        |                                                             |\n|-----------------------------|-------------------------------------------------------------|\n| Operating System            | Windows 10 Home                                             |\n| GPU                         | Nvidia GTX 2080 TI                                          |\n| CUDA Version                | 11.0                                                        |\n| Driver Version              | 445.75                                                      |\n### Step 1\uff1aInstall Anaconda\nhttps://docs.anaconda.com/anaconda/install/\n### Step 2\uff1aBuild a virtual environment\nRun the following commands in sequence in Anaconda Prompt:\n```\nconda create -n tf-gpu tensorflow-gpu=2.1.0\nconda activate tf-gpu\nconda install jupyterlab\njupyter lab\n```\nRun the following command in the notebook or just conda install the package:\n```\n!pip install moviepy==1.0.2\n```\nFollow the commands below to use fast-style-transfer\n## Documentation\n### Training Style Transfer Networks\nUse `style.py` to train a new style transfer network. Run `python style.py` to view all the possible parameters. Training takes 4-6 hours on a Maxwell Titan X. [More detailed documentation here](docs.md#stylepy). **Before you run this, you should run `setup.sh`**. Example usage:\n\n    python style.py --style path/to/style/img.jpg \\\n      --checkpoint-dir checkpoint/path \\\n      --test path/to/test/img.jpg \\\n      --test-dir path/to/test/dir \\\n      --content-weight 1.5e1 \\\n      --checkpoint-iterations 1000 \\\n      --batch-size 20\n\n### Evaluating Style Transfer Networks\nUse `evaluate.py` to evaluate a style transfer network. Run `python evaluate.py` to view all the possible parameters. Evaluation takes 100 ms per frame (when batch size is 1) on a Maxwell Titan X. [More detailed documentation here](docs.md#evaluatepy). Takes several seconds per frame on a CPU. **Models for evaluation are [located here](https://drive.google.com/drive/folders/0B9jhaT37ydSyRk9UX0wwX3BpMzQ?resourcekey=0-Z9LcNHC-BTB4feKwm4loXw&usp=sharing)**. Example usage:\n\n    python evaluate.py --checkpoint path/to/style/model.ckpt \\\n      --in-path dir/of/test/imgs/ \\\n      --out-path dir/for/results/\n\n### Stylizing Video\nUse `transform_video.py` to transfer style into a video. Run `python transform_video.py` to view all the possible parameters. Requires `ffmpeg`. [More detailed documentation here](docs.md#transform_videopy). Example usage:\n\n    python transform_video.py --in-path path/to/input/vid.mp4 \\\n      --checkpoint path/to/style/model.ckpt \\\n      --out-path out/video.mp4 \\\n      --device /gpu:0 \\\n      --batch-size 4\n\n### Requirements\nYou will need the following to run the above:\n- TensorFlow 0.11.0\n- Python 2.7.9, Pillow 3.4.2, scipy 0.18.1, numpy 1.11.2\n- If you want to train (and don't want to wait for 4 months):\n  - A decent GPU\n  - All the required NVIDIA software to run TF on a GPU (cuda, etc)\n- ffmpeg 3.1.3 if you want to stylize video\n\n### Citation\n```\n  @misc{engstrom2016faststyletransfer,\n    author = {Logan Engstrom},\n    title = {Fast Style Transfer},\n    year = {2016},\n    howpublished = {\\url{https://github.com/lengstrom/fast-style-transfer/}},\n    note = {commit xxxxxxx}\n  }\n```\n\n### Attributions/Thanks\n- This project could not have happened without the advice (and GPU access) given by [Anish Athalye](http://www.anishathalye.com/). \n  - The project also borrowed some code from Anish's [Neural Style](https://github.com/anishathalye/neural-style/)\n- Some readme/docs formatting was borrowed from Justin Johnson's [Fast Neural Style](https://github.com/jcjohnson/fast-neural-style)\n- The image of the Stata Center at the very beginning of the README was taken by [Juan Paulo](https://juanpaulo.me/)\n\n### Related Work\n- Michael Ramos ported this network [to use CoreML on iOS](https://medium.com/@rambossa/diy-prisma-fast-style-transfer-app-with-coreml-and-tensorflow-817c3b90dacd)\n",
        "releases": []
    }
}