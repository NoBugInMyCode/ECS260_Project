{
    "https://api.github.com/repos/baowenbo/DAIN": {
        "forks": 839,
        "watchers": 8260,
        "stars": 8260,
        "languages": {
            "Python": 324315,
            "Cuda": 165725,
            "C++": 45873,
            "Jupyter Notebook": 12952,
            "Shell": 1809
        },
        "commits": [
            "2023-02-13T12:40:11Z",
            "2023-02-13T12:39:44Z",
            "2020-11-19T08:13:06Z",
            "2020-10-29T04:16:31Z",
            "2020-07-12T12:25:42Z",
            "2020-06-29T17:23:34Z",
            "2020-06-27T22:21:05Z",
            "2020-06-27T22:20:42Z",
            "2020-06-27T22:19:30Z",
            "2020-06-27T21:58:15Z",
            "2020-06-19T15:39:28Z",
            "2020-06-07T17:43:19Z",
            "2020-06-07T15:13:02Z",
            "2020-06-07T00:50:35Z",
            "2020-05-25T12:06:31Z",
            "2020-05-25T12:05:39Z",
            "2020-05-25T12:04:02Z",
            "2020-05-25T10:14:50Z",
            "2020-05-13T02:24:09Z",
            "2020-05-01T12:04:37Z",
            "2020-04-18T07:28:26Z",
            "2020-03-27T22:01:43Z",
            "2020-03-27T11:26:24Z",
            "2020-03-27T11:22:08Z",
            "2020-03-23T08:12:47Z",
            "2020-03-23T08:11:13Z",
            "2020-03-23T07:53:29Z",
            "2020-03-23T05:00:21Z",
            "2020-03-23T04:40:42Z",
            "2020-03-23T04:35:05Z"
        ],
        "creation_date": "2019-03-22T02:37:19Z",
        "contributors": 8,
        "topics": [],
        "subscribers": 185,
        "readme": "# DAIN (Depth-Aware Video Frame Interpolation)\n[Project](https://sites.google.com/view/wenbobao/dain) **|** [Paper](http://arxiv.org/abs/1904.00830)\n\n[Wenbo Bao](https://sites.google.com/view/wenbobao/home),\n[Wei-Sheng Lai](http://graduatestudents.ucmerced.edu/wlai24/), \n[Chao Ma](https://sites.google.com/site/chaoma99/),\nXiaoyun Zhang, \nZhiyong Gao, \nand [Ming-Hsuan Yang](http://faculty.ucmerced.edu/mhyang/)\n\nIEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CVPR 2019\n\nThis work is developed based on our TPAMI work [MEMC-Net](https://github.com/baowenbo/MEMC-Net), where we propose the adaptive warping layer. Please also consider referring to it.\n\n### Table of Contents\n1. [Introduction](#introduction)\n1. [Citation](#citation)\n1. [Requirements and Dependencies](#requirements-and-dependencies)\n1. [Installation](#installation)\n1. [Testing Pre-trained Models](#testing-pre-trained-models)\n1. [Downloading Results](#downloading-results)\n1. [Slow-motion Generation](#slow-motion-generation)\n1. [Training New Models](#training-new-models)\n1. [Google Colab Demo](#google-colab-demo)\n\n### Introduction\nWe propose the **D**epth-**A**ware video frame **IN**terpolation (**DAIN**) model to explicitly detect the occlusion by exploring the depth cue.\nWe develop a depth-aware flow projection layer to synthesize intermediate flows that preferably sample closer objects than farther ones.\nOur method achieves state-of-the-art performance on the Middlebury dataset. \nWe provide videos [here](https://www.youtube.com/watch?v=-f8f0igQi5I&t=5s).\n\n<!--![teaser](http://vllab.ucmerced.edu/wlai24/LapSRN/images/emma_text.gif)-->\n\n<!--[![teaser](https://img.youtube.com/vi/icJ0WbPsE20/0.jpg)](https://www.youtube.com/watch?v=icJ0WbPsE20&feature=youtu.be)\n<!--<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/icJ0WbPsE20\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n![teaser](http://vllab1.ucmerced.edu/~wenbobao/DAIN/kart-turn_compare.gif)\n\n\n<!--\u54c8\u54c8\u6211\u662f\u6ce8\u91ca\uff0c\u4e0d\u4f1a\u5728\u6d4f\u89c8\u5668\u4e2d\u663e\u793a\u3002\nBeanbags\nhttps://drive.google.com/open?id=170vdxANGoNKO5_8MYOuiDvoIXzucv7HW\nDimentrodon\nhttps://drive.google.com/open?id=14n7xvb9hjTKqfcr7ZpEFyfMvx6E8NhD_\nDogDance\nhttps://drive.google.com/open?id=1YWAyAJ3T48fMFv2K8j8wIVcmQm39cRof\nGrove2\nhttps://drive.google.com/open?id=1sJLwdQdL6JYXSQo_Bev0aQMleWacxCsN\nGrove3\nhttps://drive.google.com/open?id=1jGj3UdGppoJO02Of8ZaNXqDH4fnXuQ8O\nHydrangea\nhttps://drive.google.com/open?id=1_4kVlhvrmCv54aXi7vZMk3-FtRQF7s0s\nMiniCooper\nhttps://drive.google.com/open?id=1pWHtyBSZsOTC7NTVdHTrv1W-dxa95BLo\nRubberWhale\nhttps://drive.google.com/open?id=1korbXsGpSgJn7THBHkLRVrJMtCt5YZPB\nUrban2\nhttps://drive.google.com/open?id=1v57RMm9x5vM36mCgPy5hresXDZWtw3Vs\nUrban3\nhttps://drive.google.com/open?id=1LMwSU0PrG4_GaDjWRI2v9hvWpYwzRKca\nVenus\nhttps://drive.google.com/open?id=1piPnEexuHaiAr4ZzWSAxGi1u1Xo_6vPp\nWalking\nhttps://drive.google.com/open?id=1CgCLmVC_WTVTAcA_IdWbLqR8MS18zHoa\n-->\n\n<p float=\"middle\">\n<img src=\"https://drive.google.com/uc?export=view&id=1YWAyAJ3T48fMFv2K8j8wIVcmQm39cRof\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=1CgCLmVC_WTVTAcA_IdWbLqR8MS18zHoa\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=1pWHtyBSZsOTC7NTVdHTrv1W-dxa95BLo\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=170vdxANGoNKO5_8MYOuiDvoIXzucv7HW\" width=\"200\"/>\n</p>\n\n<p float=\"middle\">\n<img src=\"https://drive.google.com/uc?export=view&id=1sJLwdQdL6JYXSQo_Bev0aQMleWacxCsN\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=1jGj3UdGppoJO02Of8ZaNXqDH4fnXuQ8O\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=1v57RMm9x5vM36mCgPy5hresXDZWtw3Vs\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=1LMwSU0PrG4_GaDjWRI2v9hvWpYwzRKca\" width=\"200\"/>\n</p>\n\n<p float=\"middle\">\n<img src=\"https://drive.google.com/uc?export=view&id=1piPnEexuHaiAr4ZzWSAxGi1u1Xo_6vPp\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=1korbXsGpSgJn7THBHkLRVrJMtCt5YZPB\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=1_4kVlhvrmCv54aXi7vZMk3-FtRQF7s0s\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=14n7xvb9hjTKqfcr7ZpEFyfMvx6E8NhD_\" width=\"200\"/>\n</p>\n\n### Citation\nIf you find the code and datasets useful in your research, please cite:\n\n    @inproceedings{DAIN,\n        author    = {Bao, Wenbo and Lai, Wei-Sheng and Ma, Chao and Zhang, Xiaoyun and Gao, Zhiyong and Yang, Ming-Hsuan}, \n        title     = {Depth-Aware Video Frame Interpolation}, \n        booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},\n        year      = {2019}\n    }\n    @article{MEMC-Net,\n         title={MEMC-Net: Motion Estimation and Motion Compensation Driven Neural Network for Video Interpolation and Enhancement},\n         author={Bao, Wenbo and Lai, Wei-Sheng, and Zhang, Xiaoyun and Gao, Zhiyong and Yang, Ming-Hsuan},\n         journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n         doi={10.1109/TPAMI.2019.2941941},\n         year={2018}\n    }\n\n### Requirements and Dependencies\n- Ubuntu (We test with Ubuntu = 16.04.5 LTS)\n- Python (We test with Python = 3.6.8 in Anaconda3 = 4.1.1)\n- Cuda & Cudnn (We test with Cuda = 9.0 and Cudnn = 7.0)\n- PyTorch (The customized depth-aware flow projection and other layers require ATen API in PyTorch = 1.0.0)\n- GCC (Compiling PyTorch 1.0.0 extension files (.c/.cu) requires gcc = 4.9.1 and nvcc = 9.0 compilers)\n- NVIDIA GPU (We use Titan X (Pascal) with compute = 6.1, but we support compute_50/52/60/61 devices, should you have devices with higher compute capability, please revise [this](https://github.com/baowenbo/DAIN/blob/master/my_package/DepthFlowProjection/setup.py))\n\n### Installation\nDownload repository:\n\n    $ git clone https://github.com/baowenbo/DAIN.git\n\nBefore building Pytorch extensions, be sure you have `pytorch >= 1.0.0`:\n    \n    $ python -c \"import torch; print(torch.__version__)\"\n    \nGenerate our PyTorch extensions:\n    \n    $ cd DAIN\n    $ cd my_package \n    $ ./build.sh\n\nGenerate the Correlation package required by [PWCNet](https://github.com/NVlabs/PWC-Net/tree/master/PyTorch/external_packages/correlation-pytorch-master):\n    \n    $ cd ../PWCNet/correlation_package_pytorch1_0\n    $ ./build.sh\n\n\n### Testing Pre-trained Models\nMake model weights dir and Middlebury dataset dir:\n\n    $ cd DAIN\n    $ mkdir model_weights\n    $ mkdir MiddleBurySet\n    \nDownload pretrained models, \n\n    $ cd model_weights\n    $ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/best.pth\n    \nand Middlebury dataset:\n    \n    $ cd ../MiddleBurySet\n    $ wget http://vision.middlebury.edu/flow/data/comp/zip/other-color-allframes.zip\n    $ unzip other-color-allframes.zip\n    $ wget http://vision.middlebury.edu/flow/data/comp/zip/other-gt-interp.zip\n    $ unzip other-gt-interp.zip\n    $ cd ..\n\npreinstallations:\n\n    $ cd PWCNet/correlation_package_pytorch1_0\n    $ sh build.sh\n    $ cd ../my_package\n    $ sh build.sh\n    $ cd ..\n\nWe are good to go by:\n\n    $ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury.py\n\nThe interpolated results are under `MiddleBurySet/other-result-author/[random number]/`, where the `random number` is used to distinguish different runnings. \n\n### Downloading Results\nOur DAIN model achieves the state-of-the-art performance on the UCF101, Vimeo90K, and Middlebury ([*eval*](http://vision.middlebury.edu/flow/eval/results/results-n1.php) and *other*).\nDownload our interpolated results with:\n    \n    $ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/UCF101_DAIN.zip\n    $ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/Vimeo90K_interp_DAIN.zip\n    $ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/Middlebury_eval_DAIN.zip\n    $ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/Middlebury_other_DAIN.zip\n    \n    \n### Slow-motion Generation\nOur model is fully capable of generating slow-motion effect with minor modification on the network architecture.\nRun the following code by specifying `time_step = 0.25` to generate x4 slow-motion effect:\n\n    $ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury_slowmotion.py --netName DAIN_slowmotion --time_step 0.25\n\nor set `time_step` to `0.125` or `0.1` as follows \n\n    $ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury_slowmotion.py --netName DAIN_slowmotion --time_step 0.125\n    $ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury_slowmotion.py --netName DAIN_slowmotion --time_step 0.1\nto generate x8 and x10 slow-motion respectively. Or if you would like to have x100 slow-motion for a little fun.\n    \n    $ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury_slowmotion.py --netName DAIN_slowmotion --time_step 0.01\n\nYou may also want to create gif animations by:\n    \n    $ cd MiddleBurySet/other-result-author/[random number]/Beanbags\n    $ convert -delay 1 *.png -loop 0 Beanbags.gif //1*10ms delay \n\nHave fun and enjoy yourself! \n\n\n### Training New Models\nDownload the Vimeo90K triplet dataset for video frame interpolation task, also see [here](https://github.com/anchen1011/toflow/blob/master/download_dataset.sh) by [Xue et al., IJCV19](https://arxiv.org/abs/1711.09078).\n    \n    $ cd DAIN\n    $ mkdir /path/to/your/dataset & cd /path/to/your/dataset \n    $ wget http://data.csail.mit.edu/tofu/dataset/vimeo_triplet.zip\n    $ unzip vimeo_triplet.zip\n    $ rm vimeo_triplet.zip\n\nDownload the pretrained MegaDepth and PWCNet models\n    \n    $ cd MegaDepth/checkpoints/test_local\n    $ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/best_generalization_net_G.pth\n    $ cd ../../../PWCNet\n    $ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/pwc_net.pth.tar\n    $ cd  ..\n    \nRun the training script:\n\n    $ CUDA_VISIBLE_DEVICES=0 python train.py --datasetPath /path/to/your/dataset --batch_size 1 --save_which 1 --lr 0.0005 --rectify_lr 0.0005 --flow_lr_coe 0.01 --occ_lr_coe 0.0 --filter_lr_coe 1.0 --ctx_lr_coe 1.0 --alpha 0.0 1.0 --patience 4 --factor 0.2\n    \nThe optimized models will be saved to the `model_weights/[random number]` directory, where [random number] is generated for different runs.\n\nReplace the pre-trained `model_weights/best.pth` model with the newly trained `model_weights/[random number]/best.pth` model.\nThen test the new model by executing: \n\n    $ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury.py\n\n### Google Colab Demo\nThis is a modification of DAIN that allows the usage of Google Colab and is able to do a full demo interpolation from a source video to a target video.\n\nOriginal Notebook File by btahir can be found [here](https://github.com/baowenbo/DAIN/issues/44).\n\nTo use the Colab, follow these steps:\n\n- Download the `Colab_DAIN.ipynb` file ([link](https://raw.githubusercontent.com/baowenbo/DAIN/master/Colab_DAIN.ipynb)).\n- Visit Google Colaboratory ([link](https://colab.research.google.com/))\n- Select the \"Upload\" option, and upload the `.ipynb` file\n- Start running the cells one by one, following the instructions.\n\nColab file authors: [Styler00Dollar](https://github.com/styler00dollar) and [Alpha](https://github.com/AlphaGit).\n\n### Contact\n[Wenbo Bao](mailto:bwb0813@gmail.com); [Wei-Sheng (Jason) Lai](mailto:phoenix104104@gmail.com)\n\n### License\nSee [MIT License](https://github.com/baowenbo/DAIN/blob/master/LICENSE)\n",
        "releases": []
    }
}