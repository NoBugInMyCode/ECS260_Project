{
    "https://api.github.com/repos/rasbt/python-machine-learning-book-2nd-edition": {
        "forks": 2821,
        "watchers": 7145,
        "stars": 7145,
        "languages": {
            "Jupyter Notebook": 51707310,
            "Python": 302935,
            "HTML": 4823,
            "Shell": 1879,
            "CSS": 133
        },
        "commits": [
            "2019-12-31T17:13:18Z",
            "2019-12-21T05:37:41Z",
            "2019-11-15T20:49:48Z",
            "2019-11-01T15:34:57Z",
            "2019-10-22T00:22:05Z",
            "2019-03-23T17:47:43Z",
            "2018-11-20T00:11:49Z",
            "2018-11-17T20:51:39Z",
            "2018-08-29T18:07:51Z",
            "2018-08-16T15:31:05Z",
            "2018-08-16T15:30:37Z",
            "2018-07-24T05:40:30Z",
            "2018-07-24T04:21:40Z",
            "2018-07-24T01:12:16Z",
            "2018-07-24T01:09:19Z",
            "2018-07-15T20:17:39Z",
            "2018-07-15T20:09:59Z",
            "2018-07-15T20:04:41Z",
            "2018-07-15T19:04:06Z",
            "2018-07-05T02:35:58Z",
            "2018-07-02T18:49:24Z",
            "2018-06-30T04:25:31Z",
            "2018-06-30T04:05:57Z",
            "2018-06-30T02:46:30Z",
            "2018-06-29T02:55:52Z",
            "2018-06-29T02:47:04Z",
            "2018-06-28T02:50:04Z",
            "2018-06-28T02:20:21Z",
            "2018-06-24T02:43:26Z",
            "2018-06-24T02:12:38Z"
        ],
        "creation_date": "2017-02-09T05:45:15Z",
        "contributors": 2,
        "topics": [
            "data-science",
            "deep-learning",
            "machine-learning",
            "python",
            "scikit-learn",
            "tensorflow"
        ],
        "subscribers": 374,
        "readme": "## Python Machine Learning (2nd Ed.) Code Repository\n\n[![Build Status](https://travis-ci.com/rasbt/python-machine-learning-book-2nd-edition.svg?token=zvSsJVLJFKzB2yqaeKN1&branch=master)](https://travis-ci.com/rasbt/python-machine-learning-book-2nd-edition)\n![Python 3.6](https://img.shields.io/badge/Python-3.6-blue.svg)\n![License](https://img.shields.io/badge/Code%20License-MIT-blue.svg)\n\n\n*Please note that a new edition (3rd edition) is now available as of December 2019. The code repository link for the 3rd edition is https://github.com/rasbt/python-machine-learning-book-3rd-edition.*\n\n\n**Python Machine Learning, 2nd Ed.**  \n\npublished September 20th, 2017\n\nPaperback: 622 pages  \nPublisher: Packt Publishing  \nLanguage: English\n\nISBN-10: 1787125939  \nISBN-13: 978-1787125933  \nKindle ASIN: B0742K7HYF  \n\n[<img src=\"./images/cover_1.jpg\" width=\"348\">](https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1787125939)\n\n\n## Links\n\n- [Amazon Page](https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1787125939)\n- [Packt Page](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning-second-edition)\n\n\n\n## Table of Contents and Code Notebooks\n\n**Helpful installation and setup instructions can be found in the [README.md file of Chapter 1](code/ch01/README.md)**\n\nTo access the code materials for a given chapter, simply click on the `open dir` links next to the chapter headlines to navigate to the chapter subdirectories located in the [code/](code/) subdirectory. You can also click on the `ipynb` links below to open and view the Jupyter notebook of each chapter directly on GitHub.\n\nIn addition, the [code/](code/) subdirectories also contain .py script files, which were created from the Jupyter Notebooks. However, I highly recommend working with the Jupyter notebook if possible in your computing environment. Not only do the Jupyter notebooks contain the images and section headings for easier navigation, but they also allow for a stepwise execution of individual code snippets, which -- in my opinion -- provide a better learning experience.\n\n**Please note that these are just the code examples accompanying the book, which I uploaded for your convenience; be aware that these notebooks may not be useful without the formulae and descriptive text.**   \n\n\n1. Machine Learning - Giving Computers the Ability to Learn from Data [[open dir](./code/ch01)] [[ipynb](./code/ch01/ch01.ipynb)] \n2. Training Machine Learning Algorithms for Classification [[open dir](./code/ch02)] [[ipynb](./code/ch02/ch02.ipynb)] \n3. A Tour of Machine Learning Classifiers Using Scikit-Learn [[open dir](./code/ch03)] [[ipynb](./code/ch03/ch03.ipynb)] \n4. Building Good Training Sets \u2013 Data Pre-Processing [[open dir](./code/ch04)] [[ipynb](./code/ch04/ch04.ipynb)] \n5. Compressing Data via Dimensionality Reduction [[open dir](./code/ch05)] [[ipynb](./code/ch05/ch05.ipynb)] \n6. Learning Best Practices for Model Evaluation and Hyperparameter Optimization [[open dir](./code/ch06)] [[ipynb](./code/ch06/ch06.ipynb)]\n7. Combining Different Models for Ensemble Learning [[open dir](./code/ch07)] [[ipynb](./code/ch07/ch07.ipynb)]\n8. Applying Machine Learning to Sentiment Analysis [[open dir](./code/ch08)] [[ipynb](./code/ch08/ch08.ipynb)] \n9. Embedding a Machine Learning Model into a Web Application [[open dir](./code/ch09)] [[ipynb](./code/ch09/ch09.ipynb)] \n10. Predicting Continuous Target Variables with Regression Analysis [[open dir](./code/ch10)] [[ipynb](./code/ch10/ch10.ipynb)] \n11. Working with Unlabeled Data \u2013 Clustering Analysis [[open dir](./code/ch11)] [[ipynb](./code/ch11/ch11.ipynb)] \n12. Implementing a Multi-layer Artificial Neural Network from Scratch [[open dir](./code/ch12)] [[ipynb](./code/ch12/ch12.ipynb)] \n13. Parallelizing Neural Network Training with TensorFlow [[open dir](./code/ch13)] [[ipynb](./code/ch13/ch13.ipynb)] \n14. Going Deeper: The Mechanics of TensorFlow [[open dir](./code/ch14)] [[ipynb](./code/ch14/ch14.ipynb)] \n15. Classifying Images with Deep Convolutional Neural Networks [[open dir](./code/ch15)] [[ipynb](./code/ch15/ch15.ipynb)] \n16. Modeling Sequential Data Using Recurrent Neural Networks [[open dir](./code/ch16)] [[ipynb](./code/ch16/ch16.ipynb)] \n\n### What\u2019s new in the second edition from the first edition?\n\n> Oh, there are so many things that we improved or added; where should I start!? The one issue on top of my priority list was to fix all the nasty typos that were introduced during the layout stage or my oversight. I really appreciated all the helpful feedback from readers in this manner! Furthermore, I addressed all the feedback about sections that may have been confusing or a bit unclear, reworded paragraphs, and added additional explanations. Also, special thanks go to the excellent editors of the second edition, who helped a lot along the way! \n\n> Also, the figures and plots became much prettier. While readers liked the graphic content a lot, some people criticized the PowerPoint-esque style and layout. Thus, I decided to overhaul every little figure with a hopefully more pleasing choice of fonts and colors. Also, the data plots look much nicer now, thanks to the matplotlib team who put a lot of work in matplotlib 2.0 and its new styling theme.\n\n> Beyond all these cosmetic fixes, new sections were added here and there. Among these is, for example, is a section on dealing with imbalanced datasets, which several readers were missing in the first edition and short section on Latent Dirichlet Allocation among others.\n\n> As time and the software world moved on after the first edition was released in September 2015, we decided to replace the introduction to deep learning via Theano. No worries, we didn't remove it but it got a substantial overhaul and is now based on TensorFlow, which has become a major player in my research toolbox since its open source release by Google in November 2015. \nAlong with the new introduction to deep learning using TensorFlow, the biggest additions to this new edition are three brand new chapters focussing on deep learning applications: A more detailed overview of the TensorFlow mechanics, an introduction to convolutional neural networks for image classification, and an introduction to recurrent neural networks for natural language processing. Of course, and in a similar vein as the rest of the book, these new chapters do not only provide readers with practical instructions and examples but also introduce the fundamental mathematics behind those concepts, which are an essential building block for understanding how deep learning works.\n\n[ [Excerpt from \"Machine Learning can be useful in almost every problem domain:\" An interview with Sebastian Raschka](https://www.packtpub.com/books/content/machine-learning-useful-every-problem-domain-interview-sebastian-raschka/) ]\n\n\n--- \n\n<br>\n<br>\n\nRaschka, Sebastian, and Vahid Mirjalili. *Python Machine Learning, 2nd Ed*. Packt Publishing, 2017.\n\n    @book{RaschkaMirjalili2017,  \n    address = {Birmingham, UK},  \n    author = {Raschka, Sebastian and Mirjalili, Vahid},  \n    edition = {2},  \n    isbn = {978-1787125933},  \n    keywords = {Clustering,Data Science,Deep Learning,  \n                Machine Learning,Neural Networks,Programming,  \n                Supervised Learning},  \n    publisher = {Packt Publishing},  \n    title = {{Python Machine Learning, 2nd Ed.}},  \n    year = {2017}  \n    }\n\n# Translations\n\n### German\n\n- ISBN-10: 3958457339\n- ISBN-13: 978-3958457331\n- [Amazon.de link](https://www.amazon.de/Machine-Learning-Python-Scikit-Learn-TensorFlow/dp/3958457339/ref=tmm_pap_swatch_0?_encoding=UTF8&qid=1513601461&sr=8-5)\n- [Publisher link](https://mitp.de/IT-WEB/Programmierung/Machine-Learning-mit-Python-oxid.html)\n\n![](images/cover-german.jpg)\n\n\n### Japanese\n\n- ISBN-10: 4295003379\n- ISBN-13: 978-4295003373\n- [Amazon.co.jp link](https://www.amazon.co.jp/Python-\u6a5f\u68b0\u5b66\u7fd2\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0-\u9054\u4eba\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30c6\u30a3\u30b9\u30c8\u306b\u3088\u308b\u7406\u8ad6\u3068\u5b9f\u8df5-impress-gear/dp/4295003379/ref=tmm_pap_swatch_0)\n\n![](images/cover-japanese.jpg)\n",
        "releases": []
    }
}