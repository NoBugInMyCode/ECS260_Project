{
    "https://api.github.com/repos/jantic/DeOldify": {
        "forks": 2593,
        "watchers": 18141,
        "stars": 18141,
        "languages": {
            "Python": 717123,
            "Jupyter Notebook": 291628,
            "Cuda": 9554,
            "C++": 2580,
            "Smarty": 1396
        },
        "commits": [
            "2024-10-19T17:17:35Z",
            "2024-10-19T17:17:25Z",
            "2024-10-19T17:03:33Z",
            "2024-10-19T17:03:03Z",
            "2024-07-27T12:13:15Z",
            "2024-06-02T19:10:36Z",
            "2023-08-26T18:28:28Z",
            "2023-04-16T16:39:31Z",
            "2023-04-16T16:37:52Z",
            "2023-03-29T15:13:03Z",
            "2023-02-15T14:28:21Z",
            "2023-02-15T07:04:31Z",
            "2023-02-11T23:49:47Z",
            "2023-02-11T23:47:52Z",
            "2023-02-11T23:47:20Z",
            "2023-02-11T23:06:41Z",
            "2023-02-09T18:26:15Z",
            "2023-01-31T19:59:45Z",
            "2023-01-31T19:53:30Z",
            "2022-11-22T11:04:46Z",
            "2022-11-11T02:52:09Z",
            "2022-11-10T21:19:06Z",
            "2022-11-10T21:18:43Z",
            "2022-09-22T00:53:34Z",
            "2022-09-22T00:50:51Z",
            "2022-09-22T00:46:58Z",
            "2022-09-19T09:34:29Z",
            "2022-09-19T09:30:48Z",
            "2022-09-19T09:27:18Z",
            "2022-09-19T09:23:30Z"
        ],
        "creation_date": "2018-10-31T23:32:34Z",
        "contributors": 30,
        "topics": [],
        "subscribers": 437,
        "readme": "\n# DeOldify\n\n**This Reposisitory is Archived**  This project was a wild ride since I started it back in 2018.  6 years ago as of this writing (October 19, 2024)!.  It's time for me to move on and put this repo in the archives as I simply don't have the time to attend to it anymore, and frankly it's ancient as far as deep-learning projects go at this point! ~Jason\n\n**Quick Start**: The easiest way to colorize images using open source DeOldify\n(for free!) is here: [DeOldify Image Colorization on DeepAI](https://deepai.org/machine-learning-model/colorizer)\n\n**Desktop**: Want to run open source DeOldify for photos and videos on the desktop?\n* Stable Diffusion Web UI Plugin- Photos and video, cross-platform (NEW!). <https://github.com/SpenserCai/sd-webui-deoldify>\n* ColorfulSoft Windows GUI- No GPU required! Photos/Windows only. <https://github.com/ColorfulSoft/DeOldify.NET>.\nNo GPU required!\n\n**In Browser (new!)**  Check out this Onnx-based in browser implementation:  https://github.com/akbartus/DeOldify-on-Browser\n\nThe **most advanced** version of DeOldify image colorization is available here,\nexclusively.  Try a few images for free! [MyHeritage In Color](https://www.myheritage.com/incolor)\n\n**Replicate:** Image: <a href=\"https://replicate.com/arielreplicate/deoldify_image\"><img src=\"https://replicate.com/arielreplicate/deoldify_image/badge\"></a> | Video: <a href=\"https://replicate.com/arielreplicate/deoldify_video\"><img src=\"https://replicate.com/arielreplicate/deoldify_video/badge\"></a>\n\n----------------------------\n\nImage (artistic) [![Colab for images](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb)\n| Video [![Colab for video](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb)\n\nHaving trouble with the default image colorizer, aka \"artistic\"?  Try the\n\"stable\" one below.  It generally won't produce colors that are as interesting as\n\"artistic\", but the glitches are noticeably reduced.\n\nImage (stable) [![Colab for stable model](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColabStable.ipynb)\n\nInstructions on how to use the Colabs above have been kindly provided in video\ntutorial form by Old Ireland in Colour's John Breslin.  It's great! Click video\nimage below to watch.\n\n[![DeOldify Tutorial](http://img.youtube.com/vi/VaEl0faDw38/0.jpg)](http://www.youtube.com/watch?v=VaEl0faDw38)\n\nGet more updates on [Twitter\n![Twitter logo](resource_images/twitter.svg)](https://twitter.com/DeOldify).\n\n## Table of Contents\n\n- [About DeOldify](#about-deoldify)\n- [Example Videos](#example-videos)\n- [Example Images](#example-images)\n- [Stuff That Should Probably Be In A Paper](#stuff-that-should-probably-be-in-a-paper)\n  - [How to Achieve Stable Video](#how-to-achieve-stable-video)\n  - [What is NoGAN?](#what-is-nogan)\n- [Why Three Models?](#why-three-models)\n- [Technical Details](#the-technical-details)\n- [Going Forward](#this-project-going-forward)\n- [Getting Started Yourself](#getting-started-yourself)\n  - [Easiest Approach](#easiest-approach)\n  - [Your Own Machine](#your-own-machine-not-as-easy)\n- [Pretrained Weights](#pretrained-weights)\n\n## About DeOldify\n\nSimply put, the mission of this project is to colorize and restore old images and\nfilm footage. We'll get into the details in a bit, but first let's see some\npretty pictures and videos!\n\n### New and Exciting Stuff in DeOldify\n\n- Glitches and artifacts are almost entirely eliminated\n- Better skin (less zombies)\n- More highly detailed and photorealistic renders\n- Much less \"blue bias\"\n- **Video** - it actually looks good!  \n- **NoGAN** - a new and weird but highly effective way to do GAN training for\n  image to image.\n\n## Example Videos\n\n**Note:**  Click images to watch\n\n### Facebook F8 Demo\n\n[![DeOldify Facebook F8 Movie Colorization Demo](http://img.youtube.com/vi/l3UXXid04Ys/0.jpg)](http://www.youtube.com/watch?v=l3UXXid04Ys)\n\n### Silent Movie Examples\n\n[![DeOldify Silent Movie Examples](http://img.youtube.com/vi/EXn-n2iqEjI/0.jpg)](http://www.youtube.com/watch?v=EXn-n2iqEjI)\n\n## Example Images\n\n\"Migrant Mother\" by Dorothea Lange (1936)\n\n![Migrant Mother](https://i.imgur.com/Bt0vnke.jpg)\n\nWoman relaxing in her livingroom in Sweden (1920)\n\n![Sweden Living Room](https://i.imgur.com/158d0oU.jpg)\n\n\"Toffs and Toughs\" by Jimmy Sime (1937)\n\n![Class Divide](https://i.imgur.com/VYuav4I.jpg)\n\nThanksgiving Maskers (1911)\n\n![Thanksgiving Maskers](https://i.imgur.com/n8qVJ5c.jpg)\n\nGlen Echo Madame Careta Gypsy Camp in Maryland (1925)\n\n![Gypsy Camp](https://i.imgur.com/1oYrJRI.jpg)\n\n\"Mr. and Mrs. Lemuel Smith and their younger children in their farm house,\nCarroll County, Georgia.\" (1941)\n\n![Georgia Farmhouse](https://i.imgur.com/I2j8ynm.jpg)\n\n\"Building the Golden Gate Bridge\" (est 1937)\n\n![Golden Gate Bridge](https://i.imgur.com/6SbFjfq.jpg)\n\n> **Note:**  What you might be wondering is while this render looks cool, are the\n> colors accurate? The original photo certainly makes it look like the towers of\n> the bridge could be white. We looked into this and it turns out the answer is\n> no - the towers were already covered in red primer by this time. So that's\n> something to keep in mind- historical accuracy remains a huge challenge!\n\n\"Terrasse de caf\u00e9, Paris\" (1925)\n\n![Cafe Paris](https://i.imgur.com/WprQwP5.jpg)\n\nNorwegian Bride (est late 1890s)\n\n![Norwegian Bride](https://i.imgur.com/MmtvrZm.jpg)\n\nZitk\u00e1la-\u0160\u00e1 (Lakota: Red Bird), also known as Gertrude Simmons Bonnin (1898)\n\n![Native Woman](https://i.imgur.com/zIGM043.jpg)\n\nChinese Opium Smokers (1880)\n\n![Opium Real](https://i.imgur.com/lVGq8Vq.jpg)\n\n## Stuff That Should Probably Be In A Paper\n\n### How to Achieve Stable Video\n\nNoGAN training is crucial to getting the kind of stable and colorful images seen\nin this iteration of DeOldify. NoGAN training combines the benefits of GAN\ntraining (wonderful colorization) while eliminating the nasty side effects\n(like flickering objects in video). Believe it or not, video is rendered using\nisolated image generation without any sort of temporal modeling tacked on. The\nprocess performs 30-60 minutes of the GAN portion of \"NoGAN\" training, using 1%\nto 3% of imagenet data once.  Then, as with still image colorization, we\n\"DeOldify\" individual frames before rebuilding the video.\n\nIn addition to improved video stability, there is an interesting thing going on\nhere worth mentioning. It turns out the models I run, even different ones and\nwith different training structures, keep arriving at more or less the same\nsolution.  That's even the case for the colorization of things you may think\nwould be arbitrary and unknowable, like the color of clothing, cars, and even\nspecial effects (as seen in \"Metropolis\").\n\n![Metropolis Special FX](https://thumbs.gfycat.com/HeavyLoneBlowfish-size_restricted.gif)\n\nMy best guess is that the models are learning some interesting rules about how to\ncolorize based on subtle cues present in the black and white images that I\ncertainly wouldn't expect to exist.  This result leads to nicely deterministic and\nconsistent results, and that means you don't have track model colorization\ndecisions because they're not arbitrary.  Additionally, they seem remarkably\nrobust so that even in moving scenes the renders are very consistent.\n\n![Moving Scene Example](https://thumbs.gfycat.com/FamiliarJubilantAsp-size_restricted.gif)\n\nOther ways to stabilize video add up as well. First, generally speaking rendering\nat a higher resolution (higher render_factor) will increase stability of\ncolorization decisions.  This stands to reason because the model has higher\nfidelity image information to work with and will have a greater chance of making\nthe \"right\" decision consistently.  Closely related to this is the use of\nresnet101 instead of resnet34 as the backbone of the generator- objects are\ndetected more consistently and correctly with this. This is especially important\nfor getting good, consistent skin rendering.  It can be particularly visually\njarring if you wind up with \"zombie hands\", for example.\n\n![Zombie Hand Example](https://thumbs.gfycat.com/ThriftyInferiorIsabellinewheatear-size_restricted.gif)\n\nAdditionally, gaussian noise augmentation during training appears to help but at\nthis point the conclusions as to just how much are bit more tenuous (I just\nhaven't formally measured this yet).  This is loosely based on work done in style\ntransfer video, described here:\n <https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42>.\n\nSpecial thanks go to Rani Horev for his contributions in implementing this noise\naugmentation.\n\n### What is NoGAN?\n\nThis is a new type of GAN training that I've developed to solve some key problems\nin the previous DeOldify model. It provides the benefits of GAN training while\nspending minimal time doing direct GAN training.  Instead, most of the training\ntime is spent pretraining the generator and critic separately with more\nstraight-forward, fast and reliable conventional methods.  A key insight here is\nthat those more \"conventional\" methods generally get you most of the results you\nneed, and that GANs can be used to close the gap on realism. During the very\nshort amount of actual GAN training the generator not only gets the full\nrealistic colorization capabilities that used to take days of progressively\nresized GAN training, but it also doesn't accrue nearly as much of the artifacts\nand other ugly baggage of GANs. In fact, you can pretty much eliminate glitches\nand artifacts almost entirely depending on your approach. As far as I know this\nis a new technique. And it's incredibly effective.\n\n#### Original DeOldify Model\n\n![Before Flicker](https://thumbs.gfycat.com/CoordinatedVeneratedHogget-size_restricted.gif)\n\n#### NoGAN-Based DeOldify Model\n\n![After Flicker](https://thumbs.gfycat.com/OilyBlackArctichare-size_restricted.gif)\n\nThe steps are as follows: First train the generator in a conventional way by\nitself with just the feature loss. Next, generate images from that, and train\nthe critic on distinguishing between those outputs and real images as a basic\nbinary classifier. Finally, train the generator and critic together in a GAN\nsetting (starting right at the target size of 192px in this case).  Now for\nthe weird part:  All the useful GAN training here only takes place within a very\nsmall window of time.  There's an inflection point where it appears the critic\nhas transferred everything it can that is useful to the generator. Past this\npoint, image quality oscillates between the best that you can get at the\ninflection point, or bad in a predictable way (orangish skin, overly red lips,\netc).  There appears to be no productive training after the inflection point.\nAnd this point lies within training on just 1% to 3% of the Imagenet Data!\nThat amounts to about 30-60 minutes of training at 192px.\n\nThe hard part is finding this inflection point.  So far, I've accomplished this\nby making a whole bunch of model save checkpoints (every 0.1% of data iterated\non) and then just looking for the point where images look great before they go\ntotally bonkers with orange skin (always the first thing to go). Additionally,\ngenerator rendering starts immediately getting glitchy and inconsistent at this\npoint, which is no good particularly for video. What I'd really like to figure\nout is what the tell-tale sign of the inflection point is that can be easily\nautomated as an early stopping point.  Unfortunately, nothing definitive is\njumping out at me yet.  For one, it's happening in the middle of training loss\ndecreasing- not when it flattens out, which would seem more reasonable on the surface.\n\nAnother key thing about NoGAN training is you can repeat pretraining the critic\non generated images after the initial GAN training, then repeat the GAN training\nitself in the same fashion.  This is how I was able to get extra colorful results\nwith the \"artistic\" model.  But this does come at a cost currently- the output of\nthe generator becomes increasingly inconsistent and you have to experiment with\nrender resolution (render_factor) to get the best result.  But the renders are\nstill glitch free and way more consistent than I was ever able to achieve with\nthe original DeOldify model. You can do about five of these repeat cycles, give\nor take, before you get diminishing returns, as far as I can tell.\n\nKeep in mind- I haven't been entirely rigorous in figuring out what all is going\non in NoGAN- I'll save that for a paper. That means there's a good chance I'm\nwrong about something.  But I think it's definitely worth putting out there now\nbecause I'm finding it very useful- it's solving basically much of my remaining\nproblems I had in DeOldify.\n\nThis builds upon a technique developed in collaboration with Jeremy Howard and\nSylvain Gugger for Fast.AI's Lesson 7 in version 3 of Practical Deep Learning\nfor Coders Part I. The particular lesson notebook can be found here:\n  <https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-superres-gan.ipynb>\n\n## Why Three Models?\n\nThere are now three models to choose from in DeOldify. Each of these has key\nstrengths and weaknesses, and so have different use cases.  Video is for video\nof course.  But stable and artistic are both for images, and sometimes one will\ndo images better than the other.\n\nMore details:\n\n- **Artistic** - This model achieves the highest quality results in image\ncoloration, in terms of interesting details and vibrance. The most notable\ndrawback however is that it's a bit of a pain to fiddle around with to get the\nbest results (you have to adjust the rendering resolution or render_factor to\nachieve this).  Additionally, the model does not do as well as stable in a few\nkey common scenarios- nature scenes and portraits.  The model uses a resnet34\nbackbone on a UNet with an emphasis on depth of layers on the decoder side.\nThis model was trained with 5 critic pretrain/GAN cycle repeats via NoGAN, in\naddition to the initial generator/critic pretrain/GAN NoGAN training, at 192px.\nThis adds up to a total of 32% of Imagenet data trained once (12.5 hours of\ndirect GAN training).\n\n- **Stable** - This model achieves the best results with landscapes and\nportraits.  Notably, it produces less \"zombies\"- where faces or limbs stay gray\nrather than being colored in properly.  It generally has less weird\nmiscolorations than artistic, but it's also less colorful in general.  This\nmodel uses a resnet101 backbone on a UNet with an emphasis on width of layers on\nthe decoder side.  This model was trained with 3 critic pretrain/GAN cycle\nrepeats via NoGAN, in addition to the initial generator/critic pretrain/GAN\nNoGAN training, at 192px.  This adds up to a total of 7% of Imagenet data\ntrained once (3 hours of direct GAN training).\n\n- **Video** - This model is optimized for smooth, consistent and flicker-free\nvideo.  This would definitely be the least colorful of the three models, but\nit's honestly not too far off from \"stable\". The model is the same as \"stable\"\nin terms of architecture, but differs in training.  It's trained for a mere 2.2% \nof Imagenet data once at 192px, using only the initial generator/critic \npretrain/GAN NoGAN training (1 hour of direct GAN training).\n\nBecause the training of the artistic and stable models was done before the\n\"inflection point\" of NoGAN training described in \"What is NoGAN???\" was\ndiscovered, I believe this amount of training on them can be knocked down\nconsiderably. As far as I can tell, the models were stopped at \"good points\"\nthat were well beyond where productive training was taking place.  I'll be\nlooking into this in the future.\n\nIdeally, eventually these three models will be consolidated into one that has all\nthese good desirable unified.  I think there's a path there, but it's going to\nrequire more work!  So for now, the most practical solution appears to be to\nmaintain multiple models.\n\n## The Technical Details\n\nThis is a deep learning based model.  More specifically, what I've done is\ncombined the following approaches:\n\n### [Self-Attention Generative Adversarial Network](https://arxiv.org/abs/1805.08318)\n\nExcept the generator is a **pretrained U-Net**, and I've just modified it to\nhave the spectral normalization and self-attention.  It's a pretty\nstraightforward translation.\n\n### [Two Time-Scale Update Rule](https://arxiv.org/abs/1706.08500)\n\nThis is also very straightforward \u2013 it's just one to one generator/critic\niterations and higher critic learning rate.\nThis is modified to incorporate a \"threshold\" critic loss that makes sure that\nthe critic is \"caught up\" before moving on to generator training.\nThis is particularly useful for the \"NoGAN\" method described below.\n\n### NoGAN\n\nThere's no paper here! This is a new type of GAN training that I've developed to\nsolve some key problems in the previous DeOldify model.\nThe gist is that you get the benefits of GAN training while spending minimal time\ndoing direct GAN training.\nMore details are in the [What is NoGAN?](#what-is-nogan) section (it's a doozy).\n\n### Generator Loss\n\nLoss during NoGAN learning is two parts:  One is a basic Perceptual Loss (or\nFeature Loss) based on VGG16 \u2013 this just biases the generator model to replicate\nthe input image.\nThe second is the loss score from the critic.  For the curious \u2013 Perceptual Loss\nisn't sufficient by itself to produce good results.\nIt tends to just encourage a bunch of brown/green/blue \u2013 you know, cheating to\nthe test, basically, which neural networks are really good at doing!\nKey thing to realize here is that GANs essentially are learning the loss function\nfor you \u2013 which is really one big step closer to toward the ideal that we're\nshooting for in machine learning.\nAnd of course you generally get much better results when you get the machine to\nlearn something you were previously hand coding.\nThat's certainly the case here.\n\n**Of note:**  There's no longer any \"Progressive Growing of GANs\" type training\ngoing on here.  It's just not needed in lieu of the superior results obtained\nby the \"NoGAN\" technique described above.\n\nThe beauty of this model is that it should be generally useful for all sorts of\nimage modification, and it should do it quite well.\nWhat you're seeing above are the results of the colorization model, but that's\njust one component in a pipeline that I'm developing with the exact same approach.\n\n## This Project, Going Forward\n\nSo that's the gist of this project \u2013 I'm looking to make old photos and film\nlook reeeeaaally good with GANs, and more importantly, make the project *useful*.\nIn the meantime though this is going to be my baby and I'll be actively updating\nand improving the code over the foreseeable future.\nI'll try to make this as user-friendly as possible, but I'm sure there's going\nto be hiccups along the way.\n\nOh and I swear I'll document the code properly...eventually.  Admittedly I'm\n*one of those* people who believes in \"self documenting code\" (LOL).\n\n## Getting Started Yourself\n\n### Easiest Approach\n\nThe easiest way to get started is to go straight to the Colab notebooks:\n\nImage [![Colab for images](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb)\n| Video [![Colab for video](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb)\n\nSpecial thanks to Matt Robinson and Mar\u00eda Benavente for their image Colab notebook\ncontributions, and Robert Bell for the video Colab notebook work!\n\n### Your Own Machine (not as easy)\n\n#### Hardware and Operating System Requirements\n\n- **(Training Only) BEEFY Graphics card**.  I'd really like to have more memory\n  than the 11 GB in my GeForce 1080TI (11GB).  You'll have a tough time with less.\n  The Generators and Critic are ridiculously large.  \n- **(Colorization Alone) A decent graphics card**. Approximately 4GB+ memory\n  video cards should be sufficient.\n- **Linux**.  I'm using Ubuntu 18.04, and I know 16.04 works fine too.  **Windows\n  is not supported and any issues brought up related to this will not be investigated.**\n\n#### Easy Install\n\nYou should now be able to do a simple install with Anaconda. Here are the steps:\n\nOpen the command line and navigate to the root folder you wish to install.  Then\ntype the following commands\n\n```console\ngit clone https://github.com/jantic/DeOldify.git DeOldify\ncd DeOldify\nconda env create -f environment.yml\n```\n\nThen start running with these commands:\n\n```console\nsource activate deoldify\njupyter lab\n```\n\nFrom there you can start running the notebooks in Jupyter Lab, via the url they\nprovide you in the console.\n\n> **Note:** You can also now do \"conda activate deoldify\" if you have the latest\nversion of conda and in fact that's now recommended. But a lot of people don't\nhave that yet so I'm not going to make it the default instruction here yet.\n\n**Alternative Install:** User daddyparodz has kindly created an installer script\nfor Ubuntu, and in particular Ubuntu on WSL, that may make things easier:\n  <https://github.com/daddyparodz/AutoDeOldifyLocal>\n\n#### Note on test_images Folder\n\nThe images in the `test_images` folder have been removed because they were using\nGit LFS and that costs a lot of money when GitHub actually charges for bandwidth\non a popular open source project (they had a billing bug for while that was\nrecently fixed).  The notebooks that use them (the image test ones) still point\nto images in that directory that I (Jason) have personally and I'd like to keep\nit that way because, after all, I'm by far the primary and most active developer.\nBut they won't work for you.  Still, those notebooks are a convenient template\nfor making your own tests if you're so inclined.\n\n#### Typical training\n\nThe notebook `ColorizeTrainingWandb` has been created to log and monitor results\nthrough [Weights & Biases](https://www.wandb.com/). You can find a description of\ntypical training by consulting [W&B Report](https://app.wandb.ai/borisd13/DeOldify/reports?view=borisd13%2FDeOldify).\n\n## Pretrained Weights\n\nTo start right away on your own machine with your own images or videos without\ntraining the models yourself, you'll need to download the \"Completed Generator\nWeights\" listed below and drop them in the /models/ folder.\n\nThe colorization inference notebooks should be able to guide you from here. The\nnotebooks to use are named ImageColorizerArtistic.ipynb,\nImageColorizerStable.ipynb, and VideoColorizer.ipynb.\n\n### Completed Generator Weights\n\n- [Artistic](https://data.deepai.org/deoldify/ColorizeArtistic_gen.pth)\n- [Stable](https://www.dropbox.com/s/axsd2g85uyixaho/ColorizeStable_gen.pth?dl=0)\n- [Video](https://data.deepai.org/deoldify/ColorizeVideo_gen.pth)\n\n### Completed Critic Weights\n\n- [Artistic](https://www.dropbox.com/s/xpq2ip9occuzgen/ColorizeArtistic_crit.pth?dl=0)\n- [Stable](https://www.dropbox.com/s/s53699e9n84q6sp/ColorizeStable_crit.pth?dl=0)\n- [Video](https://www.dropbox.com/s/xnq1z1oppvgpgtn/ColorizeVideo_crit.pth?dl=0)\n\n### Pretrain Only Generator Weights\n\n- [Artistic](https://www.dropbox.com/s/h782d1zar3vdblw/ColorizeArtistic_PretrainOnly_gen.pth?dl=0)\n- [Stable](https://www.dropbox.com/s/mz5n9hiq6hmwjq7/ColorizeStable_PretrainOnly_gen.pth?dl=0)\n- [Video](https://www.dropbox.com/s/ix993ci6ve7crlk/ColorizeVideo_PretrainOnly_gen.pth?dl=0)\n\n### Pretrain Only Critic Weights\n\n- [Artistic](https://www.dropbox.com/s/gr81b3pkidwlrc7/ColorizeArtistic_PretrainOnly_crit.pth?dl=0)\n- [Stable](https://www.dropbox.com/s/007qj0kkkxt5gb4/ColorizeStable_PretrainOnly_crit.pth?dl=0)\n- [Video](https://www.dropbox.com/s/wafc1uogyjuy4zq/ColorizeVideo_PretrainOnly_crit.pth?dl=0)\n\n## Want the Old DeOldify?\n\nWe suspect some of you are going to want access to the original DeOldify model\nfor various reasons.  We have that archived here:  <https://github.com/dana-kelley/DeOldify>\n\n## Want More?\n\nFollow [#DeOldify](https://twitter.com/search?q=%23Deoldify) on Twitter.\n\n## License\n\nAll code in this repository is under the MIT license as specified by the LICENSE\nfile.\n\nThe model weights listed in this readme under the \"Pretrained Weights\" section\nare trained by ourselves and are released under the MIT license.\n\n## A Statement on Open Source Support\n\nWe believe that open source has done a lot of good for the world.\u00a0 After all,\nDeOldify simply wouldn't exist without it. But we also believe that there needs\nto be boundaries on just how much is reasonable to be expected from an open\nsource project maintained by just two developers.\n\nOur stance is that we're providing the code and documentation on research that\nwe believe is beneficial to the world.\u00a0 What we have provided are novel takes\non colorization, GANs, and video that are hopefully somewhat friendly for\ndevelopers and researchers to learn from and adopt. This is the culmination of\nwell over a year of continuous work, free for you. What wasn't free was\nshouldered by us, the developers.\u00a0 We left our jobs, bought expensive GPUs, and\nhad huge electric bills as a result of dedicating ourselves to this.\n\nWhat we haven't provided here is a ready to use free \"product\" or \"app\", and we\ndon't ever intend on providing that.\u00a0 It's going to remain a Linux based project\nwithout Windows support, coded in Python, and requiring people to have some extra\ntechnical background to be comfortable using it.\u00a0 Others have stepped in with\ntheir own apps made with DeOldify, some paid and some free, which is what we want!\nWe're instead focusing on what we believe we can do best- making better\ncommercial models that people will pay for.\nDoes that mean you're not getting the very best for free?\u00a0 Of course. We simply\ndon't believe that we're obligated to provide that, nor is it feasible! We\ncompete on research and sell that.\u00a0 Not a GUI or web service that wraps said\nresearch- that part isn't something we're going to be great at anyways. We're not\nabout to shoot ourselves in the foot by giving away our actual competitive\nadvantage for free, quite frankly.\n\nWe're also not willing to go down the rabbit hole of providing endless, open\nended and personalized support on this open source project.\u00a0 Our position is\nthis:\u00a0 If you have the proper background and resources, the project provides\nmore than enough to get you started. We know this because we've seen plenty of\npeople using it and making money off of their own projects with it.\n\nThus, if you have an issue come up and it happens to be an actual bug that\nhaving it be fixed will benefit users generally, then great- that's something\nwe'll be happy to look into.\n\nIn contrast, if you're asking about something that really amounts to asking for\npersonalized and time consuming support that won't benefit anybody else, we're\nnot going to help. It's simply not in our interest to do that. We have bills to\npay, after all. And if you're asking for help on something that can already be\nderived from the documentation or code?\u00a0 That's simply annoying, and we're not\ngoing to pretend to be ok with that.\n",
        "releases": []
    }
}