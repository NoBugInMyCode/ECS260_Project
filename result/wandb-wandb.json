{
    "https://api.github.com/repos/wandb/wandb": {
        "forks": 691,
        "watchers": 9390,
        "stars": 9390,
        "languages": {
            "Python": 5153464,
            "Go": 1162252,
            "Rust": 284641,
            "C#": 64756,
            "Shell": 17947,
            "C++": 9593,
            "Jupyter Notebook": 9063,
            "Dockerfile": 3456,
            "C": 1226
        },
        "commits": [
            "2025-01-21T17:34:08Z",
            "2025-01-21T17:22:25Z",
            "2025-01-21T17:09:11Z",
            "2025-01-17T21:45:04Z",
            "2025-01-17T20:17:27Z",
            "2025-01-17T20:17:17Z",
            "2025-01-17T19:34:41Z",
            "2025-01-17T19:09:25Z",
            "2025-01-17T18:56:27Z",
            "2025-01-17T18:43:52Z",
            "2025-01-17T15:59:25Z",
            "2025-01-17T00:57:52Z",
            "2025-01-17T00:41:09Z",
            "2025-01-17T00:39:31Z",
            "2025-01-17T00:23:54Z",
            "2025-01-16T22:42:36Z",
            "2025-01-16T21:29:53Z",
            "2025-01-16T21:18:26Z",
            "2025-01-16T20:40:39Z",
            "2025-01-16T19:26:44Z",
            "2025-01-16T18:38:43Z",
            "2025-01-16T00:03:38Z",
            "2025-01-15T20:32:40Z",
            "2025-01-15T19:13:11Z",
            "2025-01-15T18:58:14Z",
            "2025-01-15T18:00:41Z",
            "2025-01-15T10:04:11Z",
            "2025-01-15T00:21:56Z",
            "2025-01-15T00:07:54Z",
            "2025-01-14T22:50:07Z"
        ],
        "creation_date": "2017-03-24T05:46:23Z",
        "contributors": 30,
        "topics": [
            "ai",
            "collaboration",
            "data-science",
            "data-versioning",
            "deep-learning",
            "experiment-track",
            "hyperparameter-optimization",
            "hyperparameter-search",
            "hyperparameter-tuning",
            "jax",
            "keras",
            "machine-learning",
            "ml-platform",
            "mlops",
            "model-versioning",
            "pytorch",
            "reinforcement-learning",
            "reproducibility",
            "tensorflow"
        ],
        "subscribers": 63,
        "readme": "<p align=\"center\">\n  <img src=\"./assets/logo-dark.svg#gh-dark-mode-only\" width=\"600\" alt=\"Weights & Biases\" />\n  <img src=\"./assets/logo-light.svg#gh-light-mode-only\" width=\"600\" alt=\"Weights & Biases\" />\n</p>\n\n<p align=\"center\">\n<a href=\"https://pypi.python.org/pypi/wandb\"><img src=\"https://img.shields.io/pypi/v/wandb\" /></a>\n<a href=\"https://anaconda.org/conda-forge/wandb\"><img src=\"https://img.shields.io/conda/vn/conda-forge/wandb\" /></a>\n<a href=\"https://pypi.python.org/pypi/wandb\"><img src=\"https://img.shields.io/pypi/pyversions/wandb\" /></a>\n<a href=\"https://circleci.com/gh/wandb/wandb\"><img src=\"https://img.shields.io/circleci/build/github/wandb/wandb/main\" /></a>\n<a href=\"https://codecov.io/gh/wandb/wandb\"><img src=\"https://img.shields.io/codecov/c/gh/wandb/wandb\" /></a>\n</p>\n<p align='center'>\n<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>\n</p>\n\nUse W&B to build better models faster. Track and visualize all the pieces of your machine learning pipeline, from datasets to production machine learning models. Get started with W&B today, [sign up for a W&B account!](https://wandb.com?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=readme)\n\n<br>\n\nBuilding an LLM app? Track, debug, evaluate, and monitor LLM apps with [Weave](https://wandb.github.io/weave?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=readme), our new suite of tools for GenAI.\n\n&nbsp;\n\n# Documentation\n\n<p align='center'>\n<a target=\"_blank\" href=\"https://docs.wandb.ai/guides/track?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=readme\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./assets/Product_Icons_dark_background/experiments-dark.svg\" width=\"12.5%\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"./assets/Product_Icons_light/experiments-light.svg\" width=\"12.5%\">\n  <img alt=\"Weights and Biases Experiments\" src=\"\">\n</picture>\n</a>\n<a target=\"_blank\" href=\"https://docs.wandb.ai/guides/reports?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=readme\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./assets/Product_Icons_dark_background/reports-dark.svg\" width=\"12.5%\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"./assets/Product_Icons_light/reports-light.svg\" width=\"12.5%\">\n  <img alt=\"Weights and Biases Reports\" src=\"\">\n</picture>\n</a>\n<a target=\"_blank\" href=\"https://docs.wandb.ai/guides/artifacts?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=readme\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./assets/Product_Icons_dark_background/artifacts-dark.svg\" width=\"12.5%\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"./assets/Product_Icons_light/artifacts-light.svg\" width=\"12.5%\">\n  <img alt=\"Weights and Biases Artifacts\" src=\"\">\n</picture>\n</a>\n<a target=\"_blank\" href=\"https://docs.wandb.ai/guides/tables?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=readme\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./assets/Product_Icons_dark_background/tables-dark.svg\" width=\"12.5%\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"./assets/Product_Icons_light/tables-light.svg\" width=\"12.5%\">\n  <img alt=\"Weights and Biases Tables\" src=\"\">\n</picture>\n</a>\n<a target=\"_blank\" href=\"https://docs.wandb.ai/guides/sweeps?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=readme\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./assets/Product_Icons_dark_background/sweeps-dark.svg\" width=\"12.5%\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"./assets/Product_Icons_light/sweeps-light.svg\" width=\"12.5%\">\n  <img alt=\"Weights and Biases Sweeps\" src=\"\">\n</picture>\n</a>\n<a target=\"_blank\" href=\"https://docs.wandb.ai/guides/model_registry?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=readme\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./assets/Product_Icons_dark_background/model-registry-dark.svg\" width=\"12.5%\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"./assets/Product_Icons_light/model-registry-light.svg\" width=\"12.5%\">\n  <img alt=\"Weights and Biases Model Management\" src=\"\">\n</picture>\n</a>\n<a target=\"_blank\" href=\"https://docs.wandb.ai/guides/artifacts/project-scoped-automations?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=readme\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./assets/Product_Icons_dark_background/automations-dark.svg\" width=\"12.5%\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"./assets/Product_Icons_light/automations-light.svg\" width=\"12.5%\">\n  <img alt=\"Weights and Biases Prompts\" src=\"\">\n</picture>\n</p>\n\nSee the [W&B Developer Guide](https://docs.wandb.ai/?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=documentation) and [API Reference Guide](https://docs.wandb.ai/ref?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=documentation) for a full technical description of the W&B platform.\n\n# Quickstart\n\nGet started with W&B in four steps:\n\n1. First, sign up for a [W&B account](https://wandb.ai/login?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=quickstart).\n\n2. Second, install\u00a0the W&B SDK with [pip](https://pip.pypa.io/en/stable/). Navigate to your terminal and type the following command:\n\n```bash\npip install wandb\n```\n\n3. Third, log into W&B:\n\n```python\nwandb.login()\n```\n\n4. Use the example code snippet below as a template to integrate W&B to your Python script:\n\n```python\nimport wandb\n\n# Start a W&B Run with wandb.init\nrun = wandb.init(project=\"my_first_project\")\n\n# Save model inputs and hyperparameters in a wandb.config object\nconfig = run.config\nconfig.learning_rate = 0.01\n\n# Model training code here ...\n\n# Log metrics over time to visualize performance with wandb.log\nfor i in range(10):\n    run.log({\"loss\": ...})\n\n# Mark the run as finished, and finish uploading all data\nrun.finish()\n```\n\nThat's it! Navigate to the W&B App to view a dashboard of your first W&B Experiment. Use the W&B App to compare multiple experiments in a unified place, dive into the results of a single run, and much more!\n\n<p align='center'>\n<img src=\"./assets/wandb_demo_experiments.gif\" width=\"100%\">\n</p>\n<p align = \"center\">\nExample W&B Dashboard that shows Runs from an Experiment.\n</p>\n\n&nbsp;\n\n# Integrations\n\nUse your favorite framework with W&B. W&B integrations make it fast and easy to set up experiment tracking and data versioning inside existing projects. For more information on how to integrate W&B with the framework of your choice, see the [Integrations chapter](https://docs.wandb.ai/guides/integrations) in the W&B Developer Guide.\n\n<!-- <p align='center'>\n<img src=\"./assets/integrations.png\" width=\"100%\" />\n</p> -->\n\n<details>\n<summary>\ud83d\udd25 PyTorch</summary>\n\nCall `.watch` and pass in your PyTorch model to automatically log gradients and store the network topology. Next, use `.log` to track other metrics. The following example demonstrates an example of how to do this:\n\n```python\nimport wandb\n\n# 1. Start a new run\nrun = wandb.init(project=\"gpt4\")\n\n# 2. Save model inputs and hyperparameters\nconfig = run.config\nconfig.dropout = 0.01\n\n# 3. Log gradients and model parameters\nrun.watch(model)\nfor batch_idx, (data, target) in enumerate(train_loader):\n    ...\n    if batch_idx % args.log_interval == 0:\n        # 4. Log metrics to visualize performance\n        run.log({\"loss\": loss})\n```\n\n- Run an example [Google Colab Notebook](http://wandb.me/pytorch-colab).\n- Read the [Developer Guide](https://docs.wandb.com/guides/integrations/pytorch?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=integrations) for technical details on how to integrate PyTorch with W&B.\n- Explore [W&B Reports](https://app.wandb.ai/wandb/getting-started/reports/Pytorch--VmlldzoyMTEwNzM?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=integrations).\n\n</details>\n<details>\n<summary>\ud83c\udf0a TensorFlow/Keras</summary>\nUse W&B Callbacks to automatically save metrics to W&B when you call `model.fit` during training.\n\nThe following code example demonstrates how your script might look like when you integrate W&B with Keras:\n\n```python\n# This script needs these libraries to be installed:\n#   tensorflow, numpy\n\nimport wandb\nfrom wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n\nimport random\nimport numpy as np\nimport tensorflow as tf\n\n\n# Start a run, tracking hyperparameters\nrun = wandb.init(\n    # set the wandb project where this run will be logged\n    project=\"my-awesome-project\",\n    # track hyperparameters and run metadata with wandb.config\n    config={\n        \"layer_1\": 512,\n        \"activation_1\": \"relu\",\n        \"dropout\": random.uniform(0.01, 0.80),\n        \"layer_2\": 10,\n        \"activation_2\": \"softmax\",\n        \"optimizer\": \"sgd\",\n        \"loss\": \"sparse_categorical_crossentropy\",\n        \"metric\": \"accuracy\",\n        \"epoch\": 8,\n        \"batch_size\": 256,\n    },\n)\n\n# [optional] use wandb.config as your config\nconfig = run.config\n\n# get the data\nmnist = tf.keras.datasets.mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\nx_train, y_train = x_train[::5], y_train[::5]\nx_test, y_test = x_test[::20], y_test[::20]\nlabels = [str(digit) for digit in range(np.max(y_train) + 1)]\n\n# build a model\nmodel = tf.keras.models.Sequential(\n    [\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\n        tf.keras.layers.Dense(config.layer_1, activation=config.activation_1),\n        tf.keras.layers.Dropout(config.dropout),\n        tf.keras.layers.Dense(config.layer_2, activation=config.activation_2),\n    ]\n)\n\n# compile the model\nmodel.compile(optimizer=config.optimizer, loss=config.loss, metrics=[config.metric])\n\n# WandbMetricsLogger will log train and validation metrics to wandb\n# WandbModelCheckpoint will upload model checkpoints to wandb\nhistory = model.fit(\n    x=x_train,\n    y=y_train,\n    epochs=config.epoch,\n    batch_size=config.batch_size,\n    validation_data=(x_test, y_test),\n    callbacks=[\n        WandbMetricsLogger(log_freq=5),\n        WandbModelCheckpoint(\"models\"),\n    ],\n)\n\n# [optional] finish the wandb run, necessary in notebooks\nrun.finish()\n```\n\nGet started integrating your Keras model with W&B today:\n\n- Run an example [Google Colab Notebook](https://wandb.me/intro-keras?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=integrations)\n- Read the [Developer Guide](https://docs.wandb.com/guides/integrations/keras?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=integrations) for technical details on how to integrate Keras with W&B.\n- Explore [W&B Reports](https://app.wandb.ai/wandb/getting-started/reports/Keras--VmlldzoyMTEwNjQ?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=integrations).\n\n</details>\n<details>\n<summary>\ud83e\udd17 Hugging Face Transformers</summary>\n\nPass `wandb` to the `report_to` argument when you run a script using a Hugging Face Trainer. W&B will automatically log losses,\nevaluation metrics, model topology, and gradients.\n\n**Note**: The environment you run your script in must have `wandb` installed.\n\nThe following example demonstrates how to integrate W&B with Hugging Face:\n\n```python\n# This script needs these libraries to be installed:\n#   numpy, transformers, datasets\n\nimport wandb\n\nimport os\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return {\"accuracy\": np.mean(predictions == labels)}\n\n\n# download prepare the data\ndataset = load_dataset(\"yelp_review_full\")\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\nsmall_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(300))\n\nsmall_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\nsmall_eval_dataset = small_train_dataset.map(tokenize_function, batched=True)\n\n# download the model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=5\n)\n\n# set the wandb project where this run will be logged\nos.environ[\"WANDB_PROJECT\"] = \"my-awesome-project\"\n\n# save your trained model checkpoint to wandb\nos.environ[\"WANDB_LOG_MODEL\"] = \"true\"\n\n# turn off watch to log faster\nos.environ[\"WANDB_WATCH\"] = \"false\"\n\n# pass \"wandb\" to the `report_to` parameter to turn on wandb logging\ntraining_args = TrainingArguments(\n    output_dir=\"models\",\n    report_to=\"wandb\",\n    logging_steps=5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    evaluation_strategy=\"steps\",\n    eval_steps=20,\n    max_steps=100,\n    save_steps=100,\n)\n\n# define the trainer and start training\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\ntrainer.train()\n\n# [optional] finish the wandb run, necessary in notebooks\nwandb.finish()\n```\n\n- Run an example [Google Colab Notebook](http://wandb.me/hf?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=integrations).\n- Read the [Developer Guide](https://docs.wandb.com/guides/integrations/huggingface?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=integrations) for technical details on how to integrate Hugging Face with W&B.\n</details>\n\n<details>\n<summary>\u26a1\ufe0f PyTorch Lightning</summary>\n\nBuild scalable, structured, high-performance PyTorch models with Lightning and log them with W&B.\n\n```python\n# This script needs these libraries to be installed:\n#   torch, torchvision, pytorch_lightning\n\nimport wandb\n\nimport os\nfrom torch import optim, nn, utils\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\n\n\nclass LitAutoEncoder(pl.LightningModule):\n    def __init__(self, lr=1e-3, inp_size=28, optimizer=\"Adam\"):\n        super().__init__()\n\n        self.encoder = nn.Sequential(\n            nn.Linear(inp_size * inp_size, 64), nn.ReLU(), nn.Linear(64, 3)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, inp_size * inp_size)\n        )\n        self.lr = lr\n\n        # save hyperparameters to self.hparamsm auto-logged by wandb\n        self.save_hyperparameters()\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = nn.functional.mse_loss(x_hat, x)\n\n        # log metrics to wandb\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\n\n\n# init the autoencoder\nautoencoder = LitAutoEncoder(lr=1e-3, inp_size=28)\n\n# setup data\nbatch_size = 32\ndataset = MNIST(os.getcwd(), download=True, transform=ToTensor())\ntrain_loader = utils.data.DataLoader(dataset, shuffle=True)\n\n# initialise the wandb logger and name your wandb project\nwandb_logger = WandbLogger(project=\"my-awesome-project\")\n\n# add your batch size to the wandb config\nwandb_logger.experiment.config[\"batch_size\"] = batch_size\n\n# pass wandb_logger to the Trainer\ntrainer = pl.Trainer(limit_train_batches=750, max_epochs=5, logger=wandb_logger)\n\n# train the model\ntrainer.fit(model=autoencoder, train_dataloaders=train_loader)\n\n# [optional] finish the wandb run, necessary in notebooks\nwandb.finish()\n```\n\n- Run an example [Google Colab Notebook](http://wandb.me/lightning?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=integrations).\n- Read the [Developer Guide](https://docs.wandb.ai/guides/integrations/lightning?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=integrations) for technical details on how to integrate PyTorch Lightning with W&B.\n</details>\n<details>\n<summary>\ud83d\udca8 XGBoost</summary>\nUse W&B Callbacks to automatically save metrics to W&B when you call `model.fit` during training.\n\nThe following code example demonstrates how your script might look like when you integrate W&B with XGBoost:\n\n```python\n# This script needs these libraries to be installed:\n#   numpy, xgboost\n\nimport wandb\nfrom wandb.xgboost import WandbCallback\n\nimport numpy as np\nimport xgboost as xgb\n\n\n# setup parameters for xgboost\nparam = {\n    \"objective\": \"multi:softmax\",\n    \"eta\": 0.1,\n    \"max_depth\": 6,\n    \"nthread\": 4,\n    \"num_class\": 6,\n}\n\n# start a new wandb run to track this script\nrun = wandb.init(\n    # set the wandb project where this run will be logged\n    project=\"my-awesome-project\",\n    # track hyperparameters and run metadata\n    config=param,\n)\n\n# download data from wandb Artifacts and prep data\nrun.use_artifact(\"wandb/intro/dermatology_data:v0\", type=\"dataset\").download(\".\")\ndata = np.loadtxt(\n    \"./dermatology.data\",\n    delimiter=\",\",\n    converters={33: lambda x: int(x == \"?\"), 34: lambda x: int(x) - 1},\n)\nsz = data.shape\n\ntrain = data[: int(sz[0] * 0.7), :]\ntest = data[int(sz[0] * 0.7) :, :]\n\ntrain_X = train[:, :33]\ntrain_Y = train[:, 34]\n\ntest_X = test[:, :33]\ntest_Y = test[:, 34]\n\nxg_train = xgb.DMatrix(train_X, label=train_Y)\nxg_test = xgb.DMatrix(test_X, label=test_Y)\nwatchlist = [(xg_train, \"train\"), (xg_test, \"test\")]\n\n# add another config to the wandb run\nnum_round = 5\nrun.config[\"num_round\"] = 5\nrun.config[\"data_shape\"] = sz\n\n# pass WandbCallback to the booster to log its configs and metrics\nbst = xgb.train(\n    param, xg_train, num_round, evals=watchlist, callbacks=[WandbCallback()]\n)\n\n# get prediction\npred = bst.predict(xg_test)\nerror_rate = np.sum(pred != test_Y) / test_Y.shape[0]\n\n# log your test metric to wandb\nrun.summary[\"Error Rate\"] = error_rate\n\n# [optional] finish the wandb run, necessary in notebooks\nrun.finish()\n```\n\n- Run an example [Google Colab Notebook](https://wandb.me/xgboost?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=integrations).\n- Read the [Developer Guide](https://docs.wandb.ai/guides/integrations/xgboost?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=integrations) for technical details on how to integrate XGBoost with W&B.\n</details>\n<details>\n<summary>\ud83e\uddee Sci-Kit Learn</summary>\nUse wandb to visualize and compare your scikit-learn models' performance:\n\n```python\n# This script needs these libraries to be installed:\n#   numpy, sklearn\n\nimport wandb\nfrom wandb.sklearn import plot_precision_recall, plot_feature_importances\nfrom wandb.sklearn import plot_class_proportions, plot_learning_curve, plot_roc\n\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n\n# load and process data\nwbcd = datasets.load_breast_cancer()\nfeature_names = wbcd.feature_names\nlabels = wbcd.target_names\n\ntest_size = 0.2\nX_train, X_test, y_train, y_test = train_test_split(\n    wbcd.data, wbcd.target, test_size=test_size\n)\n\n# train model\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\nmodel_params = model.get_params()\n\n# get predictions\ny_pred = model.predict(X_test)\ny_probas = model.predict_proba(X_test)\nimportances = model.feature_importances_\nindices = np.argsort(importances)[::-1]\n\n# start a new wandb run and add your model hyperparameters\nrun = wandb.init(project=\"my-awesome-project\", config=model_params)\n\n# Add additional configs to wandb\nrun.config.update(\n    {\n        \"test_size\": test_size,\n        \"train_len\": len(X_train),\n        \"test_len\": len(X_test),\n    }\n)\n\n# log additional visualisations to wandb\nplot_class_proportions(y_train, y_test, labels)\nplot_learning_curve(model, X_train, y_train)\nplot_roc(y_test, y_probas, labels)\nplot_precision_recall(y_test, y_probas, labels)\nplot_feature_importances(model)\n\n# [optional] finish the wandb run, necessary in notebooks\nrun.finish()\n```\n\n- Run an example [Google Colab Notebook](https://wandb.me/scikit-colab?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=integrations).\n- Read the [Developer Guide](https://docs.wandb.ai/guides/integrations/scikit?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=integrations) for technical details on how to integrate Scikit-Learn with W&B.\n</details>\n\n&nbsp;\n\n# W&B Hosting Options\n\nWeights & Biases is available in the cloud or installed on your private infrastructure. Set up a W&B Server in a production environment in one of three ways:\n\n1. [Production Cloud](https://docs.wandb.ai/guides/hosting/hosting-options/self-managed#on-prem-private-cloud?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=hosting): Set up a production deployment on a private cloud in just a few steps using terraform scripts provided by W&B.\n2. [Dedicated Cloud](https://docs.wandb.ai/guides/hosting/hosting-options/wb-managed#dedicated-cloud?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=hosting): A managed, dedicated deployment on W&B's single-tenant infrastructure in your choice of cloud region.\n3. [On-Prem/Bare Metal](https://docs.wandb.ai/guides/hosting/how-to-guides/bare-metal?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=hosting): W&B supports setting up a production server on most bare metal servers in your on-premise data centers. Quickly get started by running `wandb server` to easily start hosting W&B on your local infrastructure.\n\nSee the [Hosting documentation](https://docs.wandb.ai/guides/hosting?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=hosting) in the W&B Developer Guide for more information.\n\n<!-- &nbsp;\n\n# Tutorials\n\nExplore example Colab Notebooks at [wandb/examples GitHub repository](https://github.com/wandb/examples/tree/master/colabs). Here are some of our favorites:\n\n[INSERT] -->\n\n&nbsp;\n\n\n# Python Version Support\n\nWe are committed to supporting our minimum required Python version for *at least* six months after its official end-of-life (EOL) date, as defined by the Python Software Foundation. You can find a list of Python EOL dates [here](https://devguide.python.org/versions/).\n\nWhen we discontinue support for a Python version, we will increment the library\u2019s minor version number to reflect this change.\n\n&nbsp;\n\n# Contribution guidelines\n\nWeights & Biases \u2764\ufe0f open source, and we welcome contributions from the community! See the [Contribution guide](https://github.com/wandb/wandb/blob/main/CONTRIBUTING.md) for more information on the development workflow and the internals of the wandb library. For wandb bugs and feature requests, visit [GitHub Issues](https://github.com/wandb/wandb/issues) or contact support@wandb.com.\n\n&nbsp;\n\n# W&B Community\n\nBe a part of the growing W&B Community and interact with the W&B team in our [Discord](https://wandb.me/discord). Stay connected with the latest ML updates and tutorials with [W&B Fully Connected](https://wandb.ai/fully-connected).\n\n&nbsp;\n\n# License\n\n[MIT License](https://github.com/wandb/wandb/blob/main/LICENSE)\n",
        "releases": [
            {
                "name": "v0.19.4",
                "date": "2025-01-17T00:09:34Z"
            },
            {
                "name": "v0.19.3",
                "date": "2025-01-14T00:35:05Z"
            },
            {
                "name": "v0.19.2",
                "date": "2025-01-07T21:18:04Z"
            },
            {
                "name": "v0.19.1",
                "date": "2024-12-13T01:07:06Z"
            },
            {
                "name": "v0.19.0",
                "date": "2024-12-05T03:02:58Z"
            },
            {
                "name": "v0.18.7",
                "date": "2024-11-14T00:05:52Z"
            },
            {
                "name": "v0.18.6",
                "date": "2024-11-06T20:22:47Z"
            },
            {
                "name": "v0.18.5",
                "date": "2024-10-17T22:45:42Z"
            },
            {
                "name": "v0.18.4",
                "date": "2024-10-17T20:44:44Z"
            },
            {
                "name": "v0.18.3",
                "date": "2024-10-01T23:28:58Z"
            },
            {
                "name": "v0.18.2",
                "date": "2024-09-27T20:16:57Z"
            },
            {
                "name": "v0.18.1",
                "date": "2024-09-16T22:31:52Z"
            },
            {
                "name": "v0.18.0",
                "date": "2024-09-11T22:13:42Z"
            },
            {
                "name": "v0.17.9",
                "date": "2024-09-05T20:46:26Z"
            },
            {
                "name": "v0.17.8",
                "date": "2024-08-28T22:11:23Z"
            },
            {
                "name": "v0.17.7",
                "date": "2024-08-15T22:37:49Z"
            },
            {
                "name": "v0.17.6",
                "date": "2024-08-08T01:27:31Z"
            },
            {
                "name": "v0.17.5",
                "date": "2024-07-19T21:09:49Z"
            },
            {
                "name": "v0.17.4",
                "date": "2024-07-03T18:24:55Z"
            },
            {
                "name": "v0.17.3",
                "date": "2024-06-24T23:04:51Z"
            },
            {
                "name": "v0.17.2",
                "date": "2024-06-17T19:05:29Z"
            },
            {
                "name": "v0.17.1",
                "date": "2024-06-07T00:54:53Z"
            },
            {
                "name": "v0.17.0",
                "date": "2024-05-07T23:10:29Z"
            },
            {
                "name": "v0.16.6",
                "date": "2024-04-03T20:37:46Z"
            },
            {
                "name": "v0.16.5",
                "date": "2024-03-25T19:03:45Z"
            },
            {
                "name": "v0.16.4",
                "date": "2024-03-05T21:28:02Z"
            },
            {
                "name": "v0.16.3",
                "date": "2024-02-06T18:50:34Z"
            },
            {
                "name": "v0.16.2",
                "date": "2024-01-09T21:46:16Z"
            },
            {
                "name": "v0.16.1",
                "date": "2023-12-05T18:58:08Z"
            },
            {
                "name": "v0.16.0",
                "date": "2023-11-07T21:04:35Z"
            },
            {
                "name": "v0.15.12",
                "date": "2023-10-04T17:31:06Z"
            },
            {
                "name": "v0.15.11",
                "date": "2023-09-21T21:19:01Z"
            },
            {
                "name": "v0.15.10",
                "date": "2023-09-06T20:06:57Z"
            },
            {
                "name": "v0.15.9",
                "date": "2023-08-28T20:59:42Z"
            },
            {
                "name": "v0.15.8",
                "date": "2023-08-01T21:13:51Z"
            },
            {
                "name": "v0.15.7",
                "date": "2023-07-25T15:21:04Z"
            },
            {
                "name": "v0.15.6",
                "date": "2023-07-24T23:25:53Z"
            },
            {
                "name": "v0.15.5",
                "date": "2023-07-05T18:52:20Z"
            },
            {
                "name": "v0.15.4",
                "date": "2023-06-06T21:20:31Z"
            },
            {
                "name": "v0.15.3",
                "date": "2023-05-17T23:02:07Z"
            },
            {
                "name": "v0.15.2",
                "date": "2023-05-05T21:40:53Z"
            },
            {
                "name": "v0.15.1",
                "date": "2023-05-02T17:36:01Z"
            },
            {
                "name": "v0.15.0",
                "date": "2023-04-19T22:39:07Z"
            },
            {
                "name": "v0.14.2",
                "date": "2023-04-07T20:48:48Z"
            },
            {
                "name": "v0.14.1",
                "date": "2023-04-05T18:10:58Z"
            },
            {
                "name": "v0.14.0",
                "date": "2023-03-14T22:58:13Z"
            },
            {
                "name": "v0.13.11",
                "date": "2023-03-07T21:32:41Z"
            },
            {
                "name": "v0.13.10",
                "date": "2023-02-07T18:39:54Z"
            },
            {
                "name": "v0.13.9",
                "date": "2023-01-12T19:43:49Z"
            },
            {
                "name": "v0.13.8",
                "date": "2023-01-10T23:09:23Z"
            },
            {
                "name": "v0.13.7",
                "date": "2022-12-14T19:48:52Z"
            },
            {
                "name": "v0.13.6",
                "date": "2022-12-06T17:56:08Z"
            },
            {
                "name": "v0.13.5",
                "date": "2022-11-03T20:34:30Z"
            },
            {
                "name": "v0.13.4",
                "date": "2022-10-05T22:10:26Z"
            },
            {
                "name": "v0.13.3",
                "date": "2022-09-08T17:32:40Z"
            },
            {
                "name": "v0.13.2",
                "date": "2022-08-22T20:04:49Z"
            },
            {
                "name": "v0.13.1",
                "date": "2022-08-05T22:14:20Z"
            },
            {
                "name": "v0.13.0",
                "date": "2022-08-03T17:25:36Z"
            },
            {
                "name": "v0.13.0rc6",
                "date": "2022-08-01T23:58:22Z"
            },
            {
                "name": "v0.13.0rc5",
                "date": "2022-07-15T21:11:17Z"
            },
            {
                "name": "v0.12.21",
                "date": "2022-07-05T19:36:36Z"
            },
            {
                "name": "v0.12.20",
                "date": "2022-06-29T15:03:55Z"
            },
            {
                "name": "v0.13.0rc4",
                "date": "2022-06-24T21:53:27Z"
            },
            {
                "name": "v0.12.19",
                "date": "2022-06-22T16:17:08Z"
            },
            {
                "name": "v0.13.0rc3",
                "date": "2022-06-14T02:25:13Z"
            },
            {
                "name": "v0.12.18",
                "date": "2022-06-09T19:03:32Z"
            },
            {
                "name": "v0.13.0rc2",
                "date": "2022-05-26T21:00:10Z"
            },
            {
                "name": "v0.12.17",
                "date": "2022-05-26T08:23:24Z"
            },
            {
                "name": "v0.13.0rc1",
                "date": "2022-05-18T00:40:24Z"
            },
            {
                "name": "v0.12.16",
                "date": "2022-05-03T17:05:22Z"
            },
            {
                "name": "v0.12.15",
                "date": "2022-04-21T17:21:25Z"
            },
            {
                "name": "v0.12.14",
                "date": "2022-04-08T21:22:21Z"
            },
            {
                "name": "v0.12.13",
                "date": "2022-04-07T11:59:05Z"
            },
            {
                "name": "v0.12.12",
                "date": "2022-04-05T18:41:30Z"
            },
            {
                "name": "v0.12.11",
                "date": "2022-03-01T16:54:44Z"
            },
            {
                "name": "v0.12.10",
                "date": "2022-02-01T17:14:57Z"
            },
            {
                "name": "v0.12.9",
                "date": "2021-12-17T00:59:09Z"
            },
            {
                "name": "v0.12.8",
                "date": "2021-12-16T16:35:52Z"
            },
            {
                "name": "v0.12.7",
                "date": "2021-11-19T17:08:43Z"
            },
            {
                "name": "v0.12.6",
                "date": "2021-10-27T18:35:07Z"
            },
            {
                "name": "v0.12.5",
                "date": "2021-10-19T22:18:00Z"
            },
            {
                "name": "v0.12.4",
                "date": "2021-10-05T03:05:12Z"
            },
            {
                "name": "v0.12.3",
                "date": "2021-09-30T17:32:42Z"
            },
            {
                "name": "v0.12.2",
                "date": "2021-09-15T15:52:28Z"
            },
            {
                "name": "v0.12.1",
                "date": "2021-08-26T19:46:08Z"
            },
            {
                "name": "v0.12.0",
                "date": "2021-08-10T18:39:14Z"
            },
            {
                "name": "v0.11.2",
                "date": "2021-08-03T00:19:38Z"
            },
            {
                "name": "v0.11.1",
                "date": "2021-07-29T20:00:07Z"
            },
            {
                "name": "v0.11.0",
                "date": "2021-07-15T23:28:38Z"
            },
            {
                "name": "v0.10.33",
                "date": "2021-06-30T01:12:57Z"
            },
            {
                "name": "v0.10.32",
                "date": "2021-06-10T19:02:32Z"
            },
            {
                "name": "v0.10.31",
                "date": "2021-05-27T21:53:30Z"
            },
            {
                "name": "v0.10.30",
                "date": "2021-05-07T22:16:22Z"
            },
            {
                "name": "v0.10.29",
                "date": "2021-05-04T05:30:14Z"
            },
            {
                "name": "v0.10.28",
                "date": "2021-04-28T23:38:45Z"
            },
            {
                "name": "v0.10.27",
                "date": "2021-04-19T23:42:36Z"
            },
            {
                "name": "v0.10.26",
                "date": "2021-04-13T23:34:42Z"
            },
            {
                "name": "v0.10.25",
                "date": "2021-04-05T23:30:07Z"
            },
            {
                "name": "v0.10.24",
                "date": "2021-03-30T22:14:00Z"
            },
            {
                "name": "v0.10.23",
                "date": "2021-03-22T17:51:59Z"
            },
            {
                "name": "v0.10.22",
                "date": "2021-03-10T00:11:36Z"
            },
            {
                "name": "v0.10.21",
                "date": "2021-03-02T17:35:07Z"
            },
            {
                "name": "v0.10.20",
                "date": "2021-02-22T23:21:04Z"
            },
            {
                "name": "v0.10.19",
                "date": "2021-02-15T03:53:50Z"
            },
            {
                "name": "v0.10.18",
                "date": "2021-02-09T00:51:32Z"
            },
            {
                "name": "v0.10.17",
                "date": "2021-02-02T06:35:34Z"
            },
            {
                "name": "v0.10.16",
                "date": "2021-02-01T22:45:13Z"
            },
            {
                "name": "v0.10.15",
                "date": "2021-01-24T23:06:34Z"
            },
            {
                "name": "v0.10.14",
                "date": "2021-01-16T04:00:39Z"
            },
            {
                "name": "v0.10.13",
                "date": "2021-01-11T19:44:53Z"
            },
            {
                "name": "v0.10.12",
                "date": "2020-12-04T03:33:08Z"
            },
            {
                "name": "v0.10.11",
                "date": "2020-11-18T20:44:28Z"
            },
            {
                "name": "v0.10.10",
                "date": "2020-11-18T01:49:58Z"
            },
            {
                "name": "v0.10.9",
                "date": "2020-11-05T07:29:33Z"
            },
            {
                "name": "v0.10.8",
                "date": "2020-11-03T17:06:02Z"
            },
            {
                "name": "v0.10.7",
                "date": "2020-10-16T16:10:39Z"
            },
            {
                "name": "v0.10.6",
                "date": "2020-10-16T16:10:12Z"
            },
            {
                "name": "v0.10.5",
                "date": "2020-10-07T22:16:04Z"
            },
            {
                "name": "v0.10.4",
                "date": "2020-09-29T20:41:45Z"
            },
            {
                "name": "v0.10.3",
                "date": "2020-09-29T16:20:09Z"
            },
            {
                "name": "v0.10.2",
                "date": "2020-09-20T16:11:45Z"
            },
            {
                "name": "v0.10.1",
                "date": "2020-09-17T01:52:08Z"
            },
            {
                "name": "v0.10.0",
                "date": "2020-09-11T19:18:36Z"
            },
            {
                "name": "v0.9.7",
                "date": "2020-09-08T20:45:14Z"
            },
            {
                "name": "v0.9.6",
                "date": "2020-09-08T20:44:34Z"
            },
            {
                "name": "v0.9.5",
                "date": "2020-08-17T23:26:30Z"
            },
            {
                "name": "v0.9.4",
                "date": "2020-07-24T22:35:03Z"
            },
            {
                "name": "v0.9.3",
                "date": "2020-07-10T20:19:43Z"
            },
            {
                "name": "v0.9.2",
                "date": "2020-07-01T19:13:10Z"
            },
            {
                "name": "v0.9.1",
                "date": "2020-06-10T22:44:52Z"
            },
            {
                "name": "v0.9.0",
                "date": "2020-06-05T21:54:17Z"
            },
            {
                "name": "v0.8.36",
                "date": "2020-05-12T18:30:47Z"
            },
            {
                "name": "v0.8.35",
                "date": "2020-05-01T18:43:10Z"
            },
            {
                "name": "v0.8.34",
                "date": "2020-05-01T18:42:27Z"
            },
            {
                "name": "v0.8.33",
                "date": "2020-04-25T00:09:03Z"
            },
            {
                "name": "v0.8.32",
                "date": "2020-04-15T07:06:31Z"
            },
            {
                "name": "v0.8.31",
                "date": "2020-03-27T16:12:58Z"
            },
            {
                "name": "v0.8.30",
                "date": "2020-03-20T00:26:49Z"
            },
            {
                "name": "v0.8.29",
                "date": "2020-03-05T20:04:25Z"
            },
            {
                "name": "v0.8.28",
                "date": "2020-02-22T02:59:40Z"
            },
            {
                "name": "v0.8.27",
                "date": "2020-02-11T19:16:48Z"
            },
            {
                "name": "v0.8.26",
                "date": "2020-02-10T19:48:34Z"
            },
            {
                "name": "v0.8.25",
                "date": "2020-02-06T21:47:13Z"
            },
            {
                "name": "v0.8.24",
                "date": "2020-02-04T16:58:25Z"
            },
            {
                "name": "v0.8.23",
                "date": "2020-02-03T21:56:40Z"
            },
            {
                "name": "v0.8.22",
                "date": "2020-01-24T21:16:43Z"
            },
            {
                "name": "v0.8.21",
                "date": "2020-01-16T02:07:03Z"
            },
            {
                "name": "v0.8.20",
                "date": "2020-01-11T01:00:32Z"
            },
            {
                "name": "v0.8.19",
                "date": "2019-12-19T02:14:25Z"
            },
            {
                "name": "v0.8.18",
                "date": "2019-12-05T02:10:15Z"
            },
            {
                "name": "v0.8.17",
                "date": "2019-12-03T22:17:31Z"
            },
            {
                "name": "",
                "date": "2019-11-06T22:14:15Z"
            },
            {
                "name": "SageMaker",
                "date": "2018-11-24T16:45:17Z"
            },
            {
                "name": "",
                "date": "2018-11-11T23:16:05Z"
            },
            {
                "name": "SageMaker",
                "date": "2018-11-11T23:01:15Z"
            },
            {
                "name": "SageMaker",
                "date": "2018-11-11T22:08:16Z"
            }
        ]
    }
}