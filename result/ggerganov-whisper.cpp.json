{
    "https://api.github.com/repos/ggerganov/whisper.cpp": {
        "forks": 3819,
        "watchers": 37049,
        "stars": 37049,
        "languages": {
            "C++": 3094396,
            "C": 1894311,
            "Cuda": 685936,
            "Objective-C": 294880,
            "Metal": 263028,
            "CMake": 117577,
            "Go": 58749,
            "Java": 54649,
            "Python": 51316,
            "Ruby": 36520,
            "Shell": 29669,
            "Makefile": 8153,
            "Dockerfile": 2316,
            "Objective-C++": 2224,
            "JavaScript": 1645,
            "Batchfile": 1416,
            "Swift": 374
        },
        "commits": [
            "2025-01-21T07:39:54Z",
            "2025-01-18T10:06:06Z",
            "2025-01-14T07:53:50Z",
            "2025-01-14T07:50:06Z",
            "2025-01-14T07:31:07Z",
            "2025-01-14T07:24:03Z",
            "2025-01-13T15:45:53Z",
            "2025-01-13T11:31:41Z",
            "2025-01-10T05:39:33Z",
            "2025-01-10T01:58:08Z",
            "2025-01-10T00:13:03Z",
            "2025-01-08T20:03:28Z",
            "2025-01-08T10:54:19Z",
            "2025-01-08T08:18:13Z",
            "2025-01-08T08:17:29Z",
            "2025-01-07T17:01:58Z",
            "2025-01-07T15:11:57Z",
            "2025-01-07T11:38:05Z",
            "2025-01-07T06:37:02Z",
            "2025-01-07T06:26:07Z",
            "2025-01-06T01:33:52Z",
            "2025-01-04T20:09:59Z",
            "2025-01-04T16:10:30Z",
            "2025-01-04T08:17:31Z",
            "2025-01-05T07:50:37Z",
            "2025-01-14T07:42:16Z",
            "2025-01-13T11:11:37Z",
            "2025-01-13T06:57:33Z",
            "2025-01-13T06:55:48Z",
            "2025-01-13T06:55:21Z"
        ],
        "creation_date": "2022-09-25T18:26:37Z",
        "contributors": 30,
        "topics": [
            "inference",
            "openai",
            "speech-recognition",
            "speech-to-text",
            "transformer",
            "whisper"
        ],
        "subscribers": 318,
        "readme": "# whisper.cpp\n\n![whisper.cpp](https://user-images.githubusercontent.com/1991296/235238348-05d0f6a4-da44-4900-a1de-d0707e75b763.jpeg)\n\n[![Actions Status](https://github.com/ggerganov/whisper.cpp/workflows/CI/badge.svg)](https://github.com/ggerganov/whisper.cpp/actions)\n[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![Conan Center](https://shields.io/conan/v/whisper-cpp)](https://conan.io/center/whisper-cpp)\n[![npm](https://img.shields.io/npm/v/whisper.cpp.svg)](https://www.npmjs.com/package/whisper.cpp/)\n\nStable: [v1.7.4](https://github.com/ggerganov/whisper.cpp/releases/tag/v1.7.4) / [Roadmap | F.A.Q.](https://github.com/ggerganov/whisper.cpp/discussions/126)\n\nHigh-performance inference of [OpenAI's Whisper](https://github.com/openai/whisper) automatic speech recognition (ASR) model:\n\n- Plain C/C++ implementation without dependencies\n- Apple Silicon first-class citizen - optimized via ARM NEON, Accelerate framework, Metal and [Core ML](#core-ml-support)\n- AVX intrinsics support for x86 architectures\n- VSX intrinsics support for POWER architectures\n- Mixed F16 / F32 precision\n- [Integer quantization support](#quantization)\n- Zero memory allocations at runtime\n- [Vulkan support](#vulkan-gpu-support)\n- Support for CPU-only inference\n- [Efficient GPU support for NVIDIA](#nvidia-gpu-support)\n- [OpenVINO Support](#openvino-support)\n- [Ascend NPU Support](#ascend-npu-support)\n- [C-style API](https://github.com/ggerganov/whisper.cpp/blob/master/include/whisper.h)\n\nSupported platforms:\n\n- [x] Mac OS (Intel and Arm)\n- [x] [iOS](examples/whisper.objc)\n- [x] [Android](examples/whisper.android)\n- [x] [Java](bindings/java/README.md)\n- [x] Linux / [FreeBSD](https://github.com/ggerganov/whisper.cpp/issues/56#issuecomment-1350920264)\n- [x] [WebAssembly](examples/whisper.wasm)\n- [x] Windows ([MSVC](https://github.com/ggerganov/whisper.cpp/blob/master/.github/workflows/build.yml#L117-L144) and [MinGW](https://github.com/ggerganov/whisper.cpp/issues/168)]\n- [x] [Raspberry Pi](https://github.com/ggerganov/whisper.cpp/discussions/166)\n- [x] [Docker](https://github.com/ggerganov/whisper.cpp/pkgs/container/whisper.cpp)\n\nThe entire high-level implementation of the model is contained in [whisper.h](include/whisper.h) and [whisper.cpp](src/whisper.cpp).\nThe rest of the code is part of the [`ggml`](https://github.com/ggerganov/ggml) machine learning library.\n\nHaving such a lightweight implementation of the model allows to easily integrate it in different platforms and applications.\nAs an example, here is a video of running the model on an iPhone 13 device - fully offline, on-device: [whisper.objc](examples/whisper.objc)\n\nhttps://user-images.githubusercontent.com/1991296/197385372-962a6dea-bca1-4d50-bf96-1d8c27b98c81.mp4\n\nYou can also easily make your own offline voice assistant application: [command](examples/command)\n\nhttps://user-images.githubusercontent.com/1991296/204038393-2f846eae-c255-4099-a76d-5735c25c49da.mp4\n\nOn Apple Silicon, the inference runs fully on the GPU via Metal:\n\nhttps://github.com/ggerganov/whisper.cpp/assets/1991296/c82e8f86-60dc-49f2-b048-d2fdbd6b5225\n\n## Quick start\n\nFirst clone the repository:\n\n```bash\ngit clone https://github.com/ggerganov/whisper.cpp.git\n```\n\nNavigate into the directory:\n\n```\ncd whisper.cpp\n```\n\nThen, download one of the Whisper [models](models/README.md) converted in [`ggml` format](#ggml-format). For example:\n\n```bash\nsh ./models/download-ggml-model.sh base.en\n```\n\nNow build the [whisper-cli](examples/cli) example and transcribe an audio file like this:\n\n```bash\n# build the project\ncmake -B build\ncmake --build build --config Release\n\n# transcribe an audio file\n./build/bin/whisper-cli -f samples/jfk.wav\n```\n\n---\n\nFor a quick demo, simply run `make base.en`.\n\nThe command downloads the `base.en` model converted to custom `ggml` format and runs the inference on all `.wav` samples in the folder `samples`.\n\nFor detailed usage instructions, run: `./build/bin/whisper-cli -h`\n\nNote that the [whisper-cli](examples/cli) example currently runs only with 16-bit WAV files, so make sure to convert your input before running the tool.\nFor example, you can use `ffmpeg` like this:\n\n```bash\nffmpeg -i input.mp3 -ar 16000 -ac 1 -c:a pcm_s16le output.wav\n```\n\n## More audio samples\n\nIf you want some extra audio samples to play with, simply run:\n\n```\nmake -j samples\n```\n\nThis will download a few more audio files from Wikipedia and convert them to 16-bit WAV format via `ffmpeg`.\n\nYou can download and run the other models as follows:\n\n```\nmake -j tiny.en\nmake -j tiny\nmake -j base.en\nmake -j base\nmake -j small.en\nmake -j small\nmake -j medium.en\nmake -j medium\nmake -j large-v1\nmake -j large-v2\nmake -j large-v3\nmake -j large-v3-turbo\n```\n\n## Memory usage\n\n| Model  | Disk    | Mem     |\n| ------ | ------- | ------- |\n| tiny   | 75 MiB  | ~273 MB |\n| base   | 142 MiB | ~388 MB |\n| small  | 466 MiB | ~852 MB |\n| medium | 1.5 GiB | ~2.1 GB |\n| large  | 2.9 GiB | ~3.9 GB |\n\n## Quantization\n\n`whisper.cpp` supports integer quantization of the Whisper `ggml` models.\nQuantized models require less memory and disk space and depending on the hardware can be processed more efficiently.\n\nHere are the steps for creating and using a quantized model:\n\n```bash\n# quantize a model with Q5_0 method\ncmake -B build\ncmake --build build --config Release\n./build/bin/quantize models/ggml-base.en.bin models/ggml-base.en-q5_0.bin q5_0\n\n# run the examples as usual, specifying the quantized model file\n./build/bin/whisper-cli -m models/ggml-base.en-q5_0.bin ./samples/gb0.wav\n```\n\n## Core ML support\n\nOn Apple Silicon devices, the Encoder inference can be executed on the Apple Neural Engine (ANE) via Core ML. This can result in significant\nspeed-up - more than x3 faster compared with CPU-only execution. Here are the instructions for generating a Core ML model and using it with `whisper.cpp`:\n\n- Install Python dependencies needed for the creation of the Core ML model:\n\n  ```bash\n  pip install ane_transformers\n  pip install openai-whisper\n  pip install coremltools\n  ```\n\n  - To ensure `coremltools` operates correctly, please confirm that [Xcode](https://developer.apple.com/xcode/) is installed and execute `xcode-select --install` to install the command-line tools.\n  - Python 3.10 is recommended.\n  - MacOS Sonoma (version 14) or newer is recommended, as older versions of MacOS might experience issues with transcription hallucination.\n  - [OPTIONAL] It is recommended to utilize a Python version management system, such as [Miniconda](https://docs.conda.io/en/latest/miniconda.html) for this step:\n    - To create an environment, use: `conda create -n py310-whisper python=3.10 -y`\n    - To activate the environment, use: `conda activate py310-whisper`\n\n- Generate a Core ML model. For example, to generate a `base.en` model, use:\n\n  ```bash\n  ./models/generate-coreml-model.sh base.en\n  ```\n\n  This will generate the folder `models/ggml-base.en-encoder.mlmodelc`\n\n- Build `whisper.cpp` with Core ML support:\n\n  ```bash\n  # using CMake\n  cmake -B build -DWHISPER_COREML=1\n  cmake --build build -j --config Release\n  ```\n\n- Run the examples as usual. For example:\n\n  ```text\n  $ ./build/bin/whisper-cli -m models/ggml-base.en.bin -f samples/jfk.wav\n\n  ...\n\n  whisper_init_state: loading Core ML model from 'models/ggml-base.en-encoder.mlmodelc'\n  whisper_init_state: first run on a device may take a while ...\n  whisper_init_state: Core ML model loaded\n\n  system_info: n_threads = 4 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | COREML = 1 |\n\n  ...\n  ```\n\n  The first run on a device is slow, since the ANE service compiles the Core ML model to some device-specific format.\n  Next runs are faster.\n\nFor more information about the Core ML implementation please refer to PR [#566](https://github.com/ggerganov/whisper.cpp/pull/566).\n\n## OpenVINO support\n\nOn platforms that support [OpenVINO](https://github.com/openvinotoolkit/openvino), the Encoder inference can be executed\non OpenVINO-supported devices including x86 CPUs and Intel GPUs (integrated & discrete).\n\nThis can result in significant speedup in encoder performance. Here are the instructions for generating the OpenVINO model and using it with `whisper.cpp`:\n\n- First, setup python virtual env. and install python dependencies. Python 3.10 is recommended.\n\n  Windows:\n\n  ```powershell\n  cd models\n  python -m venv openvino_conv_env\n  openvino_conv_env\\Scripts\\activate\n  python -m pip install --upgrade pip\n  pip install -r requirements-openvino.txt\n  ```\n\n  Linux and macOS:\n\n  ```bash\n  cd models\n  python3 -m venv openvino_conv_env\n  source openvino_conv_env/bin/activate\n  python -m pip install --upgrade pip\n  pip install -r requirements-openvino.txt\n  ```\n\n- Generate an OpenVINO encoder model. For example, to generate a `base.en` model, use:\n\n  ```\n  python convert-whisper-to-openvino.py --model base.en\n  ```\n\n  This will produce ggml-base.en-encoder-openvino.xml/.bin IR model files. It's recommended to relocate these to the same folder as `ggml` models, as that\n  is the default location that the OpenVINO extension will search at runtime.\n\n- Build `whisper.cpp` with OpenVINO support:\n\n  Download OpenVINO package from [release page](https://github.com/openvinotoolkit/openvino/releases). The recommended version to use is [2023.0.0](https://github.com/openvinotoolkit/openvino/releases/tag/2023.0.0).\n\n  After downloading & extracting package onto your development system, set up required environment by sourcing setupvars script. For example:\n\n  Linux:\n\n  ```bash\n  source /path/to/l_openvino_toolkit_ubuntu22_2023.0.0.10926.b4452d56304_x86_64/setupvars.sh\n  ```\n\n  Windows (cmd):\n\n  ```powershell\n  C:\\Path\\To\\w_openvino_toolkit_windows_2023.0.0.10926.b4452d56304_x86_64\\setupvars.bat\n  ```\n\n  And then build the project using cmake:\n\n  ```bash\n  cmake -B build -DWHISPER_OPENVINO=1\n  cmake --build build -j --config Release\n  ```\n\n- Run the examples as usual. For example:\n\n  ```text\n  $ ./build/bin/whisper-cli -m models/ggml-base.en.bin -f samples/jfk.wav\n\n  ...\n\n  whisper_ctx_init_openvino_encoder: loading OpenVINO model from 'models/ggml-base.en-encoder-openvino.xml'\n  whisper_ctx_init_openvino_encoder: first run on a device may take a while ...\n  whisper_openvino_init: path_model = models/ggml-base.en-encoder-openvino.xml, device = GPU, cache_dir = models/ggml-base.en-encoder-openvino-cache\n  whisper_ctx_init_openvino_encoder: OpenVINO model loaded\n\n  system_info: n_threads = 4 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | COREML = 0 | OPENVINO = 1 |\n\n  ...\n  ```\n\n  The first time run on an OpenVINO device is slow, since the OpenVINO framework will compile the IR (Intermediate Representation) model to a device-specific 'blob'. This device-specific blob will get\n  cached for the next run.\n\nFor more information about the OpenVINO implementation please refer to PR [#1037](https://github.com/ggerganov/whisper.cpp/pull/1037).\n\n## NVIDIA GPU support\n\nWith NVIDIA cards the processing of the models is done efficiently on the GPU via cuBLAS and custom CUDA kernels.\nFirst, make sure you have installed `cuda`: https://developer.nvidia.com/cuda-downloads\n\nNow build `whisper.cpp` with CUDA support:\n\n```\ncmake -B build -DGGML_CUDA=1\ncmake --build build -j --config Release\n```\n\n## Vulkan GPU support\nCross-vendor solution which allows you to accelerate workload on your GPU.\nFirst, make sure your graphics card driver provides support for Vulkan API.\n\nNow build `whisper.cpp` with Vulkan support:\n```\ncmake -B build -DGGML_VULKAN=1\ncmake --build build -j --config Release\n```\n\n## BLAS CPU support via OpenBLAS\n\nEncoder processing can be accelerated on the CPU via OpenBLAS.\nFirst, make sure you have installed `openblas`: https://www.openblas.net/\n\nNow build `whisper.cpp` with OpenBLAS support:\n\n```\ncmake -B build -DGGML_BLAS=1\ncmake --build build -j --config Release\n```\n\n## Ascend NPU support\n\nAscend NPU provides inference acceleration via [`CANN`](https://www.hiascend.com/en/software/cann) and AI cores.\n\nFirst, check if your Ascend NPU device is supported:\n\n**Verified devices**\n| Ascend NPU                    | Status  |\n|:-----------------------------:|:-------:|\n| Atlas 300T A2                 | Support |\n\nThen, make sure you have installed [`CANN toolkit`](https://www.hiascend.com/en/software/cann/community) . The lasted version of CANN is recommanded.\n\nNow build `whisper.cpp` with CANN support:\n\n```\ncmake -B build -DGGML_CANN=1\ncmake --build build -j --config Release\n```\n\nRun the inference examples as usual, for example:\n\n```\n./build/bin/whisper-cli -f samples/jfk.wav -m models/ggml-base.en.bin -t 8\n```\n\n*Notes:*\n\n- If you have trouble with Ascend NPU device, please create a issue with **[CANN]** prefix/tag.\n- If you run successfully with your Ascend NPU device, please help update the table `Verified devices`.\n\n## Docker\n\n### Prerequisites\n\n- Docker must be installed and running on your system.\n- Create a folder to store big models & intermediate files (ex. /whisper/models)\n\n### Images\n\nWe have two Docker images available for this project:\n\n1. `ghcr.io/ggerganov/whisper.cpp:main`: This image includes the main executable file as well as `curl` and `ffmpeg`. (platforms: `linux/amd64`, `linux/arm64`)\n2. `ghcr.io/ggerganov/whisper.cpp:main-cuda`: Same as `main` but compiled with CUDA support. (platforms: `linux/amd64`)\n\n### Usage\n\n```shell\n# download model and persist it in a local folder\ndocker run -it --rm \\\n  -v path/to/models:/models \\\n  whisper.cpp:main \"./models/download-ggml-model.sh base /models\"\n# transcribe an audio file\ndocker run -it --rm \\\n  -v path/to/models:/models \\\n  -v path/to/audios:/audios \\\n  whisper.cpp:main \"./main -m /models/ggml-base.bin -f /audios/jfk.wav\"\n# transcribe an audio file in samples folder\ndocker run -it --rm \\\n  -v path/to/models:/models \\\n  whisper.cpp:main \"./main -m /models/ggml-base.bin -f ./samples/jfk.wav\"\n```\n\n## Installing with Conan\n\nYou can install pre-built binaries for whisper.cpp or build it from source using [Conan](https://conan.io/). Use the following command:\n\n```\nconan install --requires=\"whisper-cpp/[*]\" --build=missing\n```\n\nFor detailed instructions on how to use Conan, please refer to the [Conan documentation](https://docs.conan.io/2/).\n\n## Limitations\n\n- Inference only\n\n## Real-time audio input example\n\nThis is a naive example of performing real-time inference on audio from your microphone.\nThe [stream](examples/stream) tool samples the audio every half a second and runs the transcription continuously.\nMore info is available in [issue #10](https://github.com/ggerganov/whisper.cpp/issues/10).\n\n```bash\ncmake -B build -DWHISPER_SDL2=ON\ncmake --build build --config Release\n./build/bin/whisper-stream -m ./models/ggml-base.en.bin -t 8 --step 500 --length 5000\n```\n\nhttps://user-images.githubusercontent.com/1991296/194935793-76afede7-cfa8-48d8-a80f-28ba83be7d09.mp4\n\n## Confidence color-coding\n\nAdding the `--print-colors` argument will print the transcribed text using an experimental color coding strategy\nto highlight words with high or low confidence:\n\n```bash\n./build/bin/whisper-cli -m models/ggml-base.en.bin -f samples/gb0.wav --print-colors\n```\n\n<img width=\"965\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1991296/197356445-311c8643-9397-4e5e-b46e-0b4b4daa2530.png\">\n\n## Controlling the length of the generated text segments (experimental)\n\nFor example, to limit the line length to a maximum of 16 characters, simply add `-ml 16`:\n\n```text\n$ ./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 16\n\nwhisper_model_load: loading model from './models/ggml-base.en.bin'\n...\nsystem_info: n_threads = 4 / 10 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 |\n\nmain: processing './samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n[00:00:00.000 --> 00:00:00.850]   And so my\n[00:00:00.850 --> 00:00:01.590]   fellow\n[00:00:01.590 --> 00:00:04.140]   Americans, ask\n[00:00:04.140 --> 00:00:05.660]   not what your\n[00:00:05.660 --> 00:00:06.840]   country can do\n[00:00:06.840 --> 00:00:08.430]   for you, ask\n[00:00:08.430 --> 00:00:09.440]   what you can do\n[00:00:09.440 --> 00:00:10.020]   for your\n[00:00:10.020 --> 00:00:11.000]   country.\n```\n\n## Word-level timestamp (experimental)\n\nThe `--max-len` argument can be used to obtain word-level timestamps. Simply use `-ml 1`:\n\n```text\n$ ./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 1\n\nwhisper_model_load: loading model from './models/ggml-base.en.bin'\n...\nsystem_info: n_threads = 4 / 10 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 |\n\nmain: processing './samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n[00:00:00.000 --> 00:00:00.320]\n[00:00:00.320 --> 00:00:00.370]   And\n[00:00:00.370 --> 00:00:00.690]   so\n[00:00:00.690 --> 00:00:00.850]   my\n[00:00:00.850 --> 00:00:01.590]   fellow\n[00:00:01.590 --> 00:00:02.850]   Americans\n[00:00:02.850 --> 00:00:03.300]  ,\n[00:00:03.300 --> 00:00:04.140]   ask\n[00:00:04.140 --> 00:00:04.990]   not\n[00:00:04.990 --> 00:00:05.410]   what\n[00:00:05.410 --> 00:00:05.660]   your\n[00:00:05.660 --> 00:00:06.260]   country\n[00:00:06.260 --> 00:00:06.600]   can\n[00:00:06.600 --> 00:00:06.840]   do\n[00:00:06.840 --> 00:00:07.010]   for\n[00:00:07.010 --> 00:00:08.170]   you\n[00:00:08.170 --> 00:00:08.190]  ,\n[00:00:08.190 --> 00:00:08.430]   ask\n[00:00:08.430 --> 00:00:08.910]   what\n[00:00:08.910 --> 00:00:09.040]   you\n[00:00:09.040 --> 00:00:09.320]   can\n[00:00:09.320 --> 00:00:09.440]   do\n[00:00:09.440 --> 00:00:09.760]   for\n[00:00:09.760 --> 00:00:10.020]   your\n[00:00:10.020 --> 00:00:10.510]   country\n[00:00:10.510 --> 00:00:11.000]  .\n```\n\n## Speaker segmentation via tinydiarize (experimental)\n\nMore information about this approach is available here: https://github.com/ggerganov/whisper.cpp/pull/1058\n\nSample usage:\n\n```py\n# download a tinydiarize compatible model\n./models/download-ggml-model.sh small.en-tdrz\n\n# run as usual, adding the \"-tdrz\" command-line argument\n./build/bin/whisper-cli -f ./samples/a13.wav -m ./models/ggml-small.en-tdrz.bin -tdrz\n...\nmain: processing './samples/a13.wav' (480000 samples, 30.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, tdrz = 1, timestamps = 1 ...\n...\n[00:00:00.000 --> 00:00:03.800]   Okay Houston, we've had a problem here. [SPEAKER_TURN]\n[00:00:03.800 --> 00:00:06.200]   This is Houston. Say again please. [SPEAKER_TURN]\n[00:00:06.200 --> 00:00:08.260]   Uh Houston we've had a problem.\n[00:00:08.260 --> 00:00:11.320]   We've had a main beam up on a volt. [SPEAKER_TURN]\n[00:00:11.320 --> 00:00:13.820]   Roger main beam interval. [SPEAKER_TURN]\n[00:00:13.820 --> 00:00:15.100]   Uh uh [SPEAKER_TURN]\n[00:00:15.100 --> 00:00:18.020]   So okay stand, by thirteen we're looking at it. [SPEAKER_TURN]\n[00:00:18.020 --> 00:00:25.740]   Okay uh right now uh Houston the uh voltage is uh is looking good um.\n[00:00:27.620 --> 00:00:29.940]   And we had a a pretty large bank or so.\n```\n\n## Karaoke-style movie generation (experimental)\n\nThe [whisper-cli](examples/cli) example provides support for output of karaoke-style movies, where the\ncurrently pronounced word is highlighted. Use the `-wts` argument and run the generated bash script.\nThis requires to have `ffmpeg` installed.\n\nHere are a few _\"typical\"_ examples:\n\n```bash\n./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -owts\nsource ./samples/jfk.wav.wts\nffplay ./samples/jfk.wav.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/199337465-dbee4b5e-9aeb-48a3-b1c6-323ac4db5b2c.mp4\n\n---\n\n```bash\n./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/mm0.wav -owts\nsource ./samples/mm0.wav.wts\nffplay ./samples/mm0.wav.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/199337504-cc8fd233-0cb7-4920-95f9-4227de3570aa.mp4\n\n---\n\n```bash\n./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/gb0.wav -owts\nsource ./samples/gb0.wav.wts\nffplay ./samples/gb0.wav.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/199337538-b7b0c7a3-2753-4a88-a0cd-f28a317987ba.mp4\n\n---\n\n## Video comparison of different models\n\nUse the [scripts/bench-wts.sh](https://github.com/ggerganov/whisper.cpp/blob/master/scripts/bench-wts.sh) script to generate a video in the following format:\n\n```bash\n./scripts/bench-wts.sh samples/jfk.wav\nffplay ./samples/jfk.wav.all.mp4\n```\n\nhttps://user-images.githubusercontent.com/1991296/223206245-2d36d903-cf8e-4f09-8c3b-eb9f9c39d6fc.mp4\n\n---\n\n## Benchmarks\n\nIn order to have an objective comparison of the performance of the inference across different system configurations,\nuse the [whisper-bench](examples/bench) tool. The tool simply runs the Encoder part of the model and prints how much time it\ntook to execute it. The results are summarized in the following Github issue:\n\n[Benchmark results](https://github.com/ggerganov/whisper.cpp/issues/89)\n\nAdditionally a script to run whisper.cpp with different models and audio files is provided [bench.py](scripts/bench.py).\n\nYou can run it with the following command, by default it will run against any standard model in the models folder.\n\n```bash\npython3 scripts/bench.py -f samples/jfk.wav -t 2,4,8 -p 1,2\n```\n\nIt is written in python with the intention of being easy to modify and extend for your benchmarking use case.\n\nIt outputs a csv file with the results of the benchmarking.\n\n## `ggml` format\n\nThe original models are converted to a custom binary format. This allows to pack everything needed into a single file:\n\n- model parameters\n- mel filters\n- vocabulary\n- weights\n\nYou can download the converted models using the [models/download-ggml-model.sh](models/download-ggml-model.sh) script\nor manually from here:\n\n- https://huggingface.co/ggerganov/whisper.cpp\n- https://ggml.ggerganov.com\n\nFor more details, see the conversion script [models/convert-pt-to-ggml.py](models/convert-pt-to-ggml.py) or [models/README.md](models/README.md).\n\n## [Bindings](https://github.com/ggerganov/whisper.cpp/discussions/categories/bindings)\n\n- [x] Rust: [tazz4843/whisper-rs](https://github.com/tazz4843/whisper-rs) | [#310](https://github.com/ggerganov/whisper.cpp/discussions/310)\n- [x] JavaScript: [bindings/javascript](bindings/javascript) | [#309](https://github.com/ggerganov/whisper.cpp/discussions/309)\n  - React Native (iOS / Android): [whisper.rn](https://github.com/mybigday/whisper.rn)\n- [x] Go: [bindings/go](bindings/go) | [#312](https://github.com/ggerganov/whisper.cpp/discussions/312)\n- [x] Java:\n  - [GiviMAD/whisper-jni](https://github.com/GiviMAD/whisper-jni)\n- [x] Ruby: [bindings/ruby](bindings/ruby) | [#507](https://github.com/ggerganov/whisper.cpp/discussions/507)\n- [x] Objective-C / Swift: [ggerganov/whisper.spm](https://github.com/ggerganov/whisper.spm) | [#313](https://github.com/ggerganov/whisper.cpp/discussions/313)\n  - [exPHAT/SwiftWhisper](https://github.com/exPHAT/SwiftWhisper)\n- [x] .NET: | [#422](https://github.com/ggerganov/whisper.cpp/discussions/422)\n  - [sandrohanea/whisper.net](https://github.com/sandrohanea/whisper.net)\n  - [NickDarvey/whisper](https://github.com/NickDarvey/whisper)\n- [x] Python: | [#9](https://github.com/ggerganov/whisper.cpp/issues/9)\n  - [stlukey/whispercpp.py](https://github.com/stlukey/whispercpp.py) (Cython)\n  - [AIWintermuteAI/whispercpp](https://github.com/AIWintermuteAI/whispercpp) (Updated fork of aarnphm/whispercpp)\n  - [aarnphm/whispercpp](https://github.com/aarnphm/whispercpp) (Pybind11)\n  - [abdeladim-s/pywhispercpp](https://github.com/abdeladim-s/pywhispercpp) (Pybind11)\n- [x] R: [bnosac/audio.whisper](https://github.com/bnosac/audio.whisper)\n- [x] Unity: [macoron/whisper.unity](https://github.com/Macoron/whisper.unity)\n\n## Examples\n\nThere are various examples of using the library for different projects in the [examples](examples) folder.\nSome of the examples are even ported to run in the browser using WebAssembly. Check them out!\n\n| Example                                             | Web                                   | Description                                                                                                                     |\n| --------------------------------------------------- | ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------- |\n| [whisper-cli](examples/cli)                         | [whisper.wasm](examples/whisper.wasm) | Tool for translating and transcribing audio using Whisper                                                                       |\n| [whisper-bench](examples/bench)                     | [bench.wasm](examples/bench.wasm)     | Benchmark the performance of Whisper on your machine                                                                            |\n| [whisper-stream](examples/stream)                   | [stream.wasm](examples/stream.wasm)   | Real-time transcription of raw microphone capture                                                                               |\n| [whisper-command](examples/command)                 | [command.wasm](examples/command.wasm) | Basic voice assistant example for receiving voice commands from the mic                                                         |\n| [whisper-server](examples/server)                   |                                       | HTTP transcription server with OAI-like API                                                                                     |\n| [whisper-talk-llama](examples/talk-llama)           |                                       | Talk with a LLaMA bot                                                                                                           |\n| [whisper.objc](examples/whisper.objc)               |                                       | iOS mobile application using whisper.cpp                                                                                        |\n| [whisper.swiftui](examples/whisper.swiftui)         |                                       | SwiftUI iOS / macOS application using whisper.cpp                                                                               |\n| [whisper.android](examples/whisper.android)         |                                       | Android mobile application using whisper.cpp                                                                                    |\n| [whisper.nvim](examples/whisper.nvim)               |                                       | Speech-to-text plugin for Neovim                                                                                                |\n| [generate-karaoke.sh](examples/generate-karaoke.sh) |                                       | Helper script to easily [generate a karaoke video](https://youtu.be/uj7hVta4blM) of raw audio capture                           |\n| [livestream.sh](examples/livestream.sh)             |                                       | [Livestream audio transcription](https://github.com/ggerganov/whisper.cpp/issues/185)                                           |\n| [yt-wsp.sh](examples/yt-wsp.sh)                     |                                       | Download + transcribe and/or translate any VOD [(original)](https://gist.github.com/DaniruKun/96f763ec1a037cc92fe1a059b643b818) |\n| [wchess](examples/wchess)                           | [wchess.wasm](examples/wchess)        | Voice-controlled chess                                                                                                          |\n\n## [Discussions](https://github.com/ggerganov/whisper.cpp/discussions)\n\nIf you have any kind of feedback about this project feel free to use the Discussions section and open a new topic.\nYou can use the [Show and tell](https://github.com/ggerganov/whisper.cpp/discussions/categories/show-and-tell) category\nto share your own projects that use `whisper.cpp`. If you have a question, make sure to check the\n[Frequently asked questions (#126)](https://github.com/ggerganov/whisper.cpp/discussions/126) discussion.\n",
        "releases": [
            {
                "name": "v1.7.4",
                "date": "2025-01-06T13:16:31Z"
            },
            {
                "name": "v1.7.3",
                "date": "2024-12-18T16:15:19Z"
            },
            {
                "name": "v1.7.3-pre",
                "date": "2024-12-09T09:34:50Z"
            },
            {
                "name": "v1.7.2",
                "date": "2024-11-19T16:55:56Z"
            },
            {
                "name": "v1.7.2-pre",
                "date": "2024-11-15T14:05:17Z"
            },
            {
                "name": "v1.7.1",
                "date": "2024-10-07T10:09:10Z"
            },
            {
                "name": "v1.7.0",
                "date": "2024-10-05T14:15:25Z"
            },
            {
                "name": "v1.6.2",
                "date": "2024-05-27T07:36:55Z"
            },
            {
                "name": "v1.6.1",
                "date": "2024-05-21T15:46:11Z"
            },
            {
                "name": "v1.6.0",
                "date": "2024-05-15T07:13:56Z"
            },
            {
                "name": "v1.5.5",
                "date": "2024-04-16T11:14:06Z"
            },
            {
                "name": "v1.5.4",
                "date": "2024-01-05T15:20:09Z"
            },
            {
                "name": "v1.5.3",
                "date": "2024-01-03T17:39:29Z"
            },
            {
                "name": "v1.5.2",
                "date": "2023-12-14T16:06:12Z"
            },
            {
                "name": "v1.5.1",
                "date": "2023-11-24T10:45:18Z"
            },
            {
                "name": "v1.5.0",
                "date": "2023-11-15T21:06:23Z"
            },
            {
                "name": "v1.4.3",
                "date": "2023-11-07T14:29:38Z"
            },
            {
                "name": "v1.4.0",
                "date": "2023-04-30T16:56:01Z"
            },
            {
                "name": "v1.3.0",
                "date": "2023-04-15T14:41:02Z"
            },
            {
                "name": "v1.2.1",
                "date": "2023-02-28T20:30:31Z"
            },
            {
                "name": "v1.2.0",
                "date": "2023-02-04T08:55:40Z"
            },
            {
                "name": "v1.1.1",
                "date": "2023-01-23T18:41:47Z"
            },
            {
                "name": "v1.1.0",
                "date": "2023-01-15T12:00:56Z"
            },
            {
                "name": "v1.0.4",
                "date": "2022-12-17T18:34:58Z"
            }
        ]
    }
}