{
    "https://api.github.com/repos/alexjc/neural-enhance": {
        "forks": 1386,
        "watchers": 11882,
        "stars": 11882,
        "languages": {
            "Python": 32003,
            "Shell": 1604
        },
        "commits": [
            "2016-11-30T15:15:55Z",
            "2016-11-30T14:28:18Z",
            "2016-11-19T19:27:56Z",
            "2016-11-19T18:42:47Z",
            "2016-11-19T11:29:08Z",
            "2016-11-18T18:24:17Z",
            "2016-11-16T15:52:17Z",
            "2016-11-12T14:47:00Z",
            "2016-11-12T14:43:23Z",
            "2016-11-12T14:41:30Z",
            "2016-11-11T14:01:33Z",
            "2016-11-11T09:29:06Z",
            "2016-11-11T09:06:59Z",
            "2016-11-11T09:00:23Z",
            "2016-11-11T00:33:06Z",
            "2016-11-10T23:33:54Z",
            "2016-11-10T16:29:18Z",
            "2016-11-10T16:20:20Z",
            "2016-11-10T16:06:52Z",
            "2016-11-10T15:55:56Z",
            "2016-11-10T15:45:30Z",
            "2016-11-10T12:16:40Z",
            "2016-11-09T16:04:35Z",
            "2016-11-08T16:41:28Z",
            "2016-11-08T16:15:54Z",
            "2016-11-08T16:15:41Z",
            "2016-11-04T13:29:15Z",
            "2016-11-04T13:28:37Z",
            "2016-11-03T23:34:52Z",
            "2016-11-03T23:32:08Z"
        ],
        "creation_date": "2016-10-01T13:01:50Z",
        "contributors": 8,
        "topics": [],
        "subscribers": 398,
        "readme": "Neural Enhance\n==============\n\n.. image:: docs/OldStation_example.gif\n\n**Example #1** \u2014 Old Station: `view comparison <http://enhance.nucl.ai/w/0f5177f4-9ce6-11e6-992c-c86000be451f/view>`_ in 24-bit HD, `original photo <https://flic.kr/p/oYhbBv>`_ CC-BY-SA @siv-athens.\n\n----\n\n`As seen on TV! <https://www.youtube.com/watch?v=LhF_56SxrGk>`_ What if you could increase the resolution of your photos using technology from CSI laboratories? Thanks to deep learning and ``#NeuralEnhance``, it's now possible to train a neural network to zoom in to your images at 2x or even 4x.  You'll get even better results by increasing the number of neurons or training with a dataset similar to your low resolution image.\n\nThe catch? The neural network is hallucinating details based on its training from example images. It's not reconstructing your photo exactly as it would have been if it was HD. That's only possible in Hollywood \u2014 but using deep learning as \"Creative AI\" works and it is just as cool!  Here's how you can get started...\n\n1. `Examples & Usage <#1-examples--usage>`_\n2. `Installation <#2-installation--setup>`_\n3. `Background & Research <#3-background--research>`_\n4. `Troubleshooting <#4-troubleshooting-problems>`_\n5. `Frequent Questions <#5-frequent-questions>`_\n\n|Python Version| |License Type| |Project Stars|\n\n.. image:: docs/EnhanceCSI_example.png\n    :target: http://enhance.nucl.ai/w/8581db92-9d61-11e6-990b-c86000be451f/view\n\n1. Examples & Usage\n===================\n\nThe main script is called ``enhance.py``, which you can run with Python 3.4+ once it's `setup <#2-installation--setup>`_ as below.  The ``--device`` argument that lets you specify which GPU or CPU to use. For the samples above, here are the performance results:\n\n* **GPU Rendering HQ** \u2014 Assuming you have CUDA setup and enough on-board RAM to fit the image and neural network, generating 1080p output should complete in 5 seconds, or 2s per image if multiple at the same time.\n* **CPU Rendering HQ** \u2014 This will take roughly 20 to 60 seconds for 1080p output, however on most machines you can run 4-8 processes simultaneously given enough system RAM. Runtime depends on the neural network size.\n\nThe default is to use ``--device=cpu``, if you have NVIDIA card setup with CUDA already try ``--device=gpu0``. On the CPU, you can also set environment variable to ``OMP_NUM_THREADS=4``, which is most useful when running the script multiple times in parallel.\n\n1.a) Enhancing Images\n---------------------\n\nA list of example command lines you can use with the pre-trained models provided in the GitHub releases:\n\n.. code:: bash\n\n    # Run the super-resolution script to repair JPEG artefacts, zoom factor 1:1.\n    python3 enhance.py --type=photo --model=repair --zoom=1 broken.jpg\n\n    # Process multiple good quality images with a single run, zoom factor 2:1.\n    python3 enhance.py --type=photo --zoom=2 file1.jpg file2.jpg\n\n    # Display output images that were given `_ne?x.png` suffix.\n    open *_ne?x.png\n\nHere's a list of currently supported models, image types, and zoom levels in one table.\n\n==================  =====================  ====================  =====================  ====================\n     FEATURES        ``--model=default``    ``--model=repair``    ``--model=denoise``    ``--model=deblur``\n==================  =====================  ====================  =====================  ====================\n ``--type=photo``            2x                     1x                     \u2026                      \u2026         \n==================  =====================  ====================  =====================  ====================\n\n\n1.b) Training Super-Resolution\n------------------------------\n\nPre-trained models are provided in the GitHub releases.  Training your own is a delicate process that may require you to pick parameters based on your image dataset.\n\n.. code:: bash\n\n    # Remove the model file as don't want to reload the data to fine-tune it.\n    rm -f ne?x*.pkl.bz2\n\n    # Pre-train the model using perceptual loss from paper [1] below.\n    python3.4 enhance.py --train \"data/*.jpg\" --model custom --scales=2 --epochs=50 \\\n        --perceptual-layer=conv2_2 --smoothness-weight=1e7 --adversary-weight=0.0 \\\n        --generator-blocks=4 --generator-filters=64\n    \n    # Train the model using an adversarial setup based on [4] below.\n    python3.4 enhance.py --train \"data/*.jpg\" --model custom --scales=2 --epochs=250 \\\n             --perceptual-layer=conv5_2 --smoothness-weight=2e4 --adversary-weight=1e3 \\\n             --generator-start=5 --discriminator-start=0 --adversarial-start=5 \\\n             --discriminator-size=64\n\n    # The newly trained model is output into this file...\n    ls ne?x-custom-*.pkl.bz2\n\n\n.. image:: docs/BankLobby_example.gif\n\n**Example #2** \u2014 Bank Lobby: `view comparison <http://enhance.nucl.ai/w/38d10880-9ce6-11e6-becb-c86000be451f/view>`_ in 24-bit HD, `original photo <https://flic.kr/p/6a8cwm>`_ CC-BY-SA @benarent.\n\n2. Installation & Setup\n=======================\n\n2.a) Using Docker Image [recommended]\n-------------------------------------\n\nThe easiest way to get up-and-running is to `install Docker <https://www.docker.com/>`_. Then, you should be able to download and run the pre-built image using the ``docker`` command line tool.  Find out more about the ``alexjc/neural-enhance`` image on its `Docker Hub <https://hub.docker.com/r/alexjc/neural-enhance/>`_ page.\n\nHere's the simplest way you can call the script using ``docker``, assuming you're familiar with using ``-v`` argument to mount folders you can use this directly to specify files to enhance:\n\n.. code:: bash\n\n    # Download the Docker image and show the help text to make sure it works.\n    docker run --rm -v `pwd`:/ne/input -it alexjc/neural-enhance --help\n\n**Single Image** \u2014 In practice, we suggest you setup an alias called ``enhance`` to automatically expose the folder containing your specified image, so the script can read it and store results where you can access them.  This is how you can do it in your terminal console on OSX or Linux:\n\n.. code:: bash\n\n    # Setup the alias. Put this in your .bashrc or .zshrc file so it's available at startup.\n    alias enhance='function ne() { docker run --rm -v \"$(pwd)/`dirname ${@:$#}`\":/ne/input -it alexjc/neural-enhance ${@:1:$#-1} \"input/`basename ${@:$#}`\"; }; ne'\n\n    # Now run any of the examples above using this alias, without the `.py` extension.\n    enhance --zoom=1 --model=repair images/broken.jpg\n\n**Multiple Images** \u2014 To enhance multiple images in a row (faster) from a folder or wildcard specification, make sure to quote the argument to the alias command:\n\n.. code:: bash\n    \n    # Process multiple images, make sure to quote the argument!\n    enhance --zoom=2 \"images/*.jpg\"\n\nIf you want to run on your NVIDIA GPU, you can instead change the alias to use the image ``alexjc/neural-enhance:gpu`` which comes with CUDA and CUDNN pre-installed.  Then run it within `nvidia-docker <https://github.com/NVIDIA/nvidia-docker>`_ and it should use your physical hardware!\n\n\n2.b) Manual Installation [developers]\n-------------------------------------\n\nThis project requires Python 3.4+ and you'll also need ``numpy`` and ``scipy`` (numerical computing libraries) as well as ``python3-dev`` installed system-wide.  If you want more detailed instructions, follow these:\n\n1. `Linux Installation of Lasagne <https://github.com/Lasagne/Lasagne/wiki/From-Zero-to-Lasagne-on-Ubuntu-14.04>`_ **(intermediate)**\n2. `Mac OSX Installation of Lasagne <http://deeplearning.net/software/theano/install.html#mac-os>`_ **(advanced)**\n3. `Windows Installation of Lasagne <https://github.com/Lasagne/Lasagne/wiki/From-Zero-to-Lasagne-on-Windows-7-%2864-bit%29>`_ **(expert)**\n\nAfterward fetching the repository, you can run the following commands from your terminal to setup a local environment:\n\n.. code:: bash\n\n    # Create a local environment for Python 3.x to install dependencies here.\n    python3 -m venv pyvenv --system-site-packages\n\n    # If you're using bash, make this the active version of Python.\n    source pyvenv/bin/activate\n\n    # Setup the required dependencies simply using the PIP module.\n    python3 -m pip install --ignore-installed -r requirements.txt\n\nAfter this, you should have ``pillow``, ``theano`` and ``lasagne`` installed in your virtual environment.  You'll also need to download this `pre-trained neural network <https://github.com/alexjc/neural-doodle/releases/download/v0.0/vgg19_conv.pkl.bz2>`_ (VGG19, 80Mb) and put it in the same folder as the script to run. To de-install everything, you can just delete the ``#/pyvenv/`` folder.\n\n.. image:: docs/Faces_example.png\n\n**Example #3** \u2014 Specialized super-resolution for faces, trained on HD examples of celebrity faces only.  The quality is significantly higher when narrowing the domain from \"photos\" in general.\n\n3. Background & Research\n========================\n\nThis code uses a combination of techniques from the following papers, as well as some minor improvements yet to be documented (watch this repository for updates):\n\n1. `Perceptual Losses for Real-Time Style Transfer and Super-Resolution <http://arxiv.org/abs/1603.08155>`_\n2. `Real-Time Super-Resolution Using Efficient Sub-Pixel Convolution <https://arxiv.org/abs/1609.05158>`_\n3. `Deeply-Recursive Convolutional Network for Image Super-Resolution <https://arxiv.org/abs/1511.04491>`_\n4. `Photo-Realistic Super-Resolution Using a Generative Adversarial Network <https://arxiv.org/abs/1609.04802>`_\n\nSpecial thanks for their help and support in various ways:\n\n* Eder Santana \u2014 Discussions, encouragement, and his ideas on `sub-pixel deconvolution <https://github.com/Tetrachrome/subpixel>`_.\n* Andrew Brock \u2014 This sub-pixel layer code is based on `his project repository <https://github.com/ajbrock/Neural-Photo-Editor>`_ using Lasagne.\n* Casper Kaae S\u00f8nderby \u2014 For suggesting a more stable alternative to sigmoid + log as GAN loss functions.\n\n\n4. Troubleshooting Problems\n===========================\n\nCan't install or Unable to find pgen, not compiling formal grammar.\n-------------------------------------------------------------------\n\nThere's a Python extension compiler called Cython, and it's missing or improperly installed. Try getting it directly from the system package manager rather than PIP.\n\n**FIX:** ``sudo apt-get install cython3``\n\n\nNotImplementedError: AbstractConv2d theano optimization failed.\n---------------------------------------------------------------\n\nThis happens when you're running without a GPU, and the CPU libraries were not found (e.g. ``libblas``).  The neural network expressions cannot be evaluated by Theano and it's raising an exception.\n\n**FIX:** ``sudo apt-get install libblas-dev libopenblas-dev``\n\n\nTypeError: max_pool_2d() got an unexpected keyword argument 'mode'\n------------------------------------------------------------------\n\nYou need to install Lasagne and Theano directly from the versions specified in ``requirements.txt``, rather than from the PIP versions.  These alternatives are older and don't have the required features.\n\n**FIX:** ``python3 -m pip install -r requirements.txt``\n\n\nValueError: unknown locale: UTF-8\n---------------------------------\n\nIt seems your terminal is misconfigured and not compatible with the way Python treats locales. You may need to change this in your ``.bashrc`` or other startup script. Alternatively, this command will fix it once for this shell instance.\n\n**FIX:** ``export LC_ALL=en_US.UTF-8``\n\n.. image:: docs/StreetView_example.gif\n\n**Example #4** \u2014 Street View: `view comparison <http://enhance.nucl.ai/w/3b3c8054-9d00-11e6-9558-c86000be451f/view>`_ in 24-bit HD, `original photo <https://flic.kr/p/gnxcXH>`_ CC-BY-SA @cyalex.\n\n----\n\n|Python Version| |License Type| |Project Stars|\n\n.. |Python Version| image:: http://aigamedev.github.io/scikit-neuralnetwork/badge_python.svg\n    :target: https://www.python.org/\n\n.. |License Type| image:: https://img.shields.io/badge/license-AGPL-blue.svg\n    :target: https://github.com/alexjc/neural-enhance/blob/master/LICENSE\n\n.. |Project Stars| image:: https://img.shields.io/github/stars/alexjc/neural-enhance.svg?style=flat\n    :target: https://github.com/alexjc/neural-enhance/stargazers\n",
        "releases": [
            {
                "name": "Release 0.3",
                "date": "2016-11-13T18:02:16Z"
            },
            {
                "name": "Release 0.2",
                "date": "2016-11-03T21:41:34Z"
            },
            {
                "name": "Neural Enhance 0.1",
                "date": "2016-10-28T22:50:25Z"
            }
        ]
    }
}