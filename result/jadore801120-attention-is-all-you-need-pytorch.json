{
    "https://api.github.com/repos/jadore801120/attention-is-all-you-need-pytorch": {
        "forks": 1992,
        "watchers": 8984,
        "stars": 8984,
        "languages": {
            "Python": 67050,
            "Shell": 468
        },
        "commits": [
            "2021-02-17T13:30:47Z",
            "2021-02-17T13:03:32Z",
            "2021-02-17T12:39:56Z",
            "2021-02-03T15:12:21Z",
            "2021-01-12T04:42:55Z",
            "2020-06-07T13:23:57Z",
            "2020-06-07T13:22:09Z",
            "2020-06-07T13:20:21Z",
            "2020-05-27T14:17:12Z",
            "2020-05-27T08:51:57Z",
            "2019-12-22T14:30:01Z",
            "2019-12-22T14:18:15Z",
            "2019-12-22T14:03:04Z",
            "2019-12-22T13:48:19Z",
            "2019-12-17T07:25:03Z",
            "2019-12-17T07:12:38Z",
            "2019-12-17T04:28:56Z",
            "2019-12-08T09:31:36Z",
            "2019-12-08T09:31:12Z",
            "2019-12-08T08:02:04Z",
            "2019-12-08T08:01:50Z",
            "2019-12-08T03:36:12Z",
            "2019-12-05T15:49:50Z",
            "2019-12-05T15:33:23Z",
            "2019-12-05T09:13:51Z",
            "2019-12-05T09:13:21Z",
            "2019-12-05T09:12:38Z",
            "2019-12-04T13:26:18Z",
            "2019-12-03T05:33:47Z",
            "2019-11-30T10:03:45Z"
        ],
        "creation_date": "2017-06-14T10:15:20Z",
        "contributors": 7,
        "topics": [
            "attention",
            "attention-is-all-you-need",
            "deep-learning",
            "natural-language-processing",
            "nlp",
            "pytorch"
        ],
        "subscribers": 97,
        "readme": "# Attention is all you need: A Pytorch Implementation\n\nThis is a PyTorch implementation of the Transformer model in \"[Attention is All You Need](https://arxiv.org/abs/1706.03762)\" (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, arxiv, 2017). \n\n\nA novel sequence to sequence framework utilizes the **self-attention mechanism**, instead of Convolution operation or Recurrent structure, and achieve the state-of-the-art performance on **WMT 2014 English-to-German translation task**. (2017/06/12)\n\n> The official Tensorflow Implementation can be found in: [tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py).\n\n> To learn more about self-attention mechanism, you could read \"[A Structured Self-attentive Sentence Embedding](https://arxiv.org/abs/1703.03130)\".\n\n<p align=\"center\">\n<img src=\"http://imgur.com/1krF2R6.png\" width=\"250\">\n</p>\n\n\nThe project support training and translation with trained model now.\n\nNote that this project is still a work in progress.\n\n**BPE related parts are not yet fully tested.**\n\n\nIf there is any suggestion or error, feel free to fire an issue to let me know. :)\n\n\n# Usage\n\n## WMT'16 Multimodal Translation: de-en\n\nAn example of training for the WMT'16 Multimodal Translation task (http://www.statmt.org/wmt16/multimodal-task.html).\n\n### 0) Download the spacy language model.\n```bash\n# conda install -c conda-forge spacy \npython -m spacy download en\npython -m spacy download de\n```\n\n### 1) Preprocess the data with torchtext and spacy.\n```bash\npython preprocess.py -lang_src de -lang_trg en -share_vocab -save_data m30k_deen_shr.pkl\n```\n\n### 2) Train the model\n```bash\npython train.py -data_pkl m30k_deen_shr.pkl -log m30k_deen_shr -embs_share_weight -proj_share_weight -label_smoothing -output_dir output -b 256 -warmup 128000 -epoch 400\n```\n\n### 3) Test the model\n```bash\npython translate.py -data_pkl m30k_deen_shr.pkl -model trained.chkpt -output prediction.txt\n```\n\n## [(WIP)] WMT'17 Multimodal Translation: de-en w/ BPE \n### 1) Download and preprocess the data with bpe:\n\n> Since the interfaces is not unified, you need to switch the main function call from `main_wo_bpe` to `main`.\n\n```bash\npython preprocess.py -raw_dir /tmp/raw_deen -data_dir ./bpe_deen -save_data bpe_vocab.pkl -codes codes.txt -prefix deen\n```\n\n### 2) Train the model\n```bash\npython train.py -data_pkl ./bpe_deen/bpe_vocab.pkl -train_path ./bpe_deen/deen-train -val_path ./bpe_deen/deen-val -log deen_bpe -embs_share_weight -proj_share_weight -label_smoothing -output_dir output -b 256 -warmup 128000 -epoch 400\n```\n\n### 3) Test the model (not ready)\n- TODO:\n\t- Load vocabulary.\n\t- Perform decoding after the translation.\n---\n# Performance\n## Training\n\n<p align=\"center\">\n<img src=\"https://i.imgur.com/S2EVtJx.png\" width=\"400\">\n<img src=\"https://i.imgur.com/IZQmUKO.png\" width=\"400\">\n</p>\n\n- Parameter settings:\n  - batch size 256 \n  - warmup step 4000 \n  - epoch 200 \n  - lr_mul 0.5\n  - label smoothing \n  - do not apply BPE and shared vocabulary\n  - target embedding / pre-softmax linear layer weight sharing. \n \n  \n## Testing \n- coming soon.\n---\n# TODO\n  - Evaluation on the generated text.\n  - Attention weight plot.\n---\n# Acknowledgement\n- The byte pair encoding parts are borrowed from [subword-nmt](https://github.com/rsennrich/subword-nmt/).\n- The project structure, some scripts and the dataset preprocessing steps are heavily borrowed from [OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py).\n- Thanks for the suggestions from @srush, @iamalbert, @Zessay, @JulesGM, @ZiJianZhao, and @huanghoujing.\n",
        "releases": []
    }
}