{
    "https://api.github.com/repos/Yelp/elastalert": {
        "forks": 1736,
        "watchers": 8005,
        "stars": 8005,
        "languages": {
            "Python": 653487,
            "Makefile": 528
        },
        "commits": [
            "2022-11-12T00:58:08Z",
            "2020-11-09T18:19:16Z",
            "2020-10-05T10:29:50Z",
            "2020-09-11T01:05:31Z",
            "2020-09-01T01:06:15Z",
            "2020-08-31T23:50:45Z",
            "2020-08-31T21:21:21Z",
            "2020-08-29T18:25:23Z",
            "2020-08-26T17:53:34Z",
            "2020-07-29T18:03:56Z",
            "2020-06-02T16:09:51Z",
            "2020-05-26T07:36:11Z",
            "2020-05-13T18:00:28Z",
            "2020-05-03T16:03:59Z",
            "2020-04-16T22:40:53Z",
            "2020-04-16T22:40:34Z",
            "2020-04-16T22:10:08Z",
            "2020-04-15T22:57:42Z",
            "2020-04-15T21:45:12Z",
            "2020-04-15T21:42:52Z",
            "2020-04-15T21:17:19Z",
            "2020-04-14T22:07:48Z",
            "2020-04-14T22:00:16Z",
            "2020-04-14T21:12:19Z",
            "2020-04-07T20:19:39Z",
            "2020-01-28T01:29:20Z",
            "2020-01-02T19:22:36Z",
            "2019-12-05T22:43:21Z",
            "2019-12-05T22:10:34Z",
            "2019-12-05T20:39:16Z"
        ],
        "creation_date": "2014-11-24T19:39:19Z",
        "contributors": 30,
        "topics": [],
        "subscribers": 244,
        "readme": "**ElastAlert is no longer maintained. Please use [ElastAlert2](https://github.com/jertel/elastalert2) instead.**\n\n\n[![Build Status](https://travis-ci.org/Yelp/elastalert.svg)](https://travis-ci.org/Yelp/elastalert)\n[![Join the chat at https://gitter.im/Yelp/elastalert](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/Yelp/elastalert?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n## ElastAlert - [Read the Docs](http://elastalert.readthedocs.org).\n### Easy & Flexible Alerting With Elasticsearch\n\nElastAlert is a simple framework for alerting on anomalies, spikes, or other patterns of interest from data in Elasticsearch.\n\nElastAlert works with all versions of Elasticsearch.\n\nAt Yelp, we use Elasticsearch, Logstash and Kibana for managing our ever increasing amount of data and logs.\nKibana is great for visualizing and querying data, but we quickly realized that it needed a companion tool for alerting\non inconsistencies in our data. Out of this need, ElastAlert was created.\n\nIf you have data being written into Elasticsearch in near real time and want to be alerted when that data matches certain patterns, ElastAlert is the tool for you. If you can see it in Kibana, ElastAlert can alert on it.\n\n## Overview\n\nWe designed ElastAlert to be reliable, highly modular, and easy to set up and configure.\n\nIt works by combining Elasticsearch with two types of components, rule types and alerts.\nElasticsearch is periodically queried and the data is passed to the rule type, which determines when\na match is found. When a match occurs, it is given to one or more alerts, which take action based on the match.\n\nThis is configured by a set of rules, each of which defines a query, a rule type, and a set of alerts.\n\nSeveral rule types with common monitoring paradigms are included with ElastAlert:\n\n- Match where there are at least X events in Y time\" (``frequency`` type)\n- Match when the rate of events increases or decreases\" (``spike`` type)\n- Match when there are less than X events in Y time\" (``flatline`` type)\n- Match when a certain field matches a blacklist/whitelist\" (``blacklist`` and ``whitelist`` type)\n- Match on any event matching a given filter\" (``any`` type)\n- Match when a field has two different values within some time\" (``change`` type)\n- Match when a never before seen term appears in a field\" (``new_term`` type)\n- Match when the number of unique values for a field is above or below a threshold (``cardinality`` type)\n\nCurrently, we have built-in support for the following alert types:\n\n- Email\n- JIRA\n- OpsGenie\n- Commands\n- HipChat\n- MS Teams\n- Slack\n- Telegram\n- GoogleChat\n- AWS SNS\n- VictorOps\n- PagerDuty\n- PagerTree\n- Exotel\n- Twilio\n- Gitter\n- Line Notify\n- Zabbix\n\nAdditional rule types and alerts can be easily imported or written.\n\nIn addition to this basic usage, there are many other features that make alerts more useful:\n\n- Alerts link to Kibana dashboards\n- Aggregate counts for arbitrary fields\n- Combine alerts into periodic reports\n- Separate alerts by using a unique key field\n- Intercept and enhance match data\n\nTo get started, check out `Running ElastAlert For The First Time` in the [documentation](http://elastalert.readthedocs.org).\n\n## Running ElastAlert\nYou can either install the latest released version of ElastAlert using pip:\n\n```pip install elastalert```\n\nor you can clone the ElastAlert repository for the most recent changes:\n\n```git clone https://github.com/Yelp/elastalert.git```\n\nInstall the module:\n\n```pip install \"setuptools>=11.3\"```\n\n```python setup.py install```\n\nThe following invocation can be used to run ElastAlert after installing\n\n``$ elastalert [--debug] [--verbose] [--start <timestamp>] [--end <timestamp>] [--rule <filename.yaml>] [--config <filename.yaml>]``\n\n``--debug`` will print additional information to the screen as well as suppresses alerts and instead prints the alert body. Not compatible with `--verbose`.\n\n``--verbose`` will print additional information without suppressing alerts. Not compatible with `--debug.`\n\n``--start`` will begin querying at the given timestamp. By default, ElastAlert will begin querying from the present.\nTimestamp format is ``YYYY-MM-DDTHH-MM-SS[-/+HH:MM]`` (Note the T between date and hour).\nEg: ``--start 2014-09-26T12:00:00`` (UTC) or ``--start 2014-10-01T07:30:00-05:00``\n\n``--end`` will cause ElastAlert to stop querying at the given timestamp. By default, ElastAlert will continue\nto query indefinitely.\n\n``--rule`` will allow you to run only one rule. It must still be in the rules folder.\nEg: ``--rule this_rule.yaml``\n\n``--config`` allows you to specify the location of the configuration. By default, it is will look for config.yaml in the current directory.\n\n## Third Party Tools And Extras\n### Kibana plugin\n![img](https://raw.githubusercontent.com/bitsensor/elastalert-kibana-plugin/master/showcase.gif)\nAvailable at the [ElastAlert Kibana plugin repository](https://github.com/bitsensor/elastalert-kibana-plugin).\n\n### Docker\nA [Dockerized version](https://github.com/bitsensor/elastalert) of ElastAlert including a REST api is build from `master` to `bitsensor/elastalert:latest`.\n\n```bash\ngit clone https://github.com/bitsensor/elastalert.git; cd elastalert\ndocker run -d -p 3030:3030 \\\n    -v `pwd`/config/elastalert.yaml:/opt/elastalert/config.yaml \\\n    -v `pwd`/config/config.json:/opt/elastalert-server/config/config.json \\\n    -v `pwd`/rules:/opt/elastalert/rules \\\n    -v `pwd`/rule_templates:/opt/elastalert/rule_templates \\\n    --net=\"host\" \\\n    --name elastalert bitsensor/elastalert:latest\n```\n\n## Documentation\n\nRead the documentation at [Read the Docs](http://elastalert.readthedocs.org).\n\nTo build a html version of the docs locally\n\n```\npip install sphinx_rtd_theme sphinx\ncd docs\nmake html\n```\n\nView in browser at build/html/index.html\n\n## Configuration\n\nSee config.yaml.example for details on configuration.\n\n## Example rules\n\nExamples of different types of rules can be found in example_rules/.\n\n- ``example_spike.yaml`` is an example of the \"spike\" rule type, which allows you to alert when the rate of events, averaged over a time period,\nincreases by a given factor. This example will send an email alert when there are 3 times more events matching a filter occurring within the\nlast 2 hours than the number of events in the previous 2 hours.\n\n- ``example_frequency.yaml`` is an example of the \"frequency\" rule type, which will alert when there are a given number of events occuring\nwithin a time period. This example will send an email when 50 documents matching a given filter occur within a 4 hour timeframe.\n\n- ``example_change.yaml`` is an example of the \"change\" rule type, which will alert when a certain field in two documents changes. In this example,\nthe alert email is sent when two documents with the same 'username' field but a different value of the 'country_name' field occur within 24 hours\nof each other.\n\n- ``example_new_term.yaml`` is an example of the \"new term\" rule type, which alerts when a new value appears in a field or fields. In this example,\nan email is sent when a new value of (\"username\", \"computer\") is encountered in example login logs.\n\n## Frequently Asked Questions\n\n### My rule is not getting any hits?\n\nSo you've managed to set up ElastAlert, write a rule, and run it, but nothing happens, or it says ``0 query hits``. First of all, we recommend using the command ``elastalert-test-rule rule.yaml`` to debug. It will show you how many documents match your filters for the last 24 hours (or more, see ``--help``), and then shows you if any alerts would have fired. If you have a filter in your rule, remove it and try again. This will show you if the index is correct and that you have at least some documents. If you have a filter in Kibana and want to recreate it in ElastAlert, you probably want to use a query string. Your filter will look like\n\n```\nfilter:\n- query:\n    query_string:\n      query: \"foo: bar AND baz: abc*\"\n```\nIf you receive an error that Elasticsearch is unable to parse it, it's likely the YAML is not spaced correctly, and the filter is not in the right format. If you are using other types of filters, like ``term``, a common pitfall is not realizing that you may need to use the analyzed token. This is the default if you are using Logstash. For example,\n\n```\nfilter:\n- term:\n    foo: \"Test Document\"\n```\n\nwill not match even if the original value for ``foo`` was exactly \"Test Document\". Instead, you want to use ``foo.raw``. If you are still having trouble troubleshooting why your documents do not match, try running ElastAlert with ``--es_debug_trace /path/to/file.log``. This will log the queries made to Elasticsearch in full so that you can see exactly what is happening.\n\n### I got hits, why didn't I get an alert?\n\nIf you got logs that had ``X query hits, 0 matches, 0 alerts sent``, it depends on the ``type`` why you didn't get any alerts. If ``type: any``, a match will occur for every hit. If you are using ``type: frequency``, ``num_events`` must occur within ``timeframe`` of each other for a match to occur. Different rules apply for different rule types.\n\nIf you see ``X matches, 0 alerts sent``, this may occur for several reasons. If you set ``aggregation``, the alert will not be sent until after that time has elapsed. If you have gotten an alert for this same rule before, that rule may be silenced for a period of time. The default is one minute between alerts. If a rule is silenced, you will see ``Ignoring match for silenced rule`` in the logs.\n\nIf you see ``X alerts sent`` but didn't get any alert, it's probably related to the alert configuration. If you are using the ``--debug`` flag, you will not receive any alerts. Instead, the alert text will be written to the console. Use ``--verbose`` to achieve the same affects without preventing alerts. If you are using email alert, make sure you have it configured for an SMTP server. By default, it will connect to localhost on port 25. It will also use the word \"elastalert\" as the \"From:\" address. Some SMTP servers will reject this because it does not have a domain while others will add their own domain automatically. See the email section in the documentation for how to configure this.\n\n### Why did I only get one alert when I expected to get several?\n\nThere is a setting called ``realert`` which is the minimum time between two alerts for the same rule. Any alert that occurs within this time will simply be dropped. The default value for this is one minute. If you want to receive an alert for every single match, even if they occur right after each other, use\n\n```\nrealert:\n  minutes: 0\n```\n\nYou can of course set it higher as well.\n\n### How can I prevent duplicate alerts?\n\nBy setting ``realert``, you will prevent the same rule from alerting twice in an amount of time.\n\n```\nrealert:\n  days: 1\n```\n\nYou can also prevent duplicates based on a certain field by using ``query_key``. For example, to prevent multiple alerts for the same user, you might use\n\n```\nrealert:\n  hours: 8\nquery_key: user\n```\n\nNote that this will also affect the way many rule types work. If you are using ``type: frequency`` for example, ``num_events`` for a single value of ``query_key`` must occur before an alert will be sent. You can also use a compound of multiple fields for this key. For example, if you only wanted to receieve an alert once for a specific error and hostname, you could use\n\n```\nquery_key: [error, hostname]\n```\n\nInternally, this works by creating a new field for each document called ``field1,field2`` with a value of ``value1,value2`` and using that as the ``query_key``.\n\nThe data for when an alert will fire again is stored in Elasticsearch in the ``elastalert_status`` index, with a ``_type`` of ``silence`` and also cached in memory.\n\n### How can I change what's in the alert?\n\nYou can use the field ``alert_text`` to add custom text to an alert. By setting ``alert_text_type: alert_text_only``, it will be the entirety of the alert. You can also add different fields from the alert by using Python style string formatting and ``alert_text_args``. For example\n\n```\nalert_text: \"Something happened with {0} at {1}\"\nalert_text_type: alert_text_only\nalert_text_args: [\"username\", \"@timestamp\"]\n```\n\nYou can also limit the alert to only containing certain fields from the document by using ``include``.\n\n```\ninclude: [\"ip_address\", \"hostname\", \"status\"]\n```\n\n### My alert only contains data for one event, how can I see more?\n\nIf you are using ``type: frequency``, you can set the option ``attach_related: true`` and every document will be included in the alert. An alternative, which works for every type, is ``top_count_keys``. This will show the top counts for each value for certain fields. For example, if you have\n\n```\ntop_count_keys: [\"ip_address\", \"status\"]\n```\n\nand 10 documents matched your alert, it may contain something like\n\n```\nip_address:\n127.0.0.1: 7\n10.0.0.1: 2\n192.168.0.1: 1\n\nstatus:\n200: 9\n500: 1\n```\n\n### How can I make the alert come at a certain time?\n\nThe ``aggregation`` feature will take every alert that has occured over a period of time and send them together in one alert. You can use cron style syntax to send all alerts that have occured since the last once by using\n\n```\naggregation:\n  schedule: '2 4 * * mon,fri'\n```\n\n### I have lots of documents and it's really slow, how can I speed it up?\n\nThere are several ways to potentially speed up queries. If you are using ``index: logstash-*``, Elasticsearch will query all shards, even if they do not possibly contain data with the correct timestamp. Instead, you can use Python time format strings and set ``use_strftime_index``\n\n```\nindex: logstash-%Y.%m\nuse_strftime_index: true\n```\n\nAnother thing you could change is ``buffer_time``. By default, ElastAlert will query large overlapping windows in order to ensure that it does not miss any events, even if they are indexed in real time. In config.yaml, you can adjust ``buffer_time`` to a smaller number to only query the most recent few minutes.\n\n```\nbuffer_time:\n  minutes: 5\n```\n\nBy default, ElastAlert will download every document in full before processing them. Instead, you can have ElastAlert simply get a count of the number of documents that have occured in between each query. To do this, set ``use_count_query: true``. This cannot be used if you use ``query_key``, because ElastAlert will not know the contents of each documents, just the total number of them. This also reduces the precision of alerts, because all events that occur between each query will be rounded to a single timestamp.\n\nIf you are using ``query_key`` (a single key, not multiple keys) you can use ``use_terms_query``. This will make ElastAlert perform a terms aggregation to get the counts for each value of a certain field. Both ``use_terms_query`` and ``use_count_query`` also require ``doc_type`` to be set to the ``_type`` of the documents. They may not be compatible with all rule types.\n\n### Can I perform aggregations?\n\nThe only aggregation supported currently is a terms aggregation, by setting ``use_terms_query``.\n\n### I'm not using @timestamp, what do I do?\n\nYou can use ``timestamp_field`` to change which field ElastAlert will use as the timestamp. You can use ``timestamp_type`` to change it between ISO 8601 and unix timestamps. You must have some kind of timestamp for ElastAlert to work. If your events are not in real time, you can use ``query_delay`` and ``buffer_time`` to adjust when ElastAlert will look for documents.\n\n### I'm using flatline but I don't see any alerts\n\nWhen using ``type: flatline``, ElastAlert must see at least one document before it will alert you that it has stopped seeing them.\n\n### How can I get a \"resolve\" event?\n\nElastAlert does not currently support stateful alerts or resolve events.\n\n### Can I set a warning threshold?\n\nCurrently, the only way to set a warning threshold is by creating a second rule with a lower threshold.\n\n## License\n\nElastAlert is licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0\n\n### Read the documentation at [Read the Docs](http://elastalert.readthedocs.org).\n\n### Questions? Drop by #elastalert on Freenode IRC.\n",
        "releases": [
            {
                "name": "",
                "date": "2015-06-25T22:12:51Z"
            }
        ]
    }
}