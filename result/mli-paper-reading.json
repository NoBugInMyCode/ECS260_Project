{
    "https://api.github.com/repos/mli/paper-reading": {
        "forks": 2485,
        "watchers": 27899,
        "stars": 27899,
        "languages": {},
        "commits": [
            "2024-11-17T19:49:37Z",
            "2024-08-08T22:52:45Z",
            "2024-08-08T03:32:53Z",
            "2023-03-30T22:12:39Z",
            "2023-03-23T22:26:25Z",
            "2023-03-22T16:51:07Z",
            "2023-03-10T23:13:35Z",
            "2023-01-11T17:28:46Z",
            "2022-12-29T02:32:18Z",
            "2022-12-29T02:30:47Z",
            "2022-12-19T19:09:14Z",
            "2022-12-19T19:04:40Z",
            "2022-12-11T18:56:04Z",
            "2022-11-14T06:36:59Z",
            "2022-11-14T00:31:19Z",
            "2022-11-14T00:23:07Z",
            "2022-11-14T00:21:17Z",
            "2022-11-14T00:20:05Z",
            "2022-11-11T06:29:43Z",
            "2022-11-11T06:28:38Z",
            "2022-11-11T06:22:50Z",
            "2022-11-02T02:07:32Z",
            "2022-11-02T02:03:23Z",
            "2022-10-27T19:06:52Z",
            "2022-10-27T18:46:29Z",
            "2022-10-25T21:38:11Z",
            "2022-10-10T01:13:50Z",
            "2022-09-13T18:34:32Z",
            "2022-09-12T17:31:12Z",
            "2022-07-29T22:16:42Z"
        ],
        "creation_date": "2021-10-22T06:34:51Z",
        "contributors": 9,
        "topics": [
            "deep-learning",
            "paper",
            "reading-list"
        ],
        "subscribers": 742,
        "readme": "# \u6df1\u5ea6\u5b66\u4e60\u8bba\u6587\u7cbe\u8bfb\n\n## \u5f55\u5236\u5b8c\u6210\u7684\u8bba\u6587\n\n| \u65e5\u671f | \u6807\u9898 | \u5c01\u9762 | \u65f6\u957f | \u89c6\u9891\uff08\u64ad\u653e\u6570\uff09 |\n| --: | -- | -- | --: | -- |\n| 9/04/24 | Llama 3.1\u8bba\u6587\u7cbe\u8bfb \u00b7 5. \u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b | <img src=\"imgs/llama3-process.jpg\" width=\"200px\"/> | 10:41| [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1c8HbeaEXi)](https://www.bilibili.com/video/BV1c8HbeaEXi)<br />  |\n| 8/28/24 | Llama 3.1\u8bba\u6587\u7cbe\u8bfb \u00b7 4. \u8bad\u7ec3infra | <img src=\"imgs/llama3-training-infra.webp\" width=\"200px\"/> | 25:04| [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1b4421f7fa)](https://www.bilibili.com/video/BV1b4421f7fa)<br />[![](https://img.shields.io/youtube/views/6XidEHVjS1A?style=social)](https://www.youtube.com/watch?v=6XidEHVjS1A)  |\n| 8/13/24 | Llama 3.1\u8bba\u6587\u7cbe\u8bfb \u00b7 3. \u6a21\u578b | <img src=\"imgs/llama3-model.webp\" width=\"200px\"/> | 26:14| [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Q4421Z7Tj)](https://www.bilibili.com/video/BV1Q4421Z7Tj)<br />[![](https://img.shields.io/youtube/views/G6gF-5g1Gg4?style=social)](https://www.youtube.com/watch?v=G6gF-5g1Gg4)  |\n| 8/05/24 | [Llama 3.1\u8bba\u6587\u7cbe\u8bfb \u00b7 2. \u9884\u8bad\u7ec3\u6570\u636e](https://arxiv.org/pdf/2407.21783) | <img src=\"imgs/llama3-pretrain-data.jpg\" width=\"200px\"/> | 23:37| [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1u142187S5)](https://www.bilibili.com/video/BV1u142187S5)[![](https://img.shields.io/youtube/views/wXFr3zIE8FM?style=social)](https://www.youtube.com/watch?v=wXFr3zIE8FM)|\n| 7/31/24 | Llama 3.1\u8bba\u6587\u7cbe\u8bfb \u00b7 1. \u5bfc\u8a00 | <img src=\"imgs/llama3-intro.jpg\" width=\"200px\"/> | 18:53| [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1WM4m1y7Uh)](https://www.bilibili.com/video/BV1WM4m1y7Uh)<br />[![](https://img.shields.io/youtube/views/-PztagF3wQE?style=social)](https://www.youtube.com/watch?v=-PztagF3wQE)  |\n| 3/30/23 | [GPT-4](https://openai.com/research/gpt-4) | <img src=\"imgs/gpt4.jpg\" width=\"200px\"/> | 1:20:38 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1vM4y1U7b5)](https://www.bilibili.com/video/BV1vM4y1U7b5)<br />[![](https://img.shields.io/youtube/views/K0SZ9mdygTw?style=social)](https://youtu.be/K0SZ9mdygTw)  |\n| 3/23/23 | \u5927\u6a21\u578b\u65f6\u4ee3\u4e0b\u505a\u79d1\u7814\u7684\u56db\u4e2a\u601d\u8def | <img src=\"imgs/limited-resources.jpg\" width=\"200px\"/> | 1:06:29 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1oX4y1d7X6)](https://www.bilibili.com/video/BV1oX4y1d7X6)<br />[![](https://img.shields.io/youtube/views/sh79Z8i15PI?style=social)](https://youtu.be/sh79Z8i15PI) |\n| 3/10/23 | [Anthropic LLM](https://arxiv.org/pdf/2204.05862.pdf) | <img src=\"imgs/anthropic_lm.jpg\" width=\"200px\"/> | 1:01:51 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1XY411B7nM)](https://www.bilibili.com/video/BV1XY411B7nM)<br />[![](https://img.shields.io/youtube/views/iqX0pgNDon0?style=social)](https://youtu.be/iqX0pgNDon0) |\n| 1/20/23 | [Helm](https://arxiv.org/pdf/2211.09110.pdf) \u5168\u9762\u8bed\u8a00\u6a21\u578b\u8bc4\u6d4b | <img src=\"imgs/helm.jpg\" width=\"200px\"/> | 1:23:37 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1z24y1B7uX)](https://www.bilibili.com/video/BV1z24y1B7uX)<br />[![](https://img.shields.io/youtube/views/WgFEw9U3BXA?style=social)](https://youtu.be/WgFEw9U3BXA) |\n| 1/11/23 | \u591a\u6a21\u6001\u8bba\u6587\u4e32\u8bb2\u00b7\u4e0b |  <img src=\"imgs/multimodal-2.jpg\" width=\"200px\"/> | 1:03:29 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1fA411Z772)](https://www.bilibili.com/video/BV1fA411Z772) <br />[![](https://img.shields.io/youtube/views/S1le41J76lQ?style=social)](https://youtu.be/S1le41J76lQ) |\n| 12/29/22 | [Instruct GPT](https://arxiv.org/pdf/2203.02155.pdf) | <img src=\"imgs/instruct-gpt.jpg\" width=\"200px\"/> | 1:07:10 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1hd4y187CR)](https://www.bilibili.com/video/BV1hd4y187CR) <br />[![](https://img.shields.io/youtube/views/zfIGAwD1jOQ?style=social)](https://youtu.be/zfIGAwD1jOQ) |\n| 12/19/22 | [Neural Corpus Indexer](https://arxiv.org/pdf/2206.02743.pdf) \u6587\u6863\u68c0\u7d22 | <img src=\"imgs/nci.jpg\" width=\"200px\"/> | 55:47 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Se411w7Sn)](https://www.bilibili.com/video/BV1Se411w7Sn) <br />[![](https://img.shields.io/youtube/views/QRffZMSGJyU?style=social)](https://youtu.be/QRffZMSGJyU) |\n| 12/12/22 | \u591a\u6a21\u6001\u8bba\u6587\u4e32\u8bb2\u00b7\u4e0a | <img src=\"imgs/multimodal-1.jpg\" width=\"200px\"/> | 1:12:27 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Vd4y1v77v)](https://www.bilibili.com/video/BV1Vd4y1v77v) <br />[![](https://img.shields.io/youtube/views/6pzBOQAXUB8?style=social)](https://youtu.be/6pzBOQAXUB8)  |\n| 11/14/22 | [OpenAI Whisper](https://cdn.openai.com/papers/whisper.pdf) \u7cbe\u8bfb | <img src=\"imgs/whisper.jpg\" width=\"200px\"/> | 1:12:16 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1VG4y1t74x)](https://www.bilibili.com/video/BV1VG4y1t74x) <br />[![](https://img.shields.io/youtube/views/3eXCJd32UnM?style=social)](https://youtu.be/3eXCJd32UnM) |\n| 11/07/22 | \u5728\u8bb2 OpenAI Whisper \u524d\u5148\u505a\u4e86\u4e00\u4e2a\u526a\u89c6\u9891\u5c0f\u5de5\u5177 | <img src=\"imgs/autocut.jpg\" width=\"200px\"/> | 23:39 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Pe4y1t7de)](https://www.bilibili.com/video/BV1Pe4y1t7de) <br />[![](https://img.shields.io/youtube/views/PwVlvCPDnrI?style=social)](https://youtu.be/PwVlvCPDnrI)  |\n| 10/23/22 | [Chain of Thought](https://arxiv.org/pdf/2201.11903.pdf) \u8bba\u6587\u3001\u4ee3\u7801\u548c\u8d44\u6e90 | <img src=\"imgs/cot.jpg\" width=\"200px\"/> | 33:21 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1t8411e7Ug)](https://www.bilibili.com/video/BV1t8411e7Ug)<br />[![](https://img.shields.io/youtube/views/H4J59iG3t5o?style=social)](https://youtu.be/H4J59iG3t5o) |\n| 9/17/22 | CLIP \u6539\u8fdb\u5de5\u4f5c\u4e32\u8bb2\uff08\u4e0b\uff09 | <img src=\"imgs/clipx-part2.jpg\" width=\"200px\"/> | 1:04:26 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1gg411U7n4)](https://www.bilibili.com/video/BV1gg411U7n4)<br />[![](https://img.shields.io/youtube/views/ugJeBivv65s?style=social)](https://youtu.be/ugJeBivv65s) |\n| 9/2/22 | CLIP \u6539\u8fdb\u5de5\u4f5c\u4e32\u8bb2\uff08\u4e0a\uff09 | <img src=\"imgs/clipx-part1.jpg\" width=\"200px\"/> | 1:14:43 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1FV4y1p7Lm)](https://www.bilibili.com/video/BV1FV4y1p7Lm)<br />[![](https://img.shields.io/youtube/views/x4CDhZz_Dvg?style=social)](https://youtu.be/x4CDhZz_Dvg) |\n| 7/29/22 | [ViLT](https://arxiv.org/pdf/2102.03334.pdf) \u8bba\u6587\u7cbe\u8bfb | <img src=\"imgs/vilt.jpg\" width=\"200px\"/> | 1:03:26 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV14r4y1j74y)](https://www.bilibili.com/video/BV14r4y1j74y)<br />[![](https://img.shields.io/youtube/views/ug8YvZOjOCE?style=social)](https://youtu.be/ug8YvZOjOCE) |\n| 7/22/22 | \u7406\u7531\u3001\u8bba\u636e\u548c\u62c5\u4fdd\u3010[\u7814\u7a76\u7684\u827a\u672f](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)\u00b7\u56db\u3011 | <img src=\"imgs/craft_research_p4.jpg\" width=\"200px\"/> | 44:14 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1SB4y1a75c)](https://www.bilibili.com/video/BV1SB4y1a75c) |\n| 7/15/22 | \u5982\u4f55\u8bb2\u597d\u6545\u4e8b\u3001\u6545\u4e8b\u91cc\u7684\u8bba\u70b9\u3010[\u7814\u7a76\u7684\u827a\u672f](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)\u00b7\u4e09\u3011| <img src=\"imgs/craft_research_p3.jpg\" width=\"200px\"/> | 43:56 |[![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1WB4y1v7ST)](https://www.bilibili.com/video/BV1WB4y1v7ST)|\n| 7/8/22 | [DALL\u00b7E 2](https://arxiv.org/pdf/2204.06125.pdf) \u9010\u6bb5\u7cbe\u8bfb | <img src=\"imgs/dalle2.jpg\" width=\"200px\"/> | 1:27:54 |[![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV17r4y1u77B)](https://www.bilibili.com/video/BV17r4y1u77B)<br />[![](https://img.shields.io/youtube/views/hO57mntSMl0?style=social)](https://youtu.be/hO57mntSMl0)|\n| 7/1/22 | \u660e\u767d\u95ee\u9898\u7684\u91cd\u8981\u6027\u3010[\u7814\u7a76\u7684\u827a\u672f](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)\u00b7\u4e8c\u3011| <img src=\"imgs/craft_research_p2.jpg\" width=\"200px\"/> | 1:03:40 |[![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV11S4y1v7S2)](https://www.bilibili.com/video/BV11S4y1v7S2/)|\n| 6/24/22 | \u8ddf\u8bfb\u8005\u5efa\u7acb\u8054\u7cfb\u3010[\u7814\u7a76\u7684\u827a\u672f](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)\u00b7\u4e00\u3011 | <img src=\"imgs/craft_research_p1.jpg\" width=\"200px\"/> | 45:01 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1hY411T7vy)](https://www.bilibili.com/video/BV1hY411T7vy/) |\n| 6/17/22 | [Zero](https://arxiv.org/pdf/1910.02054.pdf) \u9010\u6bb5\u7cbe\u8bfb | <img src=\"imgs/zero.jpg\" width=\"200px\"/> | 52:21 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1tY411g7ZT)](https://www.bilibili.com/video/BV1tY411g7ZT/) |\n| 6/10/22 | [DETR](https://arxiv.org/pdf/2005.12872.pdf) \u9010\u6bb5\u7cbe\u8bfb | <img src=\"imgs/detr.jpg\" width=\"200px\"/> | 54:22 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1GB4y1X72R)](https://www.bilibili.com/video/BV1GB4y1X72R/) |\n| 6/3/22 | [Megatron LM](https://arxiv.org/pdf/1909.08053.pdf) \u9010\u6bb5\u7cbe\u8bfb | <img src=\"imgs/megatron_lm.jpg\" width=\"200px\"/> | 56:07 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1nB4y1R7Yz)](https://www.bilibili.com/video/BV1nB4y1R7Yz/) |\n| 5/27/22 | [GPipe](https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf) \u9010\u6bb5\u7cbe\u8bfb | <img src=\"imgs/gpipe.jpg\" width=\"200px\"/> | 58:47 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1v34y1E7zu)](https://www.bilibili.com/video/BV1v34y1E7zu/) <br />[![](https://img.shields.io/youtube/views/eXjRpS_BTbs?style=social)](https://youtu.be/eXjRpS_BTbs)  |\n| 5/5/22 | [Pathways](https://arxiv.org/pdf/2203.12533.pdf) \u9010\u6bb5\u7cbe\u8bfb | <img src=\"imgs/pathways.jpg\" width=\"200px\"/> | 1:02:13 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1xB4y1m7Xi)](https://www.bilibili.com/video/BV1xB4y1m7Xi/) <br />[![](https://img.shields.io/youtube/views/8hS1ZtgG0wU?style=social)](https://youtu.be/8hS1ZtgG0wU) |\n| 4/28/22 | [\u89c6\u9891\u7406\u89e3\u8bba\u6587\u4e32\u8bb2](https://arxiv.org/pdf/2012.06567.pdf)\uff08\u4e0b\uff09 | <img src=\"imgs/video-survey-p2.jpg\" width=\"200px\"/> | 1:08:32 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV11Y411P7ep)](https://www.bilibili.com/video/BV11Y411P7ep/) <br />[![](https://img.shields.io/youtube/views/J2YC0-k57NM?style=social)](https://youtu.be/J2YC0-k57NM) |\n| 4/21/22 | [\u53c2\u6570\u670d\u52a1\u5668\uff08Parameter Server\uff09](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf) \u9010\u6bb5\u7cbe\u8bfb | <img src=\"imgs/ps.jpg\" width=\"200px\"/> | 1:37:40 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1YA4y197G8)](https://www.bilibili.com/video/BV1YA4y197G8/) <br />[![](https://img.shields.io/youtube/views/xt-AwUrDxQk?style=social)](https://youtu.be/xt-AwUrDxQk) |\n| 4/14/22 | [\u89c6\u9891\u7406\u89e3\u8bba\u6587\u4e32\u8bb2](https://arxiv.org/pdf/2012.06567.pdf)\uff08\u4e0a\uff09 | <img src=\"imgs/video-survey-p1.jpg\" width=\"200px\"/> | 51:15 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1fL4y157yA)](https://www.bilibili.com/video/BV1fL4y157yA/) <br />[![](https://img.shields.io/youtube/views/gK7AGO6okhc?style=social)](https://youtu.be/gK7AGO6okhc) |\n| 3/31/22 | [I3D](https://arxiv.org/pdf/1705.07750.pdf) \u8bba\u6587\u7cbe\u8bfb | <img src=\"imgs/i3d.jpg\" width=\"200px\"/> | 52:31 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1tY4y1p7hq)](https://www.bilibili.com/video/BV1tY4y1p7hq/) <br />[![](https://img.shields.io/youtube/views/9lIkKiAn6uE?style=social)](https://youtu.be/9lIkKiAn6uE) |\n| 3/24/22 | \u65af\u5766\u798f 2022 \u5e74 [AI \u6307\u6570\u62a5\u544a](https://aiindex.stanford.edu/wp-content/uploads/2022/03/2022-AI-Index-Report_Master.pdf) \u7cbe\u8bfb | <img src=\"imgs/ai_index_22.jpg\" width=\"200px\"/> | 1:19:56 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1s44y1N7eu)](https://www.bilibili.com/video/BV1s44y1N7eu/) <br />[![](https://img.shields.io/youtube/views/K8h_xjQ6ufY?style=social)](https://youtu.be/K8h_xjQ6ufY) |\n| 3/17/22 | [AlphaCode](https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf) \u8bba\u6587\u7cbe\u8bfb | <img src=\"imgs/alphacode.jpg\" width=\"200px\"/> | 44:00 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1ab4y1s7rc)](https://www.bilibili.com/video/BV1ab4y1s7rc/) <br />[![](https://img.shields.io/youtube/views/t8Gzkca9pW4?style=social)](https://youtu.be/t8Gzkca9pW4) |\n| 3/10/22 | [OpenAI Codex](https://arxiv.org/pdf/2107.03374.pdf) \u8bba\u6587\u7cbe\u8bfb | <img src=\"imgs/codex.jpg\" width=\"200px\"/> | 47:58 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1iY41137Zi)](https://www.bilibili.com/video/BV1iY41137Zi/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1490959755963666432)](https://www.zhihu.com/zvideo/1490959755963666432)<br />[![](https://img.shields.io/youtube/views/oZriUGkQSNM?style=social)](https://youtu.be/oZriUGkQSNM) |\n| 3/3/22 | [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [GPT-3](https://arxiv.org/abs/2005.14165) \u7cbe\u8bfb | <img src=\"imgs/gpt3.jpg\" width=\"200px\"/> | 1:29:58 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1AF411b7xQ)](https://www.bilibili.com/video/BV1AF411b7xQ/)<br />[![](https://img.shields.io/youtube/views/t70Bl3w7bxY?style=social)](https://youtu.be/t70Bl3w7bxY) |\n| 2/24/22 | [Two-Stream](https://proceedings.neurips.cc/paper/2014/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Paper.pdf) \u9010\u6bb5\u7cbe\u8bfb |  <img src=\"imgs/twostream.jpg\" width=\"200px\"/> | 52:57 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1mq4y1x7RU)](https://www.bilibili.com/video/BV1mq4y1x7RU/)<br />[![](https://img.shields.io/youtube/views/vuqwKP2iDe0?style=social)](https://youtu.be/vuqwKP2iDe0) |\n| 2/10/22 | [CLIP](https://openai.com/blog/clip/) \u9010\u6bb5\u7cbe\u8bfb | <img src=\"imgs/clip.jpg\" width=\"200px\"/> | 1:38:25 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1SL4y1s7LQ)](https://www.bilibili.com/video/BV1SL4y1s7LQ/)<br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1475706654562299904)](https://www.zhihu.com/zvideo/1475706654562299904) <br />[![](https://img.shields.io/youtube/views/OZF1t_Hieq8?style=social)](https://youtu.be/OZF1t_Hieq8) |\n| 2/6/22 | \u4f60\uff08\u88ab\uff09\u5410\u69fd\u8fc7[\u8bba\u6587\u4e0d\u591f novel](https://perceiving-systems.blog/en/post/novelty-in-science) \u5417\uff1f| <img src=\"imgs/novelty.jpg\" width=\"200px\"/> | 14:11 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1ea41127Bq)](https://www.bilibili.com/video/BV1ea41127Bq/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1475719090198876161)](https://www.zhihu.com/zvideo/1475719090198876161) |\n| 1/23/22 | [AlphaFold 2](https://www.nature.com/articles/s41586-021-03819-2.pdf) \u7cbe\u8bfb | <img src=\"imgs/alphafold_2.jpg\" width=\"200px\"/> |  1:15:28 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1oR4y1K7Xr)](https://www.bilibili.com/video/BV1oR4y1K7Xr/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1469132410537717760)](https://www.zhihu.com/zvideo/1469132410537717760)  <br />[![](https://img.shields.io/youtube/views/Oy3OCoGUr-w?style=social)](https://youtu.be/Oy3OCoGUr-w) |\n| 1/18/22 | \u5982\u4f55\u5224\u65ad\uff08\u4f60\u81ea\u5df1\u7684\uff09\u7814\u7a76\u5de5\u4f5c\u7684\u4ef7\u503c | <img src=\"imgs/research_value.jpg\" width=\"200px\"/> |  9:59 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1oL411c7Us)](https://www.bilibili.com/video/BV1oL411c7Us/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1475716940051869696)](https://www.zhihu.com/zvideo/1475716940051869696) |\n| 1/15/22 | [Swin Transformer](https://arxiv.org/pdf/2103.14030.pdf) \u7cbe\u8bfb | <img src=\"imgs/swin_transformer.jpg\" width=\"200px\"/> | 1:00:21 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV13L4y1475U)](https://www.bilibili.com/video/BV13L4y1475U/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1466282983652691968)](https://www.zhihu.com/zvideo/1466282983652691968)   <br />[![](https://img.shields.io/youtube/views/luP3-Fs0QCo?style=social)](https://youtu.be/luP3-Fs0QCo) |\n| 1/7/22 | [\u6307\u5bfc\u6570\u5b66\u76f4\u89c9](https://www.nature.com/articles/s41586-021-04086-x.pdf) | <img src=\"imgs/math_conj.jpg\" width=\"200px\"/> | 52:51 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1YZ4y1S72j)](https://www.bilibili.com/video/BV1YZ4y1S72j/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1464060386375299072)](https://www.zhihu.com/zvideo/1464060386375299072)  <br />[![](https://img.shields.io/youtube/views/czFGjvhtss8?style=social)](https://youtu.be/czFGjvhtss8) |\n| 1/5/22 | AlphaFold 2 \u9884\u544a | <img src=\"imgs/alphafold_2_preview.jpg\" width=\"200px\"/> | 03:28 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Eu411U7Te)](https://www.bilibili.com/video/BV1Eu411U7Te/) |\n| 12/20/21 | [\u5bf9\u6bd4\u5b66\u4e60](#contrastive_learning)\u8bba\u6587\u7efc\u8ff0 | <img src=\"imgs/contrastive.jpg\" width=\"200px\"/> | 1:32:01 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV19S4y1M7hm)](https://www.bilibili.com/video/BV19S4y1M7hm/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1460828005077164032)](https://www.zhihu.com/zvideo/1460828005077164032)  <br />[![](https://img.shields.io/youtube/views/1pvxufGRuW4?style=social)](https://www.youtube.com/watch?v=1pvxufGRuW4) |\n| 12/15/21 | [MoCo](https://arxiv.org/pdf/1911.05722.pdf) \u9010\u6bb5\u7cbe\u8bfb | <img src=\"imgs/mocov1.jpg\" width=\"200px\"/> | 1:24:11 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1C3411s7t9)](https://www.bilibili.com/video/BV1C3411s7t9/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1454723120678936576)](https://www.zhihu.com/zvideo/1454723120678936576)   <br />[![](https://img.shields.io/youtube/views/1pvxufGRuW4?style=social)](https://www.youtube.com/watch?v=1pvxufGRuW4) |\n| 12/9/21 | \u5982\u4f55\u627e\u7814\u7a76\u60f3\u6cd5 1 | <img src=\"imgs/mae_idea.jpg\" width=\"200px\"/> | 5:34 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1qq4y1z7F2)](https://www.bilibili.com/video/BV1qq4y1z7F2/) |\n| 12/8/21 | [MAE](https://arxiv.org/pdf/2111.06377.pdf) \u9010\u6bb5\u7cbe\u8bfb | <img src=\"imgs/mae.jpg\" width=\"200px\"/> | 47:04 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1sq4y1q77t)](https://www.bilibili.com/video/BV1sq4y1q77t/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1452458167968251904)](https://www.zhihu.com/zvideo/1452458167968251904)  <br />[![](https://img.shields.io/youtube/views/mYlX2dpdHHM?style=social)](https://youtu.be/mYlX2dpdHHM) |\n| 11/29/21 | [ViT](https://arxiv.org/pdf/2010.11929.pdf) \u9010\u6bb5\u7cbe\u8bfb | <img src=\"imgs/vit.jpg\" width=\"200px\"/> | 1:11:30 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV15P4y137jb)](https://www.bilibili.com/video/BV15P4y137jb/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1449195245754380288)](https://www.zhihu.com/zvideo/1449195245754380288)  <br />[![](https://img.shields.io/youtube/views/FRFt3x0bO94?style=social)](https://youtu.be/FRFt3x0bO94) |\n| 11/18/21 | [BERT](https://arxiv.org/abs/1810.04805) \u9010\u6bb5\u7cbe\u8bfb | <img src=\"imgs/bert.jpg\" width=\"200px\"/> | 45:49  | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1PL411M7eQ)](https://www.bilibili.com/video/BV1PL411M7eQ/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1445340200976785408)](https://www.zhihu.com/zvideo/1445340200976785408)  <br />[![](https://img.shields.io/youtube/views/ULD3uIb2MHQ?style=social)](https://youtu.be/ULD3uIb2MHQ) |\n| 11/9/21 | [GAN](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf) \u9010\u6bb5\u7cbe\u8bfb | <img src=\"imgs/gan.jpg\" width=\"200px\"/> | 46:16  | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1rb4y187vD)](https://www.bilibili.com/video/BV1rb4y187vD/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1442091389241159681)](https://www.zhihu.com/zvideo/1442091389241159681)  <br />[![](https://img.shields.io/youtube/views/g_0HtlrLiDo?style=social)](https://www.youtube.com/watch?v=g_0HtlrLiDo) |\n| 11/3/21 | \u96f6\u57fa\u7840\u591a\u56fe\u8be6\u89e3 [\u56fe\u795e\u7ecf\u7f51\u7edc](https://distill.pub/2021/gnn-intro/)\uff08GNN/GCN\uff09 | <img src=\"imgs/gnn.jpg\" width=\"200px\"/> | 1:06:19 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1iT4y1d7zP)](https://www.bilibili.com/video/BV1iT4y1d7zP/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1439540657619087360)](https://www.zhihu.com/zvideo/1439540657619087360)  <br />[![](https://img.shields.io/youtube/views/sejA2PtCITw?style=social)](https://youtu.be/sejA2PtCITw) |\n| 10/27/21 | [Transformer](https://arxiv.org/abs/1706.03762) \u9010\u6bb5\u7cbe\u8bfb<br> \uff08\u89c6\u9891\u4e2d\u63d0\u5230\u7684\u6587\u732e [^transformer]) |<img src=\"imgs/transformer.jpg\" width=\"200px\"/> | 1:27:05 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1pu411o7BE)](https://www.bilibili.com/video/BV1pu411o7BE/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1437034536677404672)](https://www.zhihu.com/zvideo/1437034536677404672)  <br />[![](https://img.shields.io/youtube/views/nzqlFIcCSWQ?style=social)](https://youtu.be/nzqlFIcCSWQ) |\n| 10/22/21 | [ResNet](https://arxiv.org/abs/1512.03385) \u8bba\u6587\u9010\u6bb5\u7cbe\u8bfb | <img src=\"imgs/resnet-2.jpg\" width=\"200px\"/> | 53:46 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1P3411y7nn)](https://www.bilibili.com/video/BV1P3411y7nn/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1434795406001180672)](https://www.zhihu.com/zvideo/1434795406001180672)  <br />[![](https://img.shields.io/youtube/views/pWMnzCX4cwQ?style=social)](https://www.youtube.com/watch?v=pWMnzCX4cwQ) |\n| 10/21/21 | \u6491\u8d77\u8ba1\u7b97\u673a\u89c6\u89c9\u534a\u8fb9\u5929\u7684 [ResNet](https://arxiv.org/abs/1512.03385) | <img src=\"imgs/resnet-1.jpg\" width=\"200px\"/> | 11:50 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Fb4y1h73E)](https://www.bilibili.com/video/BV1Fb4y1h73E/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1434787226101751808)](https://www.zhihu.com/zvideo/1434787226101751808)  <br />[![](https://img.shields.io/youtube/views/NnSldWhSqvY?style=social)](https://www.youtube.com/watch?v=NnSldWhSqvY) |\n| 10/15/21 | [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) \u8bba\u6587\u9010\u6bb5\u7cbe\u8bfb | <img src=\"imgs/alexnet-2.jpg\" width=\"200px\"/> | 55:21 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1hq4y157t1)](https://www.bilibili.com/video/BV1hq4y157t1/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1432354207483871232)](https://www.zhihu.com/zvideo/1432354207483871232)  <br />[![](https://img.shields.io/youtube/views/wYmlILPsLlY?style=social)](https://www.youtube.com/watch?v=wYmlILPsLlY) |\n| 10/14/21 | 9\u5e74\u540e\u91cd\u8bfb\u6df1\u5ea6\u5b66\u4e60\u5960\u57fa\u4f5c\u4e4b\u4e00\uff1a[AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) | <img src=\"imgs/alexnet-1.jpg\" width=\"200px\"/> | 19:59 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1ih411J7Kz)](https://www.bilibili.com/video/BV1ih411J7Kz/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1432155856322920448)](https://www.zhihu.com/zvideo/1432155856322920448)  <br />[![](https://img.shields.io/youtube/views/vdYH0fE6thY?style=social)](https://www.youtube.com/watch?v=vdYH0fE6thY) |\n| 10/06/21 | \u5982\u4f55\u8bfb\u8bba\u6587 | <img src=\"imgs/read-paper.jpg\" width=\"200px\"/> | 06:39 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1H44y1t75x)](https://www.bilibili.com/video/BV1H44y1t75x/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1428973951632969728)](https://www.zhihu.com/zvideo/1428973951632969728)  <br />[![](https://img.shields.io/youtube/views/txjl_Q4jCyQ?style=social)](https://www.youtube.com/watch?v=txjl_Q4jCyQ&list=PLFXJ6jwg0qW-7UM8iUTj3qKqdhbQULP5I&index=1) |\n\n[^transformer]: 1 [\u65af\u5766\u798f100+\u4f5c\u8005\u7684200+\u9875\u7efc\u8ff0](https://arxiv.org/abs/2108.07258)\uff0c2 [\u5bf9LayerNorm\u7684\u65b0\u7814\u7a76](https://arxiv.org/pdf/1911.07013.pdf)\uff0c3 [\u5bf9Attention\u5728Transformer\u91cc\u9762\u4f5c\u7528\u7684\u7814\u7a76](https://arxiv.org/abs/2103.03404)\n\n\n## \u6240\u6709\u8bba\u6587\n\n\u5305\u62ec\u5df2\u7ecf\u5f55\u5236\u5b8c\u6210\u548c\u4e4b\u540e\u5c06\u8981\u4ecb\u7ecd\u7684\u8bba\u6587\u3002\u9009\u53d6\u7684\u539f\u5219\u662f10\u5e74\u5185\u6df1\u5ea6\u5b66\u4e60\u91cc\u6709\u5f71\u54cd\u529b\u6587\u7ae0\uff08\u5fc5\u8bfb\u6587\u7ae0\uff09\uff0c\u6216\u8005\u8fd1\u671f\u6bd4\u8f83\u6709\u610f\u601d\u7684\u6587\u7ae0\u3002\u5f53\u7136\u8fd9\u5341\u5e74\u91cc\u91cd\u8981\u7684\u5de5\u4f5c\u592a\u591a\u4e86\uff0c\u4e0d\u53ef\u80fd\u4e00\u4e00\u8fc7\u4e00\u904d\u3002\u5728\u9009\u53d6\u7684\u65f6\u5019\u6211\u4f1a\u504f\u5411\u4e00\u4e9b\u4e4b\u524d [\u76f4\u64ad\u8bfe](https://c.d2l.ai/zh-v2/) \u4e2d\u6ca1\u8bb2\u5230\u8fc7\u7684\u3002 \u6b22\u8fce\u5927\u5bb6\u5728 [\u8ba8\u8bba\u533a](https://github.com/mli/paper-reading/discussions) \u91cc\u63d0\u4f9b\u5efa\uff08\u70b9\uff09\u8bae\uff08\u6b4c\uff09\u3002\n\n\u603b\u8bba\u6587\u6570 67\uff0c\u5f55\u5236\u5b8c\u6210\u6570 32\n\n\uff08\u8fd9\u91cc\u5f15\u7528\u91c7\u7528\u7684\u662f semanticscholar\uff0c\u662f\u56e0\u4e3a\u5b83\u63d0\u4f9b [API](https://api.semanticscholar.org/api-docs/graph#operation/get_graph_get_paper) \u53ef\u4ee5\u81ea\u52a8\u83b7\u53d6\uff0c\u4e0d\u7528\u624b\u52a8\u66f4\u65b0\u3002\uff09\n\n### \u8ba1\u7b97\u673a\u89c6\u89c9 - CNN\n\n\n| \u5df2\u5f55\u5236 | \u5e74\u4efd | \u540d\u5b57                                                         | \u7b80\u4ecb                 | \u5f15\u7528 |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| \u2705      | 2012 | [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) | \u6df1\u5ea6\u5b66\u4e60\u70ed\u6f6e\u7684\u5960\u57fa\u4f5c                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fabd1c342495432171beb7ca8fd9551ef13cbd0ff%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff) |\n| | 2014 | [VGG](https://arxiv.org/pdf/1409.1556.pdf) | \u4f7f\u7528 3x3 \u5377\u79ef\u6784\u9020\u66f4\u6df1\u7684\u7f51\u7edc                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Feb42cf88027de515750f230b23b1a057dc782108%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman/eb42cf88027de515750f230b23b1a057dc782108) |\n| | 2014 | [GoogleNet](https://arxiv.org/pdf/1409.4842.pdf) | \u4f7f\u7528\u5e76\u884c\u67b6\u6784\u6784\u9020\u66f4\u6df1\u7684\u7f51\u7edc                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe15cf50aa89fee8535703b9f9512fca5bfc43327%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Going-deeper-with-convolutions-Szegedy-Liu/e15cf50aa89fee8535703b9f9512fca5bfc43327) |\n|  \u2705  | 2015 |  [ResNet](https://arxiv.org/pdf/1512.03385.pdf) | \u6784\u5efa\u6df1\u5c42\u7f51\u7edc\u90fd\u8981\u6709\u7684\u6b8b\u5dee\u8fde\u63a5\u3002               |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2c03df8b48bf3fa39054345bafabfeff15bfd11d%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Deep-Residual-Learning-for-Image-Recognition-He-Zhang/2c03df8b48bf3fa39054345bafabfeff15bfd11d)  |\n|  | 2017 | [MobileNet](https://arxiv.org/pdf/1704.04861.pdf) | \u9002\u5408\u7ec8\u7aef\u8bbe\u5907\u7684\u5c0fCNN                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3647d6d0f151dc05626449ee09cc7bce55be497e%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/MobileNets%3A-Efficient-Convolutional-Neural-Networks-Howard-Zhu/3647d6d0f151dc05626449ee09cc7bce55be497e)  |\n| | 2019 | [EfficientNet](https://arxiv.org/pdf/1905.11946.pdf) | \u901a\u8fc7\u67b6\u6784\u641c\u7d22\u5f97\u5230\u7684CNN                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/EfficientNet%3A-Rethinking-Model-Scaling-for-Neural-Tan-Le/4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9)  |\n| | 2021 |  [Non-deep networks](https://arxiv.org/pdf/2110.07641.pdf) | \u8ba9\u4e0d\u6df1\u7684\u7f51\u7edc\u4e5f\u80fd\u5728ImageNet\u5237\u5230SOTA                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0d7f6086772079bc3e243b7b375a9ca1a517ba8b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Non-deep-Networks-Goyal-Bochkovskiy/0d7f6086772079bc3e243b7b375a9ca1a517ba8b) |\n\n### \u8ba1\u7b97\u673a\u89c6\u89c9 - Transformer\n\n| \u5df2\u5f55\u5236 | \u5e74\u4efd | \u540d\u5b57                                                         | \u7b80\u4ecb                 | \u5f15\u7528 |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| \u2705 | 2020 | [ViT](https://arxiv.org/pdf/2010.11929.pdf) | Transformer\u6740\u5165CV\u754c                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7b15fa1b8d413fbe14ef7a97f651f47f5aff3903%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/An-Image-is-Worth-16x16-Words%3A-Transformers-for-at-Dosovitskiy-Beyer/7b15fa1b8d413fbe14ef7a97f651f47f5aff3903)  |\n| \u2705 | 2021 | [Swin Transformer](https://arxiv.org/pdf/2103.14030.pdf) | \u591a\u5c42\u6b21\u7684Vision Transformer                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc8b25fab5608c3e033d34b4483ec47e68ba109b7%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Swin-Transformer%3A-Hierarchical-Vision-Transformer-Liu-Lin/c8b25fab5608c3e033d34b4483ec47e68ba109b7) |\n| | 2021 | [MLP-Mixer](https://arxiv.org/pdf/2105.01601.pdf) | \u4f7f\u7528MLP\u66ff\u6362self-attention            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2def61f556f9a5576ace08911496b7c7e4f970a4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/MLP-Mixer%3A-An-all-MLP-Architecture-for-Vision-Tolstikhin-Houlsby/2def61f556f9a5576ace08911496b7c7e4f970a4)  |\n| \u2705 | 2021 | [MAE](https://arxiv.org/pdf/2111.06377.pdf) | BERT\u7684CV\u7248             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc1962a8cf364595ed2838a097e9aa7cd159d3118%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Masked-Autoencoders-Are-Scalable-Vision-Learners-He-Chen/c1962a8cf364595ed2838a097e9aa7cd159d3118)  |\n\n### \u751f\u6210\u6a21\u578b\n\n| \u5df2\u5f55\u5236 | \u5e74\u4efd | \u540d\u5b57                                              | \u7b80\u4ecb         | \u5f15\u7528 |\n| ------ | ---- | ------------------------------------------------- | ------------ | ------------------------------------------------------------ |\n|  \u2705 | 2014 | [GAN](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf) | \u751f\u6210\u6a21\u578b\u7684\u5f00\u521b\u5de5\u4f5c                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F54e325aee6b2d476bbbb88615ac15e251c6e8214%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Generative-Adversarial-Nets-Goodfellow-Pouget-Abadie/54e325aee6b2d476bbbb88615ac15e251c6e8214)  |\n|  | 2015 | [DCGAN](https://arxiv.org/pdf/1511.06434.pdf) | \u4f7f\u7528CNN\u7684GAN          |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8388f1be26329fa45e5807e968a641ce170ea078%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Unsupervised-Representation-Learning-with-Deep-Radford-Metz/8388f1be26329fa45e5807e968a641ce170ea078)  |\n|  | 2016 | [pix2pix](https://arxiv.org/pdf/1611.07004.pdf) |           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8acbe90d5b852dadea7810345451a99608ee54c7%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Image-to-Image-Translation-with-Conditional-Isola-Zhu/8acbe90d5b852dadea7810345451a99608ee54c7)  |\n|  | 2016 | [SRGAN](https://arxiv.org/pdf/1609.04802.pdf) | \u56fe\u7247\u8d85\u5206\u8fa8\u7387          |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdf0c54fe61f0ffb9f0e36a17c2038d9a1964cba3%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Photo-Realistic-Single-Image-Super-Resolution-Using-Ledig-Theis/df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3)  |\n|  | 2017 | [WGAN](https://arxiv.org/abs/1701.07875) | \u8bad\u7ec3\u66f4\u52a0\u5bb9\u6613          |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2f85b7376769473d2bed56f855f115e23d727094%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Wasserstein-GAN-Arjovsky-Chintala/2f85b7376769473d2bed56f855f115e23d727094)  |\n|  | 2017 | [CycleGAN](https://arxiv.org/abs/1703.10593) |           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc43d954cf8133e6254499f3d68e45218067e4941%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Unpaired-Image-to-Image-Translation-Using-Networks-Zhu-Park/c43d954cf8133e6254499f3d68e45218067e4941)  |\n|  | 2018 | [StyleGAN](https://arxiv.org/abs/1812.04948) |           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fceb2ebef0b41e31c1a21b28c2734123900c005e2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/A-Style-Based-Generator-Architecture-for-Generative-Karras-Laine/ceb2ebef0b41e31c1a21b28c2734123900c005e2)  |\n| | 2019 | [StyleGAN2](https://arxiv.org/pdf/1912.04958.pdf) |        |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff3e3d1f86a534a3654d0ee263142e44f4e2c61e9%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Analyzing-and-Improving-the-Image-Quality-of-Karras-Laine/f3e3d1f86a534a3654d0ee263142e44f4e2c61e9)  |\n| | 2020 | [DDPM](https://arxiv.org/pdf/2006.11239.pdf) | Diffusion Models   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F289db3be7bf77e06e75541ba93269de3d604ac72%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Denoising-Diffusion-Probabilistic-Models-Ho-Jain/289db3be7bf77e06e75541ba93269de3d604ac72)  |\n| | 2021 | [Improved DDPM](https://arxiv.org/pdf/2102.09672.pdf) | \u6539\u8fdb\u7684 DDPM   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fde18baa4964804cf471d85a5a090498242d2e79f%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Improved-Denoising-Diffusion-Probabilistic-Models-Nichol-Dhariwal/de18baa4964804cf471d85a5a090498242d2e79f)  |\n| | 2021 | [Guided Diffusion Models](https://arxiv.org/pdf/2105.05233.pdf) | \u53f7\u79f0\u8d85\u8d8a GAN  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F64ea8f180d0682e6c18d1eb688afdb2027c02794%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Diffusion-Models-Beat-GANs-on-Image-Synthesis-Dhariwal-Nichol/64ea8f180d0682e6c18d1eb688afdb2027c02794)  |\n| | 2021 | [StyleGAN3](https://arxiv.org/pdf/2106.12423.pdf) |        |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc1ff08b59f00c44f34dfdde55cd53370733a2c19%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Alias-Free-Generative-Adversarial-Networks-Karras-Aittala/c1ff08b59f00c44f34dfdde55cd53370733a2c19)  |\n|  \u2705  | 2022 | [DALL.E 2](https://arxiv.org/pdf/2204.06125.pdf) | CLIP + Diffusion models\uff0c\u6587\u672c\u751f\u6210\u56fe\u50cf\u65b0\u9ad8\u5ea6     |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc57293882b2561e1ba03017902df9fc2f289dea2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Hierarchical-Text-Conditional-Image-Generation-with-Ramesh-Dhariwal/c57293882b2561e1ba03017902df9fc2f289dea2)  |\n\n### \u8ba1\u7b97\u673a\u89c6\u89c9 - Object Detection\n\n| \u5df2\u5f55\u5236 | \u5e74\u4efd | \u540d\u5b57                                              | \u7b80\u4ecb         | \u5f15\u7528 |\n| ------ | ---- | ------------------------------------------------- | ------------ | ------------------------------------------------------------ |\n|        | 2014 | [R-CNN](https://arxiv.org/pdf/1311.2524v5.pdf)    | Two-stage             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2f4df08d9072fc2ac181b7fced6a245315ce05c8%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/2f4df08d9072fc2ac181b7fced6a245315ce05c8)  |\n|        | 2015 | [Fast R-CNN](http://arxiv.org/abs/1504.08083v2)   |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7ffdbc358b63378f07311e883dddacc9faeeaf4b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/7ffdbc358b63378f07311e883dddacc9faeeaf4b)  |\n|        | 2015 | [Faster R-CNN](http://arxiv.org/abs/1506.01497v3) |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F424561d8585ff8ebce7d5d07de8dbf7aae5e7270%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/424561d8585ff8ebce7d5d07de8dbf7aae5e7270)  |\n|        | 2016 | [SSD](http://arxiv.org/abs/1512.02325v5)          | Single stage          |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0)  |\n|        | 2016 | [YOLO](http://arxiv.org/abs/1506.02640v5)         |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff8e79ac0ea341056ef20f2616628b3e964764cfd%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/f8e79ac0ea341056ef20f2616628b3e964764cfd)  |\n|        | 2017 | [Mask R-CNN](http://arxiv.org/abs/1703.06870v3)   |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fea99a5535388196d0d44be5b4d7dd02029a43bb2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ea99a5535388196d0d44be5b4d7dd02029a43bb2)  |\n|        | 2017 | [YOLOv2](http://arxiv.org/abs/1612.08242v1)       |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7d39d69b23424446f0400ef603b2e3e22d0309d6%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/7d39d69b23424446f0400ef603b2e3e22d0309d6)  |\n|        | 2018 | [YOLOv3](http://arxiv.org/abs/1804.02767v1)       |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe4845fb1e624965d4f036d7fd32e8dcdd2408148%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/e4845fb1e624965d4f036d7fd32e8dcdd2408148)  |\n|        | 2019 | [CenterNet](https://arxiv.org/pdf/1904.07850.pdf) | Anchor free           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6a2e2fd1b5bb11224daef98b3fb6d029f68a73f2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Objects-as-Points-Zhou-Wang/6a2e2fd1b5bb11224daef98b3fb6d029f68a73f2)  |\n|   \u2705     | 2020 | [DETR](https://arxiv.org/pdf/2005.12872.pdf)      | Transformer           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F962dc29fdc3fbdc5930a10aba114050b82fe5a3e%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/End-to-End-Object-Detection-with-Transformers-Carion-Massa/962dc29fdc3fbdc5930a10aba114050b82fe5a3e)  |\n\n<a name=\"contrastive_learning\"></a>\n\n### \u8ba1\u7b97\u673a\u89c6\u89c9 - \u5bf9\u6bd4\u5b66\u4e60\n\n\n| \u5df2\u5f55\u5236 | \u5e74\u4efd | \u540d\u5b57                                                         | \u7b80\u4ecb                 | \u5f15\u7528 |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| \u2705      | 2018 | [InstDisc](https://arxiv.org/pdf/1805.01978.pdf) | \u63d0\u51fa\u5b9e\u4f8b\u5224\u522b\u548cmemory bank\u505a\u5bf9\u6bd4\u5b66\u4e60                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F155b7782dbd713982a4133df3aee7adfd0b6b304%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Unsupervised-Feature-Learning-via-Non-parametric-Wu-Xiong/155b7782dbd713982a4133df3aee7adfd0b6b304)  |\n| \u2705      | 2018 | [CPC](https://arxiv.org/pdf/1807.03748.pdf) | \u5bf9\u6bd4\u9884\u6d4b\u7f16\u7801\uff0c\u56fe\u50cf\u8bed\u97f3\u6587\u672c\u5f3a\u5316\u5b66\u4e60\u5168\u90fd\u80fd\u505a                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb227f3e4c0dc96e5ac5426b85485a70f2175a205%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Representation-Learning-with-Contrastive-Predictive-Oord-Li/b227f3e4c0dc96e5ac5426b85485a70f2175a205) |\n| \u2705      | 2019 | [InvaSpread](https://arxiv.org/pdf/1904.03436.pdf) | \u4e00\u4e2a\u7f16\u7801\u5668\u7684\u7aef\u5230\u7aef\u5bf9\u6bd4\u5b66\u4e60                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Unsupervised-Embedding-Learning-via-Invariant-and-Ye-Zhang/e4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b)  |\n| \u2705  | 2019 |  [CMC](https://arxiv.org/pdf/1906.05849.pdf) | \u591a\u89c6\u89d2\u4e0b\u7684\u5bf9\u6bd4\u5b66\u4e60               |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F97f4d09175705be4677d675fa27e55defac44800%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Contrastive-Multiview-Coding-Tian-Krishnan/97f4d09175705be4677d675fa27e55defac44800)  |\n| \u2705 | 2019 | [MoCov1](https://arxiv.org/pdf/1911.05722.pdf) | \u65e0\u76d1\u7763\u8bad\u7ec3\u6548\u679c\u4e5f\u5f88\u597d                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fec46830a4b275fd01d4de82bffcabe6da086128f%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Momentum-Contrast-for-Unsupervised-Visual-Learning-He-Fan/ec46830a4b275fd01d4de82bffcabe6da086128f) |\n|  \u2705 | 2020 |  [SimCLRv1](https://arxiv.org/pdf/2002.05709.pdf) |  \u7b80\u5355\u7684\u5bf9\u6bd4\u5b66\u4e60 (\u6570\u636e\u589e\u5f3a + MLP head + \u5927batch\u8bad\u7ec3\u4e45)                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F34733eaf66007516347a40ad5d9bbe1cc9dacb6b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/A-Simple-Framework-for-Contrastive-Learning-of-Chen-Kornblith/34733eaf66007516347a40ad5d9bbe1cc9dacb6b)  |\n|  \u2705 | 2020 | [MoCov2](https://arxiv.org/pdf/2003.04297.pdf) | MoCov1 + improvements from SimCLRv1                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fa1b8a8df281bbaec148a897927a49ea47ea31515%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Improved-Baselines-with-Momentum-Contrastive-Chen-Fan/a1b8a8df281bbaec148a897927a49ea47ea31515)  |\n|  \u2705 | 2020 |  [SimCLRv2](https://arxiv.org/pdf/2006.10029.pdf) | \u5927\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u578b\u5f88\u9002\u5408\u505a\u534a\u76d1\u7763\u5b66\u4e60                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3e7f5f4382ac6f9c4fef6197dd21abf74456acd1%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Big-Self-Supervised-Models-are-Strong-Learners-Chen-Kornblith/3e7f5f4382ac6f9c4fef6197dd21abf74456acd1)  |\n| \u2705  | 2020 |  [BYOL](https://arxiv.org/pdf/2006.07733.pdf) | \u4e0d\u9700\u8981\u8d1f\u6837\u672c\u7684\u5bf9\u6bd4\u5b66\u4e60                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F38f93092ece8eee9771e61c1edaf11b1293cae1b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Bootstrap-Your-Own-Latent%3A-A-New-Approach-to-Grill-Strub/38f93092ece8eee9771e61c1edaf11b1293cae1b) |\n|  \u2705 | 2020 |  [SWaV](https://arxiv.org/pdf/2006.09882.pdf) | \u805a\u7c7b\u5bf9\u6bd4\u5b66\u4e60                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F10161d83d29fc968c4612c9e9e2b61a2fc25842e%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Unsupervised-Learning-of-Visual-Features-by-Cluster-Caron-Misra/10161d83d29fc968c4612c9e9e2b61a2fc25842e) |\n|  \u2705 | 2020 |  [SimSiam](https://arxiv.org/pdf/2011.10566.pdf) | \u5316\u7e41\u4e3a\u7b80\u7684\u5b6a\u751f\u8868\u5f81\u5b66\u4e60                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Exploring-Simple-Siamese-Representation-Learning-Chen-He/0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d)  |\n| \u2705 | 2021 | [MoCov3](https://arxiv.org/pdf/2104.02057.pdf) | \u5982\u4f55\u66f4\u7a33\u5b9a\u7684\u81ea\u76d1\u7763\u8bad\u7ec3ViT                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F739ceacfafb1c4eaa17509351b647c773270b3ae%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/An-Empirical-Study-of-Training-Self-Supervised-Chen-Xie/739ceacfafb1c4eaa17509351b647c773270b3ae)  |\n|  \u2705 | 2021 |  [DINO](https://arxiv.org/pdf/2104.14294.pdf) | transformer\u52a0\u81ea\u76d1\u7763\u5728\u89c6\u89c9\u4e5f\u5f88\u9999                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fad4a0938c48e61b7827869e4ac3baffd0aefab35%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Emerging-Properties-in-Self-Supervised-Vision-Caron-Touvron/ad4a0938c48e61b7827869e4ac3baffd0aefab35)  |\n\n\n### \u8ba1\u7b97\u673a\u89c6\u89c9 - \u89c6\u9891\u7406\u89e3\n\n| \u5df2\u5f55\u5236 | \u5e74\u4efd | \u540d\u5b57                                                         | \u7b80\u4ecb                 | \u5f15\u7528 |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| \u2705 | 2014 |  [DeepVideo](https://cs.stanford.edu/people/karpathy/deepvideo/) | \u63d0\u51fasports1M\u6570\u636e\u96c6\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\u505a\u89c6\u9891\u7406\u89e3 |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6d4c9c923e9f145d1c01a2de2afc38ec23c44253%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Large-Scale-Video-Classification-with-Convolutional-Karpathy-Toderici/6d4c9c923e9f145d1c01a2de2afc38ec23c44253)  |\n| \u2705 | 2014 |  [Two-stream](https://arxiv.org/pdf/1406.2199.pdf) | \u5f15\u5165\u5149\u6d41\u505a\u65f6\u5e8f\u5efa\u6a21\uff0c\u795e\u7ecf\u7f51\u7edc\u9996\u6b21\u8d85\u8d8a\u624b\u5de5\u7279\u5f81 |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F67dccc9a856b60bdc4d058d83657a089b8ad4486%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Two-Stream-Convolutional-Networks-for-Action-in-Simonyan-Zisserman/67dccc9a856b60bdc4d058d83657a089b8ad4486)  |\n| \u2705 | 2014 |  [C3D](https://arxiv.org/pdf/1412.0767.pdf) |  \u6bd4\u8f83\u6df1\u76843D-CNN\u505a\u89c6\u9891\u7406\u89e3 |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fd25c65d261ea0e6a458be4c50c40ffe5bc508f77%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Learning-Spatiotemporal-Features-with-3D-Networks-Tran-Bourdev/d25c65d261ea0e6a458be4c50c40ffe5bc508f77)  |\n| \u2705 | 2015 |  [Beyond-short-snippets](https://arxiv.org/pdf/1503.08909.pdf) | \u5c1d\u8bd5\u4f7f\u7528LSTM  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5418b2a482720e013d487a385c26fae0f017c6a6%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Beyond-short-snippets%3A-Deep-networks-for-video-Ng-Hausknecht/5418b2a482720e013d487a385c26fae0f017c6a6)  |\n| \u2705 | 2016 |  [Convolutional fusion](https://arxiv.org/pdf/1604.06573.pdf) | \u505aearly fusion\u6765\u52a0\u5f3a\u65f6\u7a7a\u95f4\u5efa\u6a21    |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9d9aced120e530484609164c836da64548693484%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Convolutional-Two-Stream-Network-Fusion-for-Video-Feichtenhofer-Pinz/9d9aced120e530484609164c836da64548693484)  |\n| \u2705 | 2016 |  [TSN](https://arxiv.org/pdf/1608.00859.pdf) | \u8d85\u7ea7\u6709\u6548\u7684\u89c6\u9891\u5206\u6bb5\u5efa\u6a21\uff0cbag of tricks in video |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fea3d7de6c0880e14455b9acb28f1bc1234321456%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Temporal-Segment-Networks%3A-Towards-Good-Practices-Wang-Xiong/ea3d7de6c0880e14455b9acb28f1bc1234321456)  |\n| \u2705 | 2017 |  [I3D](https://arxiv.org/pdf/1705.07750.pdf) | \u63d0\u51faKinetics\u6570\u636e\u96c6\uff0c\u81a8\u80c02D\u7f51\u7edc\u52303D\uff0c\u5f00\u542f3D-CNN\u65f6\u4ee3  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb61a3f8b80bbd44f24544dc915f52fd30bbdf485%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Quo-Vadis%2C-Action-Recognition-A-New-Model-and-the-Carreira-Zisserman/b61a3f8b80bbd44f24544dc915f52fd30bbdf485)  |\n| \u2705 | 2017 |  [R2+1D](https://arxiv.org/pdf/1711.11248.pdf) | \u62c6\u52063D\u5377\u79ef\u6838\uff0c\u4f7f3D\u7f51\u7edc\u5bb9\u6613\u4f18\u5316  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F89c3050522a0bb9820c32dc7444e003ef0d3e2e4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/A-Closer-Look-at-Spatiotemporal-Convolutions-for-Tran-Wang/89c3050522a0bb9820c32dc7444e003ef0d3e2e4)  |\n| \u2705 | 2017 |  [Non-local](https://arxiv.org/pdf/1711.07971.pdf) | \u5f15\u5165\u81ea\u6ce8\u610f\u529b\u505a\u89c6\u89c9\u95ee\u9898  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8899094797e82c5c185a0893896320ef77f60e64%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Non-local-Neural-Networks-Wang-Girshick/8899094797e82c5c185a0893896320ef77f60e64)  |\n| \u2705 | 2018 |  [SlowFast](https://arxiv.org/pdf/1812.03982.pdf) | \u5feb\u6162\u4e24\u652f\u63d0\u5347\u6548\u7387   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8b47b9c3c35b2b2a78bff7822605b3040f87d699%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/SlowFast-Networks-for-Video-Recognition-Feichtenhofer-Fan/8b47b9c3c35b2b2a78bff7822605b3040f87d699)  |\n| \u2705 | 2021 |  [TimeSformer](https://arxiv.org/pdf/2102.05095.pdf) | \u89c6\u9891\u4e2d\u7b2c\u4e00\u4e2a\u5f15\u5165transformer\uff0c\u5f00\u542fvideo transformer\u65f6\u4ee3 |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc143ea9e30b1f2d93a9c060253845423f9e60e1f%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Is-Space-Time-Attention-All-You-Need-for-Video-Bertasius-Wang/c143ea9e30b1f2d93a9c060253845423f9e60e1f)  |\n\n\n### \u591a\u6a21\u6001\u5b66\u4e60\n\n\n| \u5df2\u5f55\u5236 | \u5e74\u4efd | \u540d\u5b57                                                         | \u7b80\u4ecb                 | \u5f15\u7528 |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| \u2705 | 2021 |  [CLIP](https://openai.com/blog/clip/) | \u56fe\u7247\u548c\u6587\u672c\u4e4b\u95f4\u7684\u5bf9\u6bd4\u5b66\u4e60                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Learning-Transferable-Visual-Models-From-Natural-Radford-Kim/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4)  |\n| \u2705 | 2021 |  [ViLT](https://arxiv.org/pdf/2102.03334.pdf) | \u7b2c\u4e00\u4e2a\u6446\u8131\u4e86\u76ee\u6807\u68c0\u6d4b\u7684\u89c6\u89c9\u6587\u672c\u6a21\u578b      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0839722fb5369c0abaff8515bfc08299efc790a1%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ViLT%3A-Vision-and-Language-Transformer-Without-or-Kim-Son/0839722fb5369c0abaff8515bfc08299efc790a1)  |\n| \u2705 | 2021 |  [ViLD](https://arxiv.org/pdf/2104.13921.pdf) | CLIP\u84b8\u998f\u5e2e\u52a9\u5f00\u96c6\u76ee\u6807\u68c0\u6d4b      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcf9b8da26d9b92e75ba49616ed2a1033f59fce14%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Open-vocabulary-Object-Detection-via-Vision-and-Gu-Lin/cf9b8da26d9b92e75ba49616ed2a1033f59fce14)  |\n| \u2705 | 2021 |  [GLIP](https://arxiv.org/pdf/2112.03857.pdf) | \u8054\u5408\u76ee\u6807\u68c0\u6d4b\u548c\u6587\u672c\u5b9a\u4f4d           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5341b412383c43f4a693ad63ec4489e3ec7688c8%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Grounded-Language-Image-Pre-training-Li-Zhang/5341b412383c43f4a693ad63ec4489e3ec7688c8)  |\n| \u2705 | 2021 |  [CLIP4Clip](https://arxiv.org/pdf/2104.08860.pdf) | \u62ffCLIP\u76f4\u63a5\u505a\u89c6\u9891\u6587\u672cretrieval       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F281ad83e06d731d5d686acf07cd701576f1188c4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/CLIP4Clip%3A-An-Empirical-Study-of-CLIP-for-End-to-Luo-Ji/281ad83e06d731d5d686acf07cd701576f1188c4)  |\n| \u2705 | 2021 |  [ActionCLIP](https://arxiv.org/pdf/2109.08472.pdf) | \u7528\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6709\u76d1\u7763\u7684\u505a\u89c6\u9891\u52a8\u4f5c\u5206\u7c7b   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdc05240a06326b5b1664f7e8c95c330b08cd0349%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ActionCLIP%3A-A-New-Paradigm-for-Video-Action-Wang-Xing/dc05240a06326b5b1664f7e8c95c330b08cd0349)  |\n| \u2705 | 2021 |  [PointCLIP](https://arxiv.org/pdf/2112.02413.pdf) | 3D\u53d82D\uff0c\u5de7\u5999\u5229\u7528CLIP\u505a\u70b9\u4e91  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff3ce9ba3fcec362b70263a7ed63d9404975496a0%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/PointCLIP%3A-Point-Cloud-Understanding-by-CLIP-Zhang-Guo/f3ce9ba3fcec362b70263a7ed63d9404975496a0)  |\n| \u2705 | 2022 |  [LSeg](https://arxiv.org/pdf/2201.03546.pdf) | \u6709\u76d1\u7763\u7684\u5f00\u96c6\u5206\u5272                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcc9826c222ac1e81b4b374dd9e0df130f298b1e8%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Language-driven-Semantic-Segmentation-Li-Weinberger/cc9826c222ac1e81b4b374dd9e0df130f298b1e8)  |\n| \u2705 | 2022 |  [GroupViT](https://arxiv.org/pdf/2202.11094.pdf) | \u53ea\u7528\u56fe\u50cf\u6587\u672c\u5bf9\u4e5f\u80fd\u65e0\u76d1\u7763\u505a\u5206\u5272        |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0b5f27a5766c5d1394a6282ad94fec21d620bd6b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/GroupViT%3A-Semantic-Segmentation-Emerges-from-Text-Xu-Mello/0b5f27a5766c5d1394a6282ad94fec21d620bd6b)  |\n| \u2705 | 2022 |  [CLIPasso](https://arxiv.org/pdf/2202.05822.pdf) | CLIP\u8de8\u754c\u751f\u6210\u7b80\u7b14\u753b   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9dec819778bebae4a468c7813f7638534c826f52%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/CLIPasso%3A-Semantically-Aware-Object-Sketching-Vinker-Pajouheshgar/9dec819778bebae4a468c7813f7638534c826f52)  |\n| \u2705 | 2022 |  [DepthCLIP](https://arxiv.org/pdf/2207.01077.pdf) | \u7528\u6587\u672c\u8de8\u754c\u4f30\u8ba1\u6df1\u5ea6   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9d0afe58801fe9e5537902e853d6e9e385340a92%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Can-Language-Understand-Depth-Zhang-Zeng/9d0afe58801fe9e5537902e853d6e9e385340a92)  |\n\n\n\n### \u81ea\u7136\u8bed\u8a00\u5904\u7406 - Transformer\n\n| \u5df2\u5f55\u5236 | \u5e74\u4efd | \u540d\u5b57                                                         | \u7b80\u4ecb                 | \u5f15\u7528 |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| \u2705 | 2017 | [Transformer](https://arxiv.org/abs/1706.03762) | \u7ee7MLP\u3001CNN\u3001RNN\u540e\u7684\u7b2c\u56db\u5927\u7c7b\u67b6\u6784                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F204e3073870fae3d05bcbc2f6a8e263d9b72e776%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776)  |\n| \u2705 | 2018 | [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) | \u4f7f\u7528 Transformer \u89e3\u7801\u5668\u6765\u505a\u9884\u8bad\u7ec3               |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcd18800a0fe0b668a1cc19f2ec95b5003d0a5035%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)  |\n| \u2705 | 2018 | [BERT](https://arxiv.org/abs/1810.04805) | Transformer\u4e00\u7edfNLP\u7684\u5f00\u59cb                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdf2b0e26d0599ce3e70df8a9da02e51594e0e992%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992)  |\n| \u2705 | 2019 | [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  |  \u66f4\u5927\u7684 GPT \u6a21\u578b\uff0c\u671d\u7740zero-shot learning\u8fc8\u4e86\u4e00\u5927\u6b65             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9405cc0d6169988371b2755e573cc28650d14dfe%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)  |\n| \u2705 | 2020 |  [GPT-3](https://arxiv.org/abs/2005.14165) | 100\u500d\u66f4\u5927\u7684 GPT-2\uff0cfew-shot learning\u6548\u679c\u663e\u8457                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6b85b63579a916f705a8e10a49bd8d849d91b1fc%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Language-Models-are-Few-Shot-Learners-Brown-Mann/6b85b63579a916f705a8e10a49bd8d849d91b1fc)  |\n| \u2705 | 2024 |  [Llama 3.1](https://arxiv.org/pdf/2407.21783) | \u5f3a\u5927\u7684Meta\u5f00\u6e90\u6a21\u578b - \u52a8\u6001\u6269\u5c55\uff0c\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u96f6\u6837\u672c\u5b66\u4e60\uff0c\u9ad8\u6548\u8ba1\u7b97                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F4176a4cecfaef26b2c503827493867e703f3411a%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/4176a4cecfaef26b2c503827493867e703f3411a)  |\n\n### \u7cfb\u7edf\n\n| \u5df2\u5f55\u5236 | \u5e74\u4efd | \u540d\u5b57                                                         | \u7b80\u4ecb                 | \u5f15\u7528 |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n|  \u2705 |  2014 | [\u53c2\u6570\u670d\u52a1\u5668](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf) | \u652f\u6301\u5343\u4ebf\u53c2\u6570\u7684\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0de0c3240bda7972bd0a3c8369ebc4b4f2e4f9c2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Scaling-Distributed-Machine-Learning-with-the-Li-Andersen/0de0c3240bda7972bd0a3c8369ebc4b4f2e4f9c2)  |\n| \u2705  | 2018 | [GPipe](https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf) | \u6d41\u6c34\u7ebf\uff08Pipeline\uff09\u5e76\u884c      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc18663fea10c8a303d045fd2c1f33cacf9b73ca3%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/GPipe%3A-Efficient-Training-of-Giant-Neural-Networks-Huang-Cheng/c18663fea10c8a303d045fd2c1f33cacf9b73ca3)  |\n| \u2705 | 2019 | [Megatron-LM](https://arxiv.org/pdf/1909.08053.pdf) | \u5f20\u91cf\uff08Tensor\uff09\u5e76\u884c      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8323c591e119eb09b28b29fd6c7bc76bd889df7a%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/8323c591e119eb09b28b29fd6c7bc76bd889df7a) |\n| \u2705 | 2019 | [Zero](https://arxiv.org/pdf/1910.02054.pdf) | \u53c2\u6570\u5206\u7247      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F00c957711b12468cb38424caccdf5291bb354033%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ZeRO%3A-Memory-optimizations-Toward-Training-Trillion-Rajbhandari-Rasley/00c957711b12468cb38424caccdf5291bb354033)  |\n| \u2705 |  2022 | [Pathways](https://arxiv.org/pdf/2203.12533.pdf) |  \u5c06Jax\u62d3\u5c55\u5230\u4e0a\u5343TPU\u6838\u4e0a       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F512e9aa873a1cd7eb61ce1f25ca7df6acb7e2352%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Pathways%3A-Asynchronous-Distributed-Dataflow-for-ML-Barham-Chowdhery/512e9aa873a1cd7eb61ce1f25ca7df6acb7e2352)  |\n\n### \u56fe\u795e\u7ecf\u7f51\u7edc\n\n| \u5df2\u5f55\u5236 | \u5e74\u4efd | \u540d\u5b57                                                         | \u7b80\u4ecb                 | \u5f15\u7528 |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n|  \u2705 |  2021 | [\u56fe\u795e\u7ecf\u7f51\u7edc\u4ecb\u7ecd](https://distill.pub/2021/gnn-intro/) | GNN\u7684\u53ef\u89c6\u5316\u4ecb\u7ecd                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2c0e0440882a42be752268d0b64243243d752a74%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/A-Gentle-Introduction-to-Graph-Neural-Networks-S%C3%A1nchez-Lengeling-Reif/2c0e0440882a42be752268d0b64243243d752a74)  |\n\n### \u4f18\u5316\u7b97\u6cd5\n\n| \u5df2\u5f55\u5236 | \u5e74\u4efd | \u540d\u5b57                                                         | \u7b80\u4ecb                 | \u5f15\u7528 |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| | 2014 | [Adam](https://arxiv.org/abs/1412.6980) | \u6df1\u5ea6\u5b66\u4e60\u91cc\u6700\u5e38\u7528\u7684\u4f18\u5316\u7b97\u6cd5\u4e4b\u4e00                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fa6cb366736791bcccc5c8639de5a8f9636bf87e8%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Adam%3A-A-Method-for-Stochastic-Optimization-Kingma-Ba/a6cb366736791bcccc5c8639de5a8f9636bf87e8)  |\n| | 2016 |  [\u4e3a\u4ec0\u4e48\u8d85\u5927\u7684\u6a21\u578b\u6cdb\u5316\u6027\u4e0d\u9519](https://arxiv.org/abs/1611.03530)   |               |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F54ddb00fa691728944fd8becea90a373d21597cf%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Understanding-deep-learning-requires-rethinking-Zhang-Bengio/54ddb00fa691728944fd8becea90a373d21597cf)  |\n| | 2017 | [\u4e3a\u4ec0\u4e48Momentum\u6709\u6548](https://distill.pub/2017/momentum/) | Distill\u7684\u53ef\u89c6\u5316\u4ecb\u7ecd            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3e8ccf9d3d843c9855c5d76ab66d3e775384da72%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Why-Momentum-Really-Works-Goh/3e8ccf9d3d843c9855c5d76ab66d3e775384da72)  |\n\n\n### \u65b0\u9886\u57df\u5e94\u7528\n\n\n| \u5df2\u5f55\u5236 | \u5e74\u4efd | \u540d\u5b57                                                         | \u7b80\u4ecb                 | \u5f15\u7528 |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| | 2016 | [AlphaGo](https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf) | \u5f3a\u5316\u5b66\u4e60\u51fa\u5708                 |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F846aedd869a00c09b40f1f1f35673cb22bc87490%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Mastering-the-game-of-Go-with-deep-neural-networks-Silver-Huang/846aedd869a00c09b40f1f1f35673cb22bc87490)  |\n| | 2020 | [AlphaFold](https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf) | \u8d62\u5f97\u6bd4\u8d5b\u7684\u7684\u86cb\u767d\u8d283D\u7ed3\u6784\u9884\u6d4b |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3a083d843f891b3574494c385699c21766ce8b7a%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Improved-protein-structure-prediction-using-from-Senior-Evans/3a083d843f891b3574494c385699c21766ce8b7a)  |\n| \u2705 | 2021 | [AlphaFold 2](https://www.nature.com/articles/s41586-021-03819-2.pdf) | \u539f\u5b50\u7ea7\u522b\u7cbe\u5ea6\u7684\u86cb\u767d\u8d283D\u7ed3\u6784\u9884\u6d4b       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdc32a984b651256a8ec282be52310e6bd33d9815%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Highly-accurate-protein-structure-prediction-with-Jumper-Evans/dc32a984b651256a8ec282be52310e6bd33d9815)  |\n| \u2705 | 2021 | [Codex](https://arxiv.org/pdf/2107.03374.pdf) | \u4f7f\u7528\u6ce8\u91ca\u751f\u6210\u4ee3\u7801       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Facbdbf49f9bc3f151b93d9ca9a06009f4f6eb269%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Evaluating-Large-Language-Models-Trained-on-Code-Chen-Tworek/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269)  |\n| \u2705 | 2021 | [\u6307\u5bfc\u6570\u5b66\u76f4\u89c9](https://www.nature.com/articles/s41586-021-04086-x.pdf) | \u5206\u6790\u4e0d\u540c\u6570\u5b66\u7269\u4f53\u4e4b\u524d\u7684\u8054\u7cfb\u6765\u5e2e\u52a9\u53d1\u73b0\u65b0\u5b9a\u7406         |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff672b8fb430606fee0bb368f16603531ce1e90c4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Advancing-mathematics-by-guiding-human-intuition-AI-Davies-Velickovic/f672b8fb430606fee0bb368f16603531ce1e90c4)  |\n| \u2705 | 2022 | [AlphaCode](https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf) | \u5ab2\u7f8e\u4e00\u822c\u7a0b\u5e8f\u5458\u7684\u7f16\u7a0b\u89e3\u9898\u6c34\u5e73       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5cbe278b65a81602a864184bbca37de91448a5f5%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Competition-Level-Code-Generation-with-AlphaCode-Li-Choi/5cbe278b65a81602a864184bbca37de91448a5f5)  |\n\n",
        "releases": []
    }
}