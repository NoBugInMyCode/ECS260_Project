{
    "https://api.github.com/repos/ceph/ceph": {
        "forks": 6056,
        "watchers": 14459,
        "stars": 14459,
        "languages": {
            "C++": 50326828,
            "Python": 12336477,
            "Raku": 3800803,
            "C": 3470858,
            "TypeScript": 3408459,
            "Shell": 2349956,
            "HTML": 881976,
            "CMake": 874020,
            "Terra": 773718,
            "Cython": 630103,
            "Jsonnet": 382417,
            "JavaScript": 141459,
            "Java": 109488,
            "Gherkin": 97105,
            "Perl": 69050,
            "Assembly": 53250,
            "SCSS": 50600,
            "Jinja": 37110,
            "Roff": 26436,
            "PowerShell": 10012,
            "Dockerfile": 9380,
            "Makefile": 4786,
            "Awk": 3196,
            "CSS": 2086,
            "DIGITAL Command Language": 2074,
            "Lua": 1304,
            "SWIG": 951,
            "Turing": 941,
            "Smarty": 173
        },
        "commits": [
            "2025-01-22T00:53:51Z",
            "2025-01-21T20:09:11Z",
            "2025-01-21T18:56:39Z",
            "2025-01-21T18:54:52Z",
            "2025-01-21T18:52:41Z",
            "2025-01-21T18:37:30Z",
            "2025-01-21T17:33:07Z",
            "2025-01-21T16:49:48Z",
            "2025-01-21T15:45:08Z",
            "2025-01-14T15:42:48Z",
            "2025-01-21T05:26:08Z",
            "2025-01-20T14:14:22Z",
            "2025-01-20T12:38:28Z",
            "2025-01-20T07:19:02Z",
            "2025-01-18T04:04:14Z",
            "2025-01-19T15:59:25Z",
            "2025-01-19T12:49:52Z",
            "2025-01-19T07:31:28Z",
            "2025-01-19T00:23:36Z",
            "2025-01-17T12:33:49Z",
            "2025-01-18T03:43:18Z",
            "2025-01-18T03:27:02Z",
            "2025-01-17T22:57:10Z",
            "2025-01-17T23:56:48Z",
            "2025-01-17T22:22:55Z",
            "2024-12-06T17:51:25Z",
            "2024-12-05T23:01:22Z",
            "2024-12-05T22:46:19Z",
            "2025-01-17T18:29:18Z",
            "2025-01-17T18:15:21Z"
        ],
        "creation_date": "2011-09-01T21:41:26Z",
        "contributors": 30,
        "topics": [
            "block-storage",
            "cloud-storage",
            "distributed-file-system",
            "distributed-storage",
            "erasure-coding",
            "fuse",
            "hdfs",
            "high-performance",
            "highly-available",
            "iscsi",
            "kubernetes",
            "nfs",
            "nvme-over-fabrics",
            "object-store",
            "posix",
            "replication",
            "s3",
            "smb",
            "software-defined-storage",
            "storage"
        ],
        "subscribers": 657,
        "readme": "# Ceph - a scalable distributed storage system\n\nSee https://ceph.com/ for current information about Ceph.\n\n## Status\n\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/2220/badge)](https://www.bestpractices.dev/projects/2220)\n[![Issue Backporting](https://github.com/ceph/ceph/actions/workflows/create-backport-trackers.yml/badge.svg)](https://github.com/ceph/ceph/actions/workflows/create-backport-trackers.yml)\n\n## Contributing Code\n\nMost of Ceph is dual-licensed under the LGPL version 2.1 or 3.0. Some\nmiscellaneous code is either public domain or licensed under a BSD-style\nlicense.\n\nThe Ceph documentation is licensed under Creative Commons Attribution Share\nAlike 3.0 (CC-BY-SA-3.0). \n\nSome headers included in the `ceph/ceph` repository are licensed under the GPL.\nSee the file `COPYING` for a full inventory of licenses by file.\n\nAll code contributions must include a valid \"Signed-off-by\" line. See the file\n`SubmittingPatches.rst` for details on this and instructions on how to generate\nand submit patches.\n\nAssignment of copyright is not required to contribute code. Code is\ncontributed under the terms of the applicable license.\n\n\n## Checking out the source\n\nClone the ceph/ceph repository from github by running the following command on\na system that has git installed:\n\n\tgit clone git@github.com:ceph/ceph\n\nAlternatively, if you are not a github user, you should run the following\ncommand on a system that has git installed:\n\n\tgit clone https://github.com/ceph/ceph.git\n\nWhen the `ceph/ceph` repository has been cloned to your system, run the\nfollowing commands to move into the cloned `ceph/ceph` repository and to check\nout the git submodules associated with it:\n\n    cd ceph\n\tgit submodule update --init --recursive --progress\n\n\n## Build Prerequisites\n\n*section last updated 06 Sep 2024*\n\nWe provide the Debian and Ubuntu ``apt`` commands in this procedure. If you use\na system with a different package manager, then you will have to use different\ncommands. \n\n#. Install ``curl``:\n\n    apt install curl\n\n#. Install package dependencies by running the ``install-deps.sh`` script:\n\n\t./install-deps.sh\n\n#. Install the ``python3-routes`` package:\n\n    apt install python3-routes\n\n\n## Building Ceph\n\nThese instructions are meant for developers who are compiling the code for\ndevelopment and testing. To build binaries that are suitable for installation\nwe recommend that you build `.deb` or `.rpm` packages, or refer to\n``ceph.spec.in`` or ``debian/rules`` to see which configuration options are\nspecified for production builds.\n\nTo build Ceph, follow this procedure: \n\n1. Make sure that you are in the top-level `ceph` directory that\n   contains `do_cmake.sh` and `CONTRIBUTING.rst`.\n2. Run the `do_cmake.sh` script:\n\n       ./do_cmake.sh\n\n   ``do_cmake.sh`` by default creates a \"debug build\" of Ceph, which can be \n   up to five times slower than a non-debug build. Pass \n   ``-DCMAKE_BUILD_TYPE=RelWithDebInfo`` to ``do_cmake.sh`` to create a \n   non-debug build.\n3. Move into the `build` directory:\n\n       cd build\n4. Use the `ninja` buildsystem to build the development environment:\n\n       ninja -j3\n\n   > [!IMPORTANT]\n   >\n   > [Ninja](https://ninja-build.org/) is the build system used by the Ceph\n   > project to build test builds.  The number of jobs used by `ninja` is \n   > derived from the number of CPU cores of the building host if unspecified. \n   > Use the `-j` option to limit the job number if build jobs are running \n   > out of memory. If you attempt to run `ninja` and receive a message that \n   > reads `g++: fatal error: Killed signal terminated program cc1plus`, then \n   > you have run out of memory.\n   >\n   > Using the `-j` option with an argument appropriate to the hardware on\n   > which the `ninja` command is run is expected to result in a successful\n   > build. For example, to limit the job number to 3, run the command `ninja\n   > -j3`. On average, each `ninja` job run in parallel needs approximately\n   > 2.5 GiB of RAM.\n\n   This documentation assumes that your build directory is a subdirectory of\n   the `ceph.git` checkout. If the build directory is located elsewhere, point\n   `CEPH_GIT_DIR` to the correct path of the checkout. Additional CMake args \n   can be specified by setting ARGS before invoking ``do_cmake.sh``. \n   See [cmake options](#cmake-options) for more details. For example:\n\n       ARGS=\"-DCMAKE_C_COMPILER=gcc-7\" ./do_cmake.sh\n\n   To build only certain targets, run a command of the following form:\n\n       ninja [target name]\n\n5. Install the vstart cluster:\n\n       ninja install\n\n    \n\n \n### CMake Options\n\nThe `-D` flag can be used with `cmake` to speed up the process of building Ceph\nand to customize the build.\n\n#### Building without RADOS Gateway\n\nThe RADOS Gateway is built by default. To build Ceph without the RADOS Gateway,\nrun a command of the following form:\n\n\tcmake -DWITH_RADOSGW=OFF [path to top-level ceph directory]\n\n#### Building with debugging and arbitrary dependency locations \n\nRun a command of the following form to build Ceph with debugging and alternate\nlocations for some external dependencies:\n\n\tcmake -DCMAKE_INSTALL_PREFIX=/opt/ceph -DCMAKE_C_FLAGS=\"-Og -g3 -gdwarf-4\" \\\n\t..\n\nCeph has several bundled dependencies such as Boost, RocksDB and Arrow. By\ndefault, `cmake` builds these bundled dependencies from source instead of using\nlibraries that are already installed on the system. You can opt to use these\nsystem libraries, as long as they meet Ceph's version requirements. To use\nsystem libraries, use `cmake` options like `WITH_SYSTEM_BOOST`, as in the\nfollowing example:\n\n\tcmake -DWITH_SYSTEM_BOOST=ON [...]\n\nTo view an exhaustive list of -D options, invoke `cmake -LH`:\n\n\tcmake -LH\n\n#### Preserving diagnostic colors\n\nIf you pipe `ninja` to `less` and would like to preserve the diagnostic colors\nin the output in order to make errors and warnings more legible, run the\nfollowing command:  \n\n\tcmake -DDIAGNOSTICS_COLOR=always ...\n\nThe above command works only with supported compilers.\n\nThe diagnostic colors will be visible when the following command is run: \n\n\tninja | less -R\n\nOther available values for `DIAGNOSTICS_COLOR` are `auto` (default) and\n`never`.\n\n## Tips and Tricks\n\n   * Use \"debug builds\" only when needed. Debugging builds are helpful for\n     development, but they can slow down performance. Use\n     `-DCMAKE_BUILD_TYPE=Release` when debugging isn't necessary.\n   * Enable Selective Daemons when testing specific components. Don't start\n     unnecessary daemons.\n   * Preserve Existing Data skip cluster reinitialization between tests by\n     using the `-n` flag.\n   * To manage a vstart cluster, stop daemons using `./stop.sh` and start them\n     with `./vstart.sh --daemon osd.${ID} [--nodaemonize]`. \n   * Restart the sockets by stopping and restarting the daemons associated with\n     them. This ensures that there are no stale sockets in the cluster.\n   * To track RocksDB performance, set `export ROCKSDB_PERF=true` and start\n     the cluster by using the command `./vstart.sh -n -d -x --bluestore`. \n   * Build with `vstart-base` using debug flags in cmake, compile, and deploy\n     via `./vstart.sh -d -n --bluestore`.\n   * To containerize, generate configurations with `vstart.sh`, and deploy with\n     Docker, mapping directories and configuring the network.\n   * Manage containers using `docker run`, `stop`, and `rm`. For detailed\n     setups, consult the Ceph-Container repository.\n\n##  Troubleshooting     \n \n   * Cluster Fails to Start: Look for errors in the logs under the `out/`\n     directory.\n   * OSD Crashes: Check the OSD logs for errors.\n   * Cluster in a `Health Error` State: Run the `ceph status` command to\n     identify the issue.\n   * RocksDB Errors: Look for RocksDB-related errors in the OSD logs.\n\n## Building a source tarball\n\nTo build a complete source tarball with everything needed to build from\nsource and/or build a (deb or rpm) package, run\n\n\t./make-dist\n\nThis will create a tarball like ceph-$version.tar.bz2 from git.\n(Ensure that any changes you want to include in your working directory\nare committed to git.)\n\n\n## Running a test cluster\n\nFrom the `ceph/` directory, run the following commands to launch a test Ceph\ncluster:\n\n\tcd build\n\tninja vstart        # builds just enough to run vstart\n\t../src/vstart.sh --debug --new -x --localhost --bluestore\n\t./bin/ceph -s\n\nMost Ceph commands are available in the `bin/` directory. For example:\n\n\t./bin/rbd create foo --size 1000\n\t./bin/rados -p foo bench 30 write\n\nTo shut down the test cluster, run the following command from the `build/`\ndirectory:\n\n\t../src/stop.sh\n\nUse the sysvinit script to start or stop individual daemons: \n\n\t./bin/init-ceph restart osd.0\n\t./bin/init-ceph stop\n\n\n## Running unit tests\n\nTo build and run all tests (in parallel using all processors), use `ctest`:\n\n\tcd build\n\tninja\n\tctest -j$(nproc)\n\n(Note: Many targets built from src/test are not run using `ctest`.\nTargets starting with \"unittest\" are run in `ninja check` and thus can\nbe run with `ctest`. Targets starting with \"ceph_test\" can not, and should\nbe run by hand.)\n\nWhen failures occur, look in build/Testing/Temporary for logs.\n\nTo build and run all tests and their dependencies without other\nunnecessary targets in Ceph:\n\n\tcd build\n\tninja check -j$(nproc)\n\nTo run an individual test manually, run `ctest` with -R (regex matching):\n\n\tctest -R [regex matching test name(s)]\n\n(Note: `ctest` does not build the test it's running or the dependencies needed\nto run it)\n\nTo run an individual test manually and see all the tests output, run\n`ctest` with the -V (verbose) flag:\n\n\tctest -V -R [regex matching test name(s)]\n\nTo run tests manually and run the jobs in parallel, run `ctest` with \nthe `-j` flag:\n\n\tctest -j [number of jobs]\n\nThere are many other flags you can give `ctest` for better control\nover manual test execution. To view these options run:\n\n\tman ctest\n\n\n## Building the Documentation\n\n### Prerequisites\n\nThe list of package dependencies for building the documentation can be\nfound in `doc_deps.deb.txt`:\n\n\tsudo apt-get install `cat doc_deps.deb.txt`\n\n### Building the Documentation\n\nTo build the documentation, ensure that you are in the top-level\n`/ceph` directory, and execute the build script. For example:\n\n\tadmin/build-doc\n\n## Reporting Issues\n\nTo report an issue and view existing issues, please visit https://tracker.ceph.com/projects/ceph.\n",
        "releases": []
    }
}