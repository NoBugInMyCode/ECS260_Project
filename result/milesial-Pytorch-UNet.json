{
    "https://api.github.com/repos/milesial/Pytorch-UNet": {
        "forks": 2543,
        "watchers": 9588,
        "stars": 9588,
        "languages": {
            "Python": 27733,
            "Batchfile": 872,
            "Shell": 650,
            "Dockerfile": 230
        },
        "commits": [
            "2024-02-11T00:59:01Z",
            "2024-02-11T00:58:12Z",
            "2024-01-07T18:30:55Z",
            "2023-12-08T10:14:51Z",
            "2023-03-02T02:52:24Z",
            "2023-03-02T02:44:04Z",
            "2023-02-28T13:52:40Z",
            "2023-01-18T22:18:17Z",
            "2023-01-18T22:02:40Z",
            "2023-01-04T14:22:11Z",
            "2023-01-01T05:38:49Z",
            "2022-12-28T03:46:55Z",
            "2022-12-21T07:06:42Z",
            "2022-12-21T06:52:44Z",
            "2022-12-21T06:51:52Z",
            "2022-12-12T16:22:27Z",
            "2022-12-12T15:12:22Z",
            "2022-12-09T14:14:59Z",
            "2022-12-06T19:22:03Z",
            "2022-10-24T13:52:32Z",
            "2022-07-24T00:30:39Z",
            "2022-07-22T09:38:53Z",
            "2022-07-08T21:20:23Z",
            "2022-07-08T20:58:22Z",
            "2022-07-08T20:57:19Z",
            "2022-05-05T13:05:50Z",
            "2022-05-04T15:51:48Z",
            "2022-04-11T21:56:51Z",
            "2022-04-11T21:55:26Z",
            "2022-04-10T03:41:35Z"
        ],
        "creation_date": "2017-08-16T12:17:08Z",
        "contributors": 21,
        "topics": [
            "convolutional-networks",
            "convolutional-neural-networks",
            "deep-learning",
            "kaggle",
            "pytorch",
            "pytorch-unet",
            "semantic-segmentation",
            "tensorboard",
            "unet",
            "wandb",
            "weights-and-biases"
        ],
        "subscribers": 71,
        "readme": "# U-Net: Semantic segmentation with PyTorch\n<a href=\"#\"><img src=\"https://img.shields.io/github/actions/workflow/status/milesial/PyTorch-UNet/main.yml?logo=github&style=for-the-badge\" /></a>\n<a href=\"https://hub.docker.com/r/milesial/unet\"><img src=\"https://img.shields.io/badge/docker%20image-available-blue?logo=Docker&style=for-the-badge\" /></a>\n<a href=\"https://pytorch.org/\"><img src=\"https://img.shields.io/badge/PyTorch-v1.13+-red.svg?logo=PyTorch&style=for-the-badge\" /></a>\n<a href=\"#\"><img src=\"https://img.shields.io/badge/python-v3.6+-blue.svg?logo=python&style=for-the-badge\" /></a>\n\n![input and output for a random image in the test dataset](https://i.imgur.com/GD8FcB7.png)\n\n\nCustomized implementation of the [U-Net](https://arxiv.org/abs/1505.04597) in PyTorch for Kaggle's [Carvana Image Masking Challenge](https://www.kaggle.com/c/carvana-image-masking-challenge) from high definition images.\n\n- [Quick start](#quick-start)\n  - [Without Docker](#without-docker)\n  - [With Docker](#with-docker)\n- [Description](#description)\n- [Usage](#usage)\n  - [Docker](#docker)\n  - [Training](#training)\n  - [Prediction](#prediction)\n- [Weights & Biases](#weights--biases)\n- [Pretrained model](#pretrained-model)\n- [Data](#data)\n\n## Quick start\n\n### Without Docker\n\n1. [Install CUDA](https://developer.nvidia.com/cuda-downloads)\n\n2. [Install PyTorch 1.13 or later](https://pytorch.org/get-started/locally/)\n\n3. Install dependencies\n```bash\npip install -r requirements.txt\n```\n\n4. Download the data and run training:\n```bash\nbash scripts/download_data.sh\npython train.py --amp\n```\n\n### With Docker\n\n1. [Install Docker 19.03 or later:](https://docs.docker.com/get-docker/)\n```bash\ncurl https://get.docker.com | sh && sudo systemctl --now enable docker\n```\n2. [Install the NVIDIA container toolkit:](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)\n```bash\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\\n   && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\\n   && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\nsudo apt-get update\nsudo apt-get install -y nvidia-docker2\nsudo systemctl restart docker\n```\n3. [Download and run the image:](https://hub.docker.com/repository/docker/milesial/unet)\n```bash\nsudo docker run --rm --shm-size=8g --ulimit memlock=-1 --gpus all -it milesial/unet\n```\n\n4. Download the data and run training:\n```bash\nbash scripts/download_data.sh\npython train.py --amp\n```\n\n## Description\nThis model was trained from scratch with 5k images and scored a [Dice coefficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) of 0.988423 on over 100k test images.\n\nIt can be easily used for multiclass segmentation, portrait segmentation, medical segmentation, ...\n\n\n## Usage\n**Note : Use Python 3.6 or newer**\n\n### Docker\n\nA docker image containing the code and the dependencies is available on [DockerHub](https://hub.docker.com/repository/docker/milesial/unet).\nYou can download and jump in the container with ([docker >=19.03](https://docs.docker.com/get-docker/)):\n\n```console\ndocker run -it --rm --shm-size=8g --ulimit memlock=-1 --gpus all milesial/unet\n```\n\n\n### Training\n\n```console\n> python train.py -h\nusage: train.py [-h] [--epochs E] [--batch-size B] [--learning-rate LR]\n                [--load LOAD] [--scale SCALE] [--validation VAL] [--amp]\n\nTrain the UNet on images and target masks\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --epochs E, -e E      Number of epochs\n  --batch-size B, -b B  Batch size\n  --learning-rate LR, -l LR\n                        Learning rate\n  --load LOAD, -f LOAD  Load model from a .pth file\n  --scale SCALE, -s SCALE\n                        Downscaling factor of the images\n  --validation VAL, -v VAL\n                        Percent of the data that is used as validation (0-100)\n  --amp                 Use mixed precision\n```\n\nBy default, the `scale` is 0.5, so if you wish to obtain better results (but use more memory), set it to 1.\n\nAutomatic mixed precision is also available with the `--amp` flag. [Mixed precision](https://arxiv.org/abs/1710.03740) allows the model to use less memory and to be faster on recent GPUs by using FP16 arithmetic. Enabling AMP is recommended.\n\n\n### Prediction\n\nAfter training your model and saving it to `MODEL.pth`, you can easily test the output masks on your images via the CLI.\n\nTo predict a single image and save it:\n\n`python predict.py -i image.jpg -o output.jpg`\n\nTo predict a multiple images and show them without saving them:\n\n`python predict.py -i image1.jpg image2.jpg --viz --no-save`\n\n```console\n> python predict.py -h\nusage: predict.py [-h] [--model FILE] --input INPUT [INPUT ...] \n                  [--output INPUT [INPUT ...]] [--viz] [--no-save]\n                  [--mask-threshold MASK_THRESHOLD] [--scale SCALE]\n\nPredict masks from input images\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --model FILE, -m FILE\n                        Specify the file in which the model is stored\n  --input INPUT [INPUT ...], -i INPUT [INPUT ...]\n                        Filenames of input images\n  --output INPUT [INPUT ...], -o INPUT [INPUT ...]\n                        Filenames of output images\n  --viz, -v             Visualize the images as they are processed\n  --no-save, -n         Do not save the output masks\n  --mask-threshold MASK_THRESHOLD, -t MASK_THRESHOLD\n                        Minimum probability value to consider a mask pixel white\n  --scale SCALE, -s SCALE\n                        Scale factor for the input images\n```\nYou can specify which model file to use with `--model MODEL.pth`.\n\n## Weights & Biases\n\nThe training progress can be visualized in real-time using [Weights & Biases](https://wandb.ai/).  Loss curves, validation curves, weights and gradient histograms, as well as predicted masks are logged to the platform.\n\nWhen launching a training, a link will be printed in the console. Click on it to go to your dashboard. If you have an existing W&B account, you can link it\n by setting the `WANDB_API_KEY` environment variable. If not, it will create an anonymous run which is automatically deleted after 7 days.\n\n\n## Pretrained model\nA [pretrained model](https://github.com/milesial/Pytorch-UNet/releases/tag/v3.0) is available for the Carvana dataset. It can also be loaded from torch.hub:\n\n```python\nnet = torch.hub.load('milesial/Pytorch-UNet', 'unet_carvana', pretrained=True, scale=0.5)\n```\nAvailable scales are 0.5 and 1.0.\n\n## Data\nThe Carvana data is available on the [Kaggle website](https://www.kaggle.com/c/carvana-image-masking-challenge/data).\n\nYou can also download it using the helper script:\n\n```\nbash scripts/download_data.sh\n```\n\nThe input images and target masks should be in the `data/imgs` and `data/masks` folders respectively (note that the `imgs` and `masks` folder should not contain any sub-folder or any other files, due to the greedy data-loader). For Carvana, images are RGB and masks are black and white.\n\nYou can use your own dataset as long as you make sure it is loaded properly in `utils/data_loading.py`.\n\n\n---\n\nOriginal paper by Olaf Ronneberger, Philipp Fischer, Thomas Brox:\n\n[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n\n![network architecture](https://i.imgur.com/jeDVpqF.png)\n",
        "releases": [
            {
                "name": "New checkpoints",
                "date": "2022-02-19T04:09:30Z"
            },
            {
                "name": "Refactor and new checkpoint",
                "date": "2021-08-19T09:13:22Z"
            },
            {
                "name": "Release of a Carvana Unet pretrained model",
                "date": "2020-07-30T01:38:29Z"
            }
        ]
    }
}