{
    "https://api.github.com/repos/ddbourgin/numpy-ml": {
        "forks": 3777,
        "watchers": 15877,
        "stars": 15877,
        "languages": {
            "Python": 1143006
        },
        "commits": [
            "2022-01-08T22:00:20Z",
            "2022-01-08T21:58:39Z",
            "2022-01-08T21:56:17Z",
            "2022-01-08T21:00:31Z",
            "2021-12-29T06:12:09Z",
            "2021-12-29T06:10:52Z",
            "2021-12-26T04:08:59Z",
            "2021-12-26T04:08:51Z",
            "2021-12-26T04:08:09Z",
            "2021-12-25T02:19:56Z",
            "2021-12-25T02:19:30Z",
            "2021-12-25T02:17:32Z",
            "2021-12-25T01:40:12Z",
            "2021-12-25T01:40:01Z",
            "2021-12-24T00:18:21Z",
            "2021-12-24T00:12:28Z",
            "2021-12-23T23:22:13Z",
            "2021-12-23T23:20:12Z",
            "2021-06-24T17:12:09Z",
            "2021-06-23T03:33:06Z",
            "2021-06-01T16:30:34Z",
            "2021-06-01T16:28:30Z",
            "2021-05-31T23:22:48Z",
            "2021-05-29T20:20:02Z",
            "2021-05-25T00:49:47Z",
            "2021-05-25T00:49:08Z",
            "2021-04-05T04:19:56Z",
            "2021-03-05T18:23:42Z",
            "2021-03-05T18:18:12Z",
            "2020-07-27T16:06:09Z"
        ],
        "creation_date": "2019-04-06T22:29:49Z",
        "contributors": 11,
        "topics": [
            "attention",
            "bayesian-inference",
            "gaussian-mixture-models",
            "gaussian-processes",
            "good-turing-smoothing",
            "gradient-boosting",
            "hidden-markov-models",
            "knn",
            "lstm",
            "machine-learning",
            "mfcc",
            "neural-networks",
            "reinforcement-learning",
            "resnet",
            "topic-modeling",
            "vae",
            "wavenet",
            "wgan-gp",
            "word2vec"
        ],
        "subscribers": 462,
        "readme": "# numpy-ml\nEver wish you had an inefficient but somewhat legible collection of machine\nlearning algorithms implemented exclusively in NumPy? No?\n\n## Installation\n\n### For rapid experimentation\nTo use this code as a starting point for ML prototyping / experimentation, just clone the repository, create a new [virtualenv](https://pypi.org/project/virtualenv/), and start hacking:\n\n```sh\n$ git clone https://github.com/ddbourgin/numpy-ml.git\n$ cd numpy-ml && virtualenv npml && source npml/bin/activate\n$ pip3 install -r requirements-dev.txt\n```\n\n### As a package\nIf you don't plan to modify the source, you can also install numpy-ml as a\nPython package: `pip3 install -u numpy_ml`.\n\nThe reinforcement learning agents train on environments defined in the [OpenAI\ngym](https://github.com/openai/gym). To install these alongside numpy-ml, you\ncan use `pip3 install -u 'numpy_ml[rl]'`.\n\n## Documentation\nFor more details on the available models, see the [project documentation](https://numpy-ml.readthedocs.io/).\n\n## Available models\n<details>\n  <summary>Click to expand!</summary>\n\n1. **Gaussian mixture model**\n    - EM training\n\n2. **Hidden Markov model**\n    - Viterbi decoding\n    - Likelihood computation\n    - MLE parameter estimation via Baum-Welch/forward-backward algorithm\n\n3. **Latent Dirichlet allocation** (topic model)\n    - Standard model with MLE parameter estimation via variational EM\n    - Smoothed model with MAP parameter estimation via MCMC\n\n4. **Neural networks**\n    * Layers / Layer-wise ops\n        - Add\n        - Flatten\n        - Multiply\n        - Softmax\n        - Fully-connected/Dense\n        - Sparse evolutionary connections\n        - LSTM\n        - Elman-style RNN\n        - Max + average pooling\n        - Dot-product attention\n        - Embedding layer\n        - Restricted Boltzmann machine (w. CD-n training)\n        - 2D deconvolution (w. padding and stride)\n        - 2D convolution (w. padding, dilation, and stride)\n        - 1D convolution (w. padding, dilation, stride, and causality)\n    * Modules\n        - Bidirectional LSTM\n        - ResNet-style residual blocks (identity and convolution)\n        - WaveNet-style residual blocks with dilated causal convolutions\n        - Transformer-style multi-headed scaled dot product attention\n    * Regularizers\n        - Dropout\n    * Normalization\n        - Batch normalization (spatial and temporal)\n        - Layer normalization (spatial and temporal)\n    * Optimizers\n        - SGD w/ momentum\n        - AdaGrad\n        - RMSProp\n        - Adam\n    * Learning Rate Schedulers\n        - Constant\n        - Exponential\n        - Noam/Transformer\n        - Dlib scheduler\n    * Weight Initializers\n        - Glorot/Xavier uniform and normal\n        - He/Kaiming uniform and normal\n        - Standard and truncated normal\n    * Losses\n        - Cross entropy\n        - Squared error\n        - Bernoulli VAE loss\n        - Wasserstein loss with gradient penalty\n        - Noise contrastive estimation loss\n    * Activations\n        - ReLU\n        - Tanh\n        - Affine\n        - Sigmoid\n        - Leaky ReLU\n        - ELU\n        - SELU\n        - GELU\n        - Exponential\n        - Hard Sigmoid\n        - Softplus\n    * Models\n        - Bernoulli variational autoencoder\n        - Wasserstein GAN with gradient penalty\n        - word2vec encoder with skip-gram and CBOW architectures\n    * Utilities\n        - `col2im` (MATLAB port)\n        - `im2col` (MATLAB port)\n        - `conv1D`\n        - `conv2D`\n        - `deconv2D`\n        - `minibatch`\n\n5. **Tree-based models**\n    - Decision trees (CART)\n    - [Bagging] Random forests\n    - [Boosting] Gradient-boosted decision trees\n\n6. **Linear models**\n    - Ridge regression\n    - Logistic regression\n    - Ordinary least squares\n    - Weighted linear regression\n    - Generalized linear model (log, logit, and identity link)\n    - Gaussian naive Bayes classifier\n    - Bayesian linear regression w/ conjugate priors\n        - Unknown mean, known variance (Gaussian prior)\n        - Unknown mean, unknown variance (Normal-Gamma / Normal-Inverse-Wishart prior)\n\n7. **n-Gram sequence models**\n    - Maximum likelihood scores\n    - Additive/Lidstone smoothing\n    - Simple Good-Turing smoothing\n\n8. **Multi-armed bandit models**\n    - UCB1\n    - LinUCB\n    - Epsilon-greedy\n    - Thompson sampling w/ conjugate priors\n        - Beta-Bernoulli sampler\n    - LinUCB\n\n8. **Reinforcement learning models**\n    - Cross-entropy method agent\n    - First visit on-policy Monte Carlo agent\n    - Weighted incremental importance sampling Monte Carlo agent\n    - Expected SARSA agent\n    - TD-0 Q-learning agent\n    - Dyna-Q / Dyna-Q+ with prioritized sweeping\n\n9. **Nonparameteric models**\n    - Nadaraya-Watson kernel regression\n    - k-Nearest neighbors classification and regression\n    - Gaussian process regression\n\n10. **Matrix factorization**\n    - Regularized alternating least-squares\n    - Non-negative matrix factorization\n\n11. **Preprocessing**\n    - Discrete Fourier transform (1D signals)\n    - Discrete cosine transform (type-II) (1D signals)\n    - Bilinear interpolation (2D signals)\n    - Nearest neighbor interpolation (1D and 2D signals)\n    - Autocorrelation (1D signals)\n    - Signal windowing\n    - Text tokenization\n    - Feature hashing\n    - Feature standardization\n    - One-hot encoding / decoding\n    - Huffman coding / decoding\n    - Byte pair encoding / decoding\n    - Term frequency-inverse document frequency (TF-IDF) encoding\n    - MFCC encoding\n\n12. **Utilities**\n    - Similarity kernels\n    - Distance metrics\n    - Priority queue\n    - Ball tree\n    - Discrete sampler\n    - Graph processing and generators\n</details>\n\n## Contributing\n\nAm I missing your favorite model? Is there something that could be cleaner /\nless confusing? Did I mess something up? Submit a PR! The only requirement is\nthat your models are written with just the [Python standard\nlibrary](https://docs.python.org/3/library/) and [NumPy](https://www.numpy.org/). The\n[SciPy library](https://scipy.github.io/devdocs/) is also permitted under special\ncircumstances ;)\n\nSee full contributing guidelines [here](./CONTRIBUTING.md).\n",
        "releases": []
    }
}