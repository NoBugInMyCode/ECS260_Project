{
    "https://api.github.com/repos/binux/pyspider": {
        "forks": 3685,
        "watchers": 16535,
        "stars": 16535,
        "languages": {
            "Python": 594223,
            "JavaScript": 56310,
            "HTML": 25020,
            "CSS": 11931,
            "Lua": 6394,
            "Dockerfile": 1398
        },
        "commits": [
            "2020-08-02T17:34:20Z",
            "2020-07-27T04:37:25Z",
            "2020-07-27T03:15:35Z",
            "2020-07-27T02:35:59Z",
            "2020-07-26T23:57:30Z",
            "2020-07-26T23:20:28Z",
            "2020-07-26T21:48:40Z",
            "2019-11-14T05:30:18Z",
            "2019-11-14T05:22:06Z",
            "2019-11-08T17:24:26Z",
            "2019-11-08T10:06:47Z",
            "2019-11-08T09:57:10Z",
            "2019-11-07T08:49:43Z",
            "2019-11-07T08:47:33Z",
            "2019-11-07T08:45:08Z",
            "2019-11-07T08:31:10Z",
            "2019-11-06T16:36:09Z",
            "2019-11-06T16:24:27Z",
            "2019-11-06T16:07:06Z",
            "2019-11-06T16:05:30Z",
            "2019-11-06T15:58:08Z",
            "2019-11-06T15:49:33Z",
            "2019-11-06T15:48:40Z",
            "2019-11-06T15:47:49Z",
            "2019-11-06T15:37:22Z",
            "2019-11-06T15:07:09Z",
            "2019-11-06T14:42:49Z",
            "2019-11-06T14:33:02Z",
            "2019-11-06T14:22:22Z",
            "2019-11-06T14:12:58Z"
        ],
        "creation_date": "2014-02-21T19:18:47Z",
        "contributors": 30,
        "topics": [
            "crawler",
            "python"
        ],
        "subscribers": 895,
        "readme": "pyspider [![Build Status]][Travis CI] [![Coverage Status]][Coverage]\n========\n\nA Powerful Spider(Web Crawler) System in Python.\n\n- Write script in Python\n- Powerful WebUI with script editor, task monitor, project manager and result viewer\n- [MySQL](https://www.mysql.com/), [MongoDB](https://www.mongodb.org/), [Redis](http://redis.io/), [SQLite](https://www.sqlite.org/), [Elasticsearch](https://www.elastic.co/products/elasticsearch); [PostgreSQL](http://www.postgresql.org/) with [SQLAlchemy](http://www.sqlalchemy.org/) as database backend\n- [RabbitMQ](http://www.rabbitmq.com/), [Redis](http://redis.io/) and [Kombu](http://kombu.readthedocs.org/) as message queue\n- Task priority, retry, periodical, recrawl by age, etc...\n- Distributed architecture, Crawl Javascript pages, Python 2.{6,7}, 3.{3,4,5,6} support, etc...\n\nTutorial: [http://docs.pyspider.org/en/latest/tutorial/](http://docs.pyspider.org/en/latest/tutorial/)  \nDocumentation: [http://docs.pyspider.org/](http://docs.pyspider.org/)  \nRelease notes: [https://github.com/binux/pyspider/releases](https://github.com/binux/pyspider/releases)  \n\nSample Code \n-----------\n\n```python\nfrom pyspider.libs.base_handler import *\n\n\nclass Handler(BaseHandler):\n    crawl_config = {\n    }\n\n    @every(minutes=24 * 60)\n    def on_start(self):\n        self.crawl('http://scrapy.org/', callback=self.index_page)\n\n    @config(age=10 * 24 * 60 * 60)\n    def index_page(self, response):\n        for each in response.doc('a[href^=\"http\"]').items():\n            self.crawl(each.attr.href, callback=self.detail_page)\n\n    def detail_page(self, response):\n        return {\n            \"url\": response.url,\n            \"title\": response.doc('title').text(),\n        }\n```\n\n\nInstallation\n------------\n\n* `pip install pyspider`\n* run command `pyspider`, visit [http://localhost:5000/](http://localhost:5000/)\n\n**WARNING:** WebUI is open to the public by default, it can be used to execute any command which may harm your system. Please use it in an internal network or [enable `need-auth` for webui](http://docs.pyspider.org/en/latest/Command-Line/#-config).\n\nQuickstart: [http://docs.pyspider.org/en/latest/Quickstart/](http://docs.pyspider.org/en/latest/Quickstart/)\n\nContribute\n----------\n\n* Use It\n* Open [Issue], send PR\n* [User Group]\n* [\u4e2d\u6587\u95ee\u7b54](http://segmentfault.com/t/pyspider)\n\n\nTODO\n----\n\n### v0.4.0\n\n- [ ] a visual scraping interface like [portia](https://github.com/scrapinghub/portia)\n\n\nLicense\n-------\nLicensed under the Apache License, Version 2.0\n\n\n[Build Status]:         https://img.shields.io/travis/binux/pyspider/master.svg?style=flat\n[Travis CI]:            https://travis-ci.org/binux/pyspider\n[Coverage Status]:      https://img.shields.io/coveralls/binux/pyspider.svg?branch=master&style=flat\n[Coverage]:             https://coveralls.io/r/binux/pyspider\n[Try]:                  https://img.shields.io/badge/try-pyspider-blue.svg?style=flat\n[Issue]:                https://github.com/binux/pyspider/issues\n[User Group]:           https://groups.google.com/group/pyspider-users\n",
        "releases": [
            {
                "name": "v0.3.10",
                "date": "2018-04-18T04:25:22Z"
            },
            {
                "name": "v0.3.9",
                "date": "2017-03-18T21:00:11Z"
            },
            {
                "name": "v0.3.8",
                "date": "2016-08-18T20:06:54Z"
            },
            {
                "name": "v0.3.7",
                "date": "2016-04-20T20:42:26Z"
            },
            {
                "name": "v0.3.6",
                "date": "2015-11-10T00:33:20Z"
            },
            {
                "name": "v0.3.5",
                "date": "2015-05-22T16:02:04Z"
            },
            {
                "name": "v0.3.4",
                "date": "2015-04-21T15:01:18Z"
            },
            {
                "name": "v0.3.3",
                "date": "2015-03-08T13:32:39Z"
            },
            {
                "name": "v0.3.2",
                "date": "2015-02-11T16:33:09Z"
            },
            {
                "name": "v0.3.1",
                "date": "2015-01-22T15:59:31Z"
            },
            {
                "name": "First PyPI Release",
                "date": "2015-01-11T05:38:50Z"
            },
            {
                "name": "First Working Release",
                "date": "2014-11-12T13:24:56Z"
            },
            {
                "name": "First Runnable Release",
                "date": "2014-03-09T03:08:02Z"
            }
        ]
    }
}