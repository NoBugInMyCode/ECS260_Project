{
    "https://api.github.com/repos/google/trax": {
        "forks": 822,
        "watchers": 8147,
        "stars": 8147,
        "languages": {
            "Python": 2584268,
            "Jupyter Notebook": 2441357,
            "Shell": 6619
        },
        "commits": [
            "2025-01-08T00:22:54Z",
            "2024-09-10T20:42:02Z",
            "2024-09-10T17:37:39Z",
            "2024-09-10T16:44:35Z",
            "2024-08-21T22:23:35Z",
            "2024-08-18T18:19:56Z",
            "2024-08-13T22:09:23Z",
            "2024-07-16T17:26:58Z",
            "2024-05-16T01:28:14Z",
            "2024-02-01T02:27:58Z",
            "2023-11-15T06:25:38Z",
            "2023-10-23T21:04:19Z",
            "2023-10-20T09:01:39Z",
            "2023-03-29T01:14:20Z",
            "2023-02-15T22:44:08Z",
            "2022-12-19T21:57:40Z",
            "2022-11-09T23:29:54Z",
            "2022-09-09T21:36:13Z",
            "2022-08-08T21:38:41Z",
            "2022-08-05T18:55:31Z",
            "2022-08-04T00:44:48Z",
            "2022-06-27T20:23:29Z",
            "2022-06-07T22:16:19Z",
            "2022-05-17T01:02:44Z",
            "2022-04-24T18:28:09Z",
            "2022-04-22T00:51:42Z",
            "2022-04-21T20:16:57Z",
            "2022-04-20T15:50:01Z",
            "2022-04-20T12:57:49Z",
            "2022-04-20T01:17:24Z"
        ],
        "creation_date": "2019-10-05T15:09:14Z",
        "contributors": 30,
        "topics": [
            "deep-learning",
            "deep-reinforcement-learning",
            "jax",
            "machine-learning",
            "numpy",
            "reinforcement-learning",
            "transformer"
        ],
        "subscribers": 140,
        "readme": "# Trax &mdash; Deep Learning with Clear Code and Speed\n\n![train tracks](https://images.pexels.com/photos/461772/pexels-photo-461772.jpeg?dl&fit=crop&crop=entropy&w=32&h=21)\n[![PyPI\nversion](https://badge.fury.io/py/trax.svg)](https://badge.fury.io/py/trax)\n[![GitHub\nIssues](https://img.shields.io/github/issues/google/trax.svg)](https://github.com/google/trax/issues)\n![GitHub Build](https://github.com/google/trax/actions/workflows/build.yaml/badge.svg)\n[![Contributions\nwelcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![License](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Gitter](https://img.shields.io/gitter/room/nwjs/nw.js.svg)](https://gitter.im/trax-ml/community)\n\n[Trax](https://trax-ml.readthedocs.io/en/latest/) is an end-to-end library for deep learning that focuses on clear code and speed. It is actively used and maintained in the [Google Brain team](https://research.google.com/teams/brain/). This notebook ([run it in colab](https://colab.research.google.com/github/google/trax/blob/master/trax/intro.ipynb)) shows how to use Trax and where you can find more information.\n\n  1. **Run a pre-trained Transformer**: create a translator in a few lines of code\n  1. **Features and resources**: [API docs](https://trax-ml.readthedocs.io/en/latest/), where to [talk to us](https://gitter.im/trax-ml/community), how to [open an issue](https://github.com/google/trax/issues) and more\n  1. **Walkthrough**: how Trax works, how to make new models and train on your own data\n\nWe welcome **contributions** to Trax! We welcome PRs with code for new models and layers as well as improvements to our code and documentation. We especially love **notebooks** that explain how models work and show how to use them to solve problems!\n\n\n\nHere are a few example notebooks:-\n\n* [**trax.data API explained**](https://github.com/google/trax/blob/master/trax/examples/trax_data_Explained.ipynb) : Explains some of the major functions in the `trax.data` API \n* [**Named Entity Recognition using Reformer**](https://github.com/google/trax/blob/master/trax/examples/NER_using_Reformer.ipynb) : Uses a [Kaggle dataset](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus) for implementing Named Entity Recognition using the [Reformer](https://arxiv.org/abs/2001.04451) architecture.\n* [**Deep N-Gram models**](https://github.com/google/trax/blob/master/trax/examples/Deep_N_Gram_Models.ipynb) : Implementation of deep n-gram models trained on Shakespeares works\n\n\n\n**General Setup**\n\nExecute the following cell (once) before running any of the code samples.\n\n\n```python\nimport os\nimport numpy as np\n\n!pip install -q -U trax\nimport trax\n```\n\n\n## 1. Run a pre-trained Transformer\n\nHere is how you create an English-German translator in a few lines of code:\n\n* create a Transformer model in Trax with [trax.models.Transformer](https://trax-ml.readthedocs.io/en/latest/trax.models.html#trax.models.transformer.Transformer)\n* initialize it from a file with pre-trained weights with [model.init_from_file](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Layer.init_from_file)\n* tokenize your input sentence to input into the model with [trax.data.tokenize](https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.tokenize)\n* decode from the Transformer with [trax.supervised.decoding.autoregressive_sample](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.decoding.autoregressive_sample)\n* de-tokenize the decoded result to get the translation with [trax.data.detokenize](https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.detokenize)\n\n\n\n```python\n# Create a Transformer model.\n# Pre-trained model config in gs://trax-ml/models/translation/ende_wmt32k.gin\nmodel = trax.models.Transformer(\n    input_vocab_size=33300,\n    d_model=512, d_ff=2048,\n    n_heads=8, n_encoder_layers=6, n_decoder_layers=6,\n    max_len=2048, mode='predict')\n\n# Initialize using pre-trained weights.\nmodel.init_from_file('gs://trax-ml/models/translation/ende_wmt32k.pkl.gz',\n                     weights_only=True)\n\n# Tokenize a sentence.\nsentence = 'It is nice to learn new things today!'\ntokenized = list(trax.data.tokenize(iter([sentence]),  # Operates on streams.\n                                    vocab_dir='gs://trax-ml/vocabs/',\n                                    vocab_file='ende_32k.subword'))[0]\n\n# Decode from the Transformer.\ntokenized = tokenized[None, :]  # Add batch dimension.\ntokenized_translation = trax.supervised.decoding.autoregressive_sample(\n    model, tokenized, temperature=0.0)  # Higher temperature: more diverse results.\n\n# De-tokenize,\ntokenized_translation = tokenized_translation[0][:-1]  # Remove batch and EOS.\ntranslation = trax.data.detokenize(tokenized_translation,\n                                   vocab_dir='gs://trax-ml/vocabs/',\n                                   vocab_file='ende_32k.subword')\nprint(translation)\n```\n\n    Es ist sch\u00f6n, heute neue Dinge zu lernen!\n\n\n## 2. Features and resources\n\nTrax includes basic models (like [ResNet](https://github.com/google/trax/blob/master/trax/models/resnet.py#L70), [LSTM](https://github.com/google/trax/blob/master/trax/models/rnn.py#L100), [Transformer](https://github.com/google/trax/blob/master/trax/models/transformer.py#L189)) and RL algorithms\n(like [REINFORCE](https://github.com/google/trax/blob/master/trax/rl/training.py#L244), [A2C](https://github.com/google/trax/blob/master/trax/rl/actor_critic_joint.py#L458), [PPO](https://github.com/google/trax/blob/master/trax/rl/actor_critic_joint.py#L209)). It is also actively used for research and includes\nnew models like the [Reformer](https://github.com/google/trax/tree/master/trax/models/reformer) and new RL algorithms like [AWR](https://arxiv.org/abs/1910.00177). Trax has bindings to a large number of deep learning datasets, including\n[Tensor2Tensor](https://github.com/tensorflow/tensor2tensor) and [TensorFlow datasets](https://www.tensorflow.org/datasets/catalog/overview).\n\n\nYou can use Trax either as a library from your own python scripts and notebooks\nor as a binary from the shell, which can be more convenient for training large models.\nIt runs without any changes on CPUs, GPUs and TPUs.\n\n* [API docs](https://trax-ml.readthedocs.io/en/latest/)\n* [chat with us](https://gitter.im/trax-ml/community)\n* [open an issue](https://github.com/google/trax/issues)\n* subscribe to [trax-discuss](https://groups.google.com/u/1/g/trax-discuss) for news\n\n\n## 3. Walkthrough\n\nYou can learn here how Trax works, how to create new models and how to train them on your own data.\n\n### Tensors and Fast Math\n\nThe basic units flowing through Trax models are *tensors* - multi-dimensional arrays, sometimes also known as numpy arrays, due to the most widely used package for tensor operations -- `numpy`. You should take a look at the [numpy guide](https://numpy.org/doc/stable/user/quickstart.html) if you don't know how to operate on tensors: Trax also uses the numpy API for that.\n\nIn Trax we want numpy operations to run very fast, making use of GPUs and TPUs to accelerate them. We also want to automatically compute gradients of functions on tensors. This is done in the `trax.fastmath` package thanks to its backends -- [JAX](https://github.com/google/jax) and [TensorFlow numpy](https://tensorflow.org/guide/tf_numpy).\n\n\n```python\nfrom trax.fastmath import numpy as fastnp\ntrax.fastmath.use_backend('jax')  # Can be 'jax' or 'tensorflow-numpy'.\n\nmatrix  = fastnp.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(f'matrix = \\n{matrix}')\nvector = fastnp.ones(3)\nprint(f'vector = {vector}')\nproduct = fastnp.dot(vector, matrix)\nprint(f'product = {product}')\ntanh = fastnp.tanh(product)\nprint(f'tanh(product) = {tanh}')\n```\n\n    matrix = \n    [[1 2 3]\n     [4 5 6]\n     [7 8 9]]\n    vector = [1. 1. 1.]\n    product = [12. 15. 18.]\n    tanh(product) = [0.99999994 0.99999994 0.99999994]\n\n\nGradients can be calculated using `trax.fastmath.grad`.\n\n\n```python\ndef f(x):\n  return 2.0 * x * x\n\ngrad_f = trax.fastmath.grad(f)\n\nprint(f'grad(2x^2) at 1 = {grad_f(1.0)}')\n```\n\n    grad(2x^2) at 1 = 4.0\n\n\n### Layers\n\nLayers are basic building blocks of Trax models. You will learn all about them in the [layers intro](https://trax-ml.readthedocs.io/en/latest/notebooks/layers_intro.html) but for now, just take a look at the implementation of one core Trax layer, `Embedding`:\n\n```python\nclass Embedding(base.Layer):\n  \"\"\"Trainable layer that maps discrete tokens/IDs to vectors.\"\"\"\n\n  def __init__(self,\n               vocab_size,\n               d_feature,\n               kernel_initializer=init.RandomNormalInitializer(1.0)):\n    \"\"\"Returns an embedding layer with given vocabulary size and vector size.\n\n    Args:\n      vocab_size: Size of the input vocabulary. The layer will assign a unique\n          vector to each ID in `range(vocab_size)`.\n      d_feature: Dimensionality/depth of the output vectors.\n      kernel_initializer: Function that creates (random) initial vectors for\n          the embedding.\n    \"\"\"\n    super().__init__(name=f'Embedding_{vocab_size}_{d_feature}')\n    self._d_feature = d_feature  # feature dimensionality\n    self._vocab_size = vocab_size\n    self._kernel_initializer = kernel_initializer\n\n  def forward(self, x):\n    \"\"\"Returns embedding vectors corresponding to input token IDs.\n\n    Args:\n      x: Tensor of token IDs.\n\n    Returns:\n      Tensor of embedding vectors.\n    \"\"\"\n    return jnp.take(self.weights, x, axis=0, mode='clip')\n\n  def init_weights_and_state(self, input_signature):\n    \"\"\"Returns tensor of newly initialized embedding vectors.\"\"\"\n    del input_signature\n    shape_w = (self._vocab_size, self._d_feature)\n    w = self._kernel_initializer(shape_w, self.rng)\n    self.weights = w\n```\n\nLayers with trainable weights like `Embedding` need to be initialized with the signature (shape and dtype) of the input, and then can be run by calling them.\n\n\n\n```python\nfrom trax import layers as tl\n\n# Create an input tensor x.\nx = np.arange(15)\nprint(f'x = {x}')\n\n# Create the embedding layer.\nembedding = tl.Embedding(vocab_size=20, d_feature=32)\nembedding.init(trax.shapes.signature(x))\n\n# Run the layer -- y = embedding(x).\ny = embedding(x)\nprint(f'shape of y = {y.shape}')\n```\n\n    x = [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n    shape of y = (15, 32)\n\n\n### Models\n\nModels in Trax are built from layers most often using the `Serial` and `Branch` combinators. You can read more about those combinators in the [layers intro](https://trax-ml.readthedocs.io/en/latest/notebooks/layers_intro.html) and\nsee the code for many models in `trax/models/`, e.g., this is how the [Transformer Language Model](https://github.com/google/trax/blob/master/trax/models/transformer.py#L167) is implemented. Below is an example of how to build a sentiment classification model.\n\n\n```python\nmodel = tl.Serial(\n    tl.Embedding(vocab_size=8192, d_feature=256),\n    tl.Mean(axis=1),  # Average on axis 1 (length of sentence).\n    tl.Dense(2),      # Classify 2 classes.\n    tl.LogSoftmax()   # Produce log-probabilities.\n)\n\n# You can print model structure.\nprint(model)\n```\n\n    Serial[\n      Embedding_8192_256\n      Mean\n      Dense_2\n      LogSoftmax\n    ]\n\n\n### Data\n\nTo train your model, you need data. In Trax, data streams are represented as python iterators, so you can call `next(data_stream)` and get a tuple, e.g., `(inputs, targets)`. Trax allows you to use [TensorFlow Datasets](https://www.tensorflow.org/datasets) easily and you can also get an iterator from your own text file using the standard `open('my_file.txt')`.\n\n\n```python\ntrain_stream = trax.data.TFDS('imdb_reviews', keys=('text', 'label'), train=True)()\neval_stream = trax.data.TFDS('imdb_reviews', keys=('text', 'label'), train=False)()\nprint(next(train_stream))  # See one example.\n```\n\n    (b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", 0)\n\n\nUsing the `trax.data` module you can create input processing pipelines, e.g., to tokenize and shuffle your data. You create data pipelines using `trax.data.Serial` and they are functions that you apply to streams to create processed streams.\n\n\n```python\ndata_pipeline = trax.data.Serial(\n    trax.data.Tokenize(vocab_file='en_8k.subword', keys=[0]),\n    trax.data.Shuffle(),\n    trax.data.FilterByLength(max_length=2048, length_keys=[0]),\n    trax.data.BucketByLength(boundaries=[  32, 128, 512, 2048],\n                             batch_sizes=[256,  64,  16,    4, 1],\n                             length_keys=[0]),\n    trax.data.AddLossWeights()\n  )\ntrain_batches_stream = data_pipeline(train_stream)\neval_batches_stream = data_pipeline(eval_stream)\nexample_batch = next(train_batches_stream)\nprint(f'shapes = {[x.shape for x in example_batch]}')  # Check the shapes.\n```\n\n    shapes = [(4, 1024), (4,), (4,)]\n\n\n### Supervised training\n\nWhen you have the model and the data, use `trax.supervised.training` to define training and eval tasks and create a training loop. The Trax training loop optimizes training and will create TensorBoard logs and model checkpoints for you.\n\n\n```python\nfrom trax.supervised import training\n\n# Training task.\ntrain_task = training.TrainTask(\n    labeled_data=train_batches_stream,\n    loss_layer=tl.WeightedCategoryCrossEntropy(),\n    optimizer=trax.optimizers.Adam(0.01),\n    n_steps_per_checkpoint=500,\n)\n\n# Evaluaton task.\neval_task = training.EvalTask(\n    labeled_data=eval_batches_stream,\n    metrics=[tl.WeightedCategoryCrossEntropy(), tl.WeightedCategoryAccuracy()],\n    n_eval_batches=20  # For less variance in eval numbers.\n)\n\n# Training loop saves checkpoints to output_dir.\noutput_dir = os.path.expanduser('~/output_dir/')\n!rm -rf {output_dir}\ntraining_loop = training.Loop(model,\n                              train_task,\n                              eval_tasks=[eval_task],\n                              output_dir=output_dir)\n\n# Run 2000 steps (batches).\ntraining_loop.run(2000)\n```\n\n    \n    Step      1: Ran 1 train steps in 0.78 secs\n    Step      1: train WeightedCategoryCrossEntropy |  1.33800304\n    Step      1: eval  WeightedCategoryCrossEntropy |  0.71843582\n    Step      1: eval      WeightedCategoryAccuracy |  0.56562500\n    \n    Step    500: Ran 499 train steps in 5.77 secs\n    Step    500: train WeightedCategoryCrossEntropy |  0.62914723\n    Step    500: eval  WeightedCategoryCrossEntropy |  0.49253047\n    Step    500: eval      WeightedCategoryAccuracy |  0.74062500\n    \n    Step   1000: Ran 500 train steps in 5.03 secs\n    Step   1000: train WeightedCategoryCrossEntropy |  0.42949259\n    Step   1000: eval  WeightedCategoryCrossEntropy |  0.35451687\n    Step   1000: eval      WeightedCategoryAccuracy |  0.83750000\n    \n    Step   1500: Ran 500 train steps in 4.80 secs\n    Step   1500: train WeightedCategoryCrossEntropy |  0.41843575\n    Step   1500: eval  WeightedCategoryCrossEntropy |  0.35207348\n    Step   1500: eval      WeightedCategoryAccuracy |  0.82109375\n    \n    Step   2000: Ran 500 train steps in 5.35 secs\n    Step   2000: train WeightedCategoryCrossEntropy |  0.38129005\n    Step   2000: eval  WeightedCategoryCrossEntropy |  0.33760912\n    Step   2000: eval      WeightedCategoryAccuracy |  0.85312500\n\n\nAfter training the model, run it like any layer to get results.\n\n\n```python\nexample_input = next(eval_batches_stream)[0][0]\nexample_input_str = trax.data.detokenize(example_input, vocab_file='en_8k.subword')\nprint(f'example input_str: {example_input_str}')\nsentiment_log_probs = model(example_input[None, :])  # Add batch dimension.\nprint(f'Model returned sentiment probabilities: {np.exp(sentiment_log_probs)}')\n```\n\n    example input_str: I first saw this when I was a teen in my last year of Junior High. I was riveted to it! I loved the special effects, the fantastic places and the trial-aspect and flashback method of telling the story.<br /><br />Several years later I read the book and while it was interesting and I could definitely see what Swift was trying to say, I think that while it's not as perfect as the book for social commentary, as a story the movie is better. It makes more sense to have it be one long adventure than having Gulliver return after each voyage and making a profit by selling the tiny Lilliput sheep or whatever.<br /><br />It's much more arresting when everyone thinks he's crazy and the sheep DO make a cameo anyway. As a side note, when I saw Laputa I was stunned. It looks very much like the Kingdom of Zeal from the Chrono Trigger video game (1995) that also made me like this mini-series even more.<br /><br />I saw it again about 4 years ago, and realized that I still enjoyed it just as much. Really high quality stuff and began an excellent run of Sweeps mini-series for NBC who followed it up with the solid Merlin and interesting Alice in Wonderland.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n    Model returned sentiment probabilities: [[3.984500e-04 9.996014e-01]]\n",
        "releases": [
            {
                "name": "v1.4.1",
                "date": "2021-10-26T20:29:38Z"
            },
            {
                "name": "v1.4.0",
                "date": "2021-10-26T20:28:56Z"
            },
            {
                "name": "v1.3.9",
                "date": "2021-05-21T19:54:43Z"
            },
            {
                "name": "v1.3.8",
                "date": "2021-04-26T19:17:01Z"
            },
            {
                "name": "v1.3.7",
                "date": "2020-12-18T17:06:24Z"
            },
            {
                "name": "v1.3.6",
                "date": "2020-10-21T02:32:42Z"
            },
            {
                "name": "v1.3.5",
                "date": "2020-09-19T22:39:13Z"
            },
            {
                "name": "v1.3.4",
                "date": "2020-09-01T13:54:29Z"
            },
            {
                "name": "v1.3.3",
                "date": "2020-07-26T05:35:55Z"
            },
            {
                "name": "v1.3.2",
                "date": "2020-07-24T23:11:12Z"
            },
            {
                "name": "v1.3.1",
                "date": "2020-07-02T00:43:01Z"
            },
            {
                "name": "v1.3.0",
                "date": "2020-06-30T06:29:28Z"
            },
            {
                "name": "v1.2.4",
                "date": "2020-04-18T18:06:59Z"
            },
            {
                "name": "v1.2.3",
                "date": "2020-02-25T17:00:50Z"
            },
            {
                "name": "v1.2.2",
                "date": "2020-01-17T17:30:21Z"
            },
            {
                "name": "v1.2.1",
                "date": "2020-01-16T21:19:28Z"
            },
            {
                "name": "v1.2.0",
                "date": "2020-01-15T23:44:49Z"
            },
            {
                "name": "v1.1.2",
                "date": "2019-11-23T08:26:30Z"
            }
        ]
    }
}