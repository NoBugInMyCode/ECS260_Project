{
    "https://api.github.com/repos/tonybeltramelli/pix2code": {
        "forks": 1444,
        "watchers": 11983,
        "stars": 11983,
        "languages": {
            "Python": 41880,
            "Shell": 1443
        },
        "commits": [
            "2021-01-24T15:23:41Z",
            "2020-03-16T14:25:59Z",
            "2020-03-15T11:58:16Z",
            "2017-12-13T08:36:55Z",
            "2017-12-04T18:18:16Z",
            "2017-12-02T22:52:05Z",
            "2017-12-02T22:47:22Z",
            "2017-10-16T13:37:09Z",
            "2017-10-16T13:36:46Z",
            "2017-10-16T12:36:40Z",
            "2017-10-03T03:06:07Z",
            "2017-10-02T11:29:39Z",
            "2017-10-02T03:05:04Z",
            "2017-10-02T03:03:58Z",
            "2017-10-02T02:46:49Z",
            "2017-10-02T02:45:20Z",
            "2017-09-25T14:19:39Z",
            "2017-09-23T07:11:57Z",
            "2017-09-19T14:27:08Z",
            "2017-09-19T13:36:13Z",
            "2017-05-30T20:45:42Z",
            "2017-05-30T09:38:19Z",
            "2017-05-25T10:10:50Z",
            "2017-05-24T06:21:59Z",
            "2017-05-24T05:59:43Z",
            "2017-05-24T05:51:41Z"
        ],
        "creation_date": "2017-05-24T05:51:41Z",
        "contributors": 3,
        "topics": [
            "datasets",
            "deep-learning",
            "deep-neural-networks",
            "front-end-development",
            "graphical-user-interface"
        ],
        "subscribers": 1141,
        "readme": "# pix2code\n*Generating Code from a Graphical User Interface Screenshot*\n\n[![License](http://img.shields.io/badge/license-APACHE2-blue.svg)](LICENSE.txt)\n\n* A video demo of the system can be seen [here](https://youtu.be/pqKeXkhFA3I)\n* The paper is available at [https://arxiv.org/abs/1705.07962](https://arxiv.org/abs/1705.07962)\n* Official research page: [https://uizard.io/research#pix2code](https://uizard.io/research#pix2code)\n\n## Abstract\nTransforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites, and mobile applications. In this paper, we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77% of accuracy for three different platforms (i.e. iOS, Android and web-based technologies).\n\n## Citation\n\n```\n@article{beltramelli2017pix2code,\n  title={pix2code: Generating Code from a Graphical User Interface Screenshot},\n  author={Beltramelli, Tony},\n  journal={arXiv preprint arXiv:1705.07962},\n  year={2017}\n}\n```\n\n## Disclaimer\n\nThe following software is shared for educational purposes only. The author and its affiliated institution are not responsible in any manner whatsoever for any damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of the use or inability to use this software.\n\nThe project pix2code is a research project demonstrating an application of deep neural networks to generate code from visual inputs.\nThe current implementation is not, in any way, intended, nor able to generate code in a real-world context.\nWe could not emphasize enough that this project is experimental and shared for educational purposes only.\nBoth the source code and the datasets are provided to foster future research in machine intelligence and are not designed for end users.\n\n## Setup\n### Prerequisites\n\n- Python 2 or 3\n- pip\n\n### Install dependencies\n\n```sh\npip install -r  requirements.txt\n```\n\n## Usage\n\nPrepare the data:\n```sh\n# reassemble and unzip the data\ncd datasets\nzip -F pix2code_datasets.zip --out datasets.zip\nunzip datasets.zip\n\ncd ../model\n\n# split training set and evaluation set while ensuring no training example in the evaluation set\n# usage: build_datasets.py <input path> <distribution (default: 6)>\n./build_datasets.py ../datasets/ios/all_data\n./build_datasets.py ../datasets/android/all_data\n./build_datasets.py ../datasets/web/all_data\n\n# transform images (normalized pixel values and resized pictures) in training dataset to numpy arrays (smaller files if you need to upload the set to train your model in the cloud)\n# usage: convert_imgs_to_arrays.py <input path> <output path>\n./convert_imgs_to_arrays.py ../datasets/ios/training_set ../datasets/ios/training_features\n./convert_imgs_to_arrays.py ../datasets/android/training_set ../datasets/android/training_features\n./convert_imgs_to_arrays.py ../datasets/web/training_set ../datasets/web/training_features\n```\n\nTrain the model:\n```sh\nmkdir bin\ncd model\n\n# provide input path to training data and output path to save trained model and metadata\n# usage: train.py <input path> <output path> <is memory intensive (default: 0)> <pretrained weights (optional)>\n./train.py ../datasets/web/training_set ../bin\n\n# train on images pre-processed as arrays\n./train.py ../datasets/web/training_features ../bin\n\n# train with generator to avoid having to fit all the data in memory (RECOMMENDED)\n./train.py ../datasets/web/training_features ../bin 1\n\n# train on top of pretrained weights\n./train.py ../datasets/web/training_features ../bin 1 ../bin/pix2code.h5\n```\n\nGenerate code for batch of GUIs:\n```sh\nmkdir code\ncd model\n\n# generate DSL code (.gui file), the default search method is greedy\n# usage: generate.py <trained weights path> <trained model name> <input image> <output path> <search method (default: greedy)>\n./generate.py ../bin pix2code ../gui_screenshots ../code\n\n# equivalent to command above\n./generate.py ../bin pix2code ../gui_screenshots ../code greedy\n\n# generate DSL code with beam search and a beam width of size 3\n./generate.py ../bin pix2code ../gui_screenshots ../code 3\n```\n\nGenerate code for a single GUI image:\n```sh\nmkdir code\ncd model\n\n# generate DSL code (.gui file), the default search method is greedy\n# usage: sample.py <trained weights path> <trained model name> <input image> <output path> <search method (default: greedy)>\n./sample.py ../bin pix2code ../test_gui.png ../code\n\n# equivalent to command above\n./sample.py ../bin pix2code ../test_gui.png ../code greedy\n\n# generate DSL code with beam search and a beam width of size 3\n./sample.py ../bin pix2code ../test_gui.png ../code 3\n```\n\nCompile generated code to target language:\n```sh\ncd compiler\n\n# compile .gui file to Android XML UI\n./android-compiler.py <input file path>.gui\n\n# compile .gui file to iOS Storyboard\n./ios-compiler.py <input file path>.gui\n\n# compile .gui file to HTML/CSS (Bootstrap style)\n./web-compiler.py <input file path>.gui\n```\n\n## FAQ\n\n### Will pix2code supports other target platforms/languages?\nNo, pix2code is only a research project and will stay in the state described in the paper for consistency reasons.\nThis project is really just a toy example but you are of course more than welcome to fork the repo and experiment yourself with other target platforms/languages.\n\n### Will I be able to use pix2code for my own frontend projects?\nNo, pix2code is experimental and won't work for your specific use cases.\n\n### How is the model performance measured?\nThe accuracy/error reported in the paper is measured at the DSL level by comparing each generated token with each expected token.\nAny difference in length between the generated token sequence and the expected token sequence is also counted as error.\n\n### How long does it take to train the model?\nOn a Nvidia Tesla K80 GPU, it takes a little less than 5 hours to optimize the 109 * 10^6 parameters for one dataset; so expect around 15 hours if you want to train the model for the three target platforms.\n\n### I am a front-end developer, will I soon lose my job?\n*(I have genuinely been asked this question multiple times)*\n\n**TL;DR** Not anytime soon will AI replace front-end developers.\n\nEven assuming a mature version of pix2code able to generate GUI code with 100% accuracy for every platforms/languages in the universe, front-enders will still be needed to implement the logic, the interactive parts, the advanced graphics and animations, and all the features users love. The product we are building at [Uizard Technologies](https://uizard.io) is intended to bridge the gap between UI/UX designers and front-end developers, not replace any of them. We want to rethink the traditional workflow that too often results in more frustration than innovation. We want designers to be as creative as possible to better serve end users, and developers to dedicate their time programming the core functionality and forget about repetitive tasks such as UI implementation. We believe in a future where AI collaborate with humans, not replace humans.\n\n## Media coverage\n\n* [Wired UK](http://www.wired.co.uk/article/pix2code-ulzard-technologies)\n* [The Next Web](https://thenextweb.com/apps/2017/05/26/ai-raw-design-turn-source-code)\n* [Fast Company](https://www.fastcodesign.com/90127911/this-startup-uses-machine-learning-to-turn-ui-designs-into-raw-code)\n* [NVIDIA Developer News](https://news.developer.nvidia.com/ai-turns-ui-designs-into-code)\n* [Lifehacker Australia](https://www.lifehacker.com.au/2017/05/generating-user-interface-code-from-images-using-machine-learning/)\n* [Two Minute Papers](https://www.youtube.com/watch?v=Fevg4aowNyc) (web series)\n* [NLP Highlights](https://soundcloud.com/nlp-highlights/17a) (podcast)\n* [Data Skeptic](https://dataskeptic.com/blog/episodes/2017/pix2code) (podcast)\n* Read comments on [Hacker News](https://news.ycombinator.com/item?id=14416530)\n",
        "releases": []
    }
}