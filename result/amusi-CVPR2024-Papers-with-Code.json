{
    "https://api.github.com/repos/amusi/CVPR2023-Papers-with-Code": {
        "forks": 2621,
        "watchers": 18737,
        "stars": 18737,
        "languages": {},
        "commits": [
            "2024-07-04T10:00:08Z",
            "2024-04-21T08:20:03Z",
            "2024-04-06T06:48:26Z",
            "2024-03-23T08:09:47Z",
            "2024-03-18T06:50:19Z",
            "2024-03-12T13:10:01Z",
            "2024-03-08T07:21:36Z",
            "2024-03-06T04:18:41Z",
            "2024-03-03T09:30:02Z",
            "2024-03-01T09:41:24Z",
            "2024-02-29T07:18:48Z",
            "2024-02-28T03:22:15Z",
            "2024-02-27T03:53:39Z",
            "2024-02-27T03:52:20Z",
            "2023-05-30T07:14:44Z",
            "2023-05-29T07:42:17Z",
            "2023-05-29T07:30:49Z",
            "2023-05-22T09:53:49Z",
            "2023-05-10T12:26:42Z",
            "2023-05-10T12:23:22Z",
            "2023-05-10T09:21:07Z",
            "2023-04-11T15:00:26Z",
            "2023-04-10T02:23:09Z",
            "2023-04-09T02:19:21Z",
            "2023-04-08T10:15:14Z",
            "2023-04-07T14:04:09Z",
            "2023-04-01T07:16:16Z",
            "2023-03-22T14:15:36Z",
            "2023-03-20T14:19:36Z",
            "2023-03-18T10:27:23Z"
        ],
        "creation_date": "2020-02-26T06:04:25Z",
        "contributors": 4,
        "topics": [
            "computer-vision",
            "cvpr",
            "cvpr2020",
            "cvpr2021",
            "cvpr2022",
            "cvpr2023",
            "cvpr2024",
            "deep-learning",
            "image-processing",
            "image-segmentation",
            "machine-learning",
            "object-detection",
            "paper",
            "python",
            "semantic-segmentation",
            "transformer",
            "transformers",
            "visual-tracking"
        ],
        "subscribers": 291,
        "readme": "# CVPR 2024 \u8bba\u6587\u548c\u5f00\u6e90\u9879\u76ee\u5408\u96c6(Papers with Code)\n\nCVPR 2024 decisions are now available on OpenReview\uff01\n\n\n> \u6ce81\uff1a\u6b22\u8fce\u5404\u4f4d\u5927\u4f6c\u63d0\u4ea4issue\uff0c\u5206\u4eabCVPR 2024\u8bba\u6587\u548c\u5f00\u6e90\u9879\u76ee\uff01\n>\n> \u6ce82\uff1a\u5173\u4e8e\u5f80\u5e74CV\u9876\u4f1a\u8bba\u6587\u4ee5\u53ca\u5176\u4ed6\u4f18\u8d28CV\u8bba\u6587\u548c\u5927\u76d8\u70b9\uff0c\u8be6\u89c1\uff1a https://github.com/amusi/daily-paper-computer-vision\n>\n> - [ECCV 2024](https://github.com/amusi/ECCV2024-Papers-with-Code)\n> - [CVPR 2023](CVPR2022-Papers-with-Code.md)\n\n\u6b22\u8fce\u626b\u7801\u52a0\u5165\u3010CVer\u5b66\u672f\u4ea4\u6d41\u7fa4\u3011\uff0c\u8fd9\u662f\u6700\u5927\u7684\u8ba1\u7b97\u673a\u89c6\u89c9AI\u77e5\u8bc6\u661f\u7403\uff01\u6bcf\u65e5\u66f4\u65b0\uff0c\u7b2c\u4e00\u65f6\u95f4\u5206\u4eab\u6700\u65b0\u6700\u524d\u6cbf\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u3001AI\u7ed8\u753b\u3001\u56fe\u50cf\u5904\u7406\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u81ea\u52a8\u9a7e\u9a76\u3001\u533b\u7597\u5f71\u50cf\u548cAIGC\u7b49\u65b9\u5411\u7684\u5b66\u4e60\u8d44\u6599\uff0c\u5b66\u8d77\u6765\uff01\n\n![](CVer\u5b66\u672f\u4ea4\u6d41\u7fa4.png)\n\n# \u3010CVPR 2024 \u8bba\u6587\u5f00\u6e90\u76ee\u5f55\u3011\n\n- [3DGS(Gaussian Splatting)](#3DGS)\n- [Avatars](#Avatars)\n- [Backbone](#Backbone)\n- [CLIP](#CLIP)\n- [MAE](#MAE)\n- [Embodied AI](#Embodied-AI)\n- [GAN](#GAN)\n- [GNN](#GNN)\n- [\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLM)](#MLLM)\n- [\u5927\u8bed\u8a00\u6a21\u578b(LLM)](#LLM)\n- [NAS](#NAS)\n- [OCR](#OCR)\n- [NeRF](#NeRF)\n- [DETR](#DETR)\n- [Prompt](#Prompt)\n- [\u6269\u6563\u6a21\u578b(Diffusion Models)](#Diffusion)\n- [ReID(\u91cd\u8bc6\u522b)](#ReID)\n- [\u957f\u5c3e\u5206\u5e03(Long-Tail)](#Long-Tail)\n- [Vision Transformer](#Vision-Transformer)\n- [\u89c6\u89c9\u548c\u8bed\u8a00(Vision-Language)](#VL)\n- [\u81ea\u76d1\u7763\u5b66\u4e60(Self-supervised Learning)](#SSL)\n- [\u6570\u636e\u589e\u5f3a(Data Augmentation)](#DA)\n- [\u76ee\u6807\u68c0\u6d4b(Object Detection)](#Object-Detection)\n- [\u5f02\u5e38\u68c0\u6d4b(Anomaly Detection)](#Anomaly-Detection)\n- [\u76ee\u6807\u8ddf\u8e2a(Visual Tracking)](#VT)\n- [\u8bed\u4e49\u5206\u5272(Semantic Segmentation)](#Semantic-Segmentation)\n- [\u5b9e\u4f8b\u5206\u5272(Instance Segmentation)](#Instance-Segmentation)\n- [\u5168\u666f\u5206\u5272(Panoptic Segmentation)](#Panoptic-Segmentation)\n- [\u533b\u5b66\u56fe\u50cf(Medical Image)](#MI)\n- [\u533b\u5b66\u56fe\u50cf\u5206\u5272(Medical Image Segmentation)](#MIS)\n- [\u89c6\u9891\u76ee\u6807\u5206\u5272(Video Object Segmentation)](#VOS)\n- [\u89c6\u9891\u5b9e\u4f8b\u5206\u5272(Video Instance Segmentation)](#VIS)\n- [\u53c2\u8003\u56fe\u50cf\u5206\u5272(Referring Image Segmentation)](#RIS)\n- [\u56fe\u50cf\u62a0\u56fe(Image Matting)](#Matting)\n- [\u56fe\u50cf\u7f16\u8f91(Image Editing)](#Image-Editing)\n- [Low-level Vision](#LLV)\n- [\u8d85\u5206\u8fa8\u7387(Super-Resolution)](#SR)\n- [\u53bb\u566a(Denoising)](#Denoising)\n- [\u53bb\u6a21\u7cca(Deblur)](#Deblur)\n- [\u81ea\u52a8\u9a7e\u9a76(Autonomous Driving)](#Autonomous-Driving)\n- [3D\u70b9\u4e91(3D Point Cloud)](#3D-Point-Cloud)\n- [3D\u76ee\u6807\u68c0\u6d4b(3D Object Detection)](#3DOD)\n- [3D\u8bed\u4e49\u5206\u5272(3D Semantic Segmentation)](#3DSS)\n- [3D\u76ee\u6807\u8ddf\u8e2a(3D Object Tracking)](#3D-Object-Tracking)\n- [3D\u8bed\u4e49\u573a\u666f\u8865\u5168(3D Semantic Scene Completion)](#3DSSC)\n- [3D\u914d\u51c6(3D Registration)](#3D-Registration)\n- [3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1(3D Human Pose Estimation)](#3D-Human-Pose-Estimation)\n- [3D\u4eba\u4f53Mesh\u4f30\u8ba1(3D Human Mesh Estimation)](#3D-Human-Pose-Estimation)\n- [\u533b\u5b66\u56fe\u50cf(Medical Image)](#Medical-Image)\n- [\u56fe\u50cf\u751f\u6210(Image Generation)](#Image-Generation)\n- [\u89c6\u9891\u751f\u6210(Video Generation)](#Video-Generation)\n- [3D\u751f\u6210(3D Generation)](#3D-Generation)\n- [\u89c6\u9891\u7406\u89e3(Video Understanding)](#Video-Understanding)\n- [\u884c\u4e3a\u68c0\u6d4b(Action Detection)](#Action-Detection)\n- [\u6587\u672c\u68c0\u6d4b(Text Detection)](#Text-Detection)\n- [\u77e5\u8bc6\u84b8\u998f(Knowledge Distillation)](#KD)\n- [\u6a21\u578b\u526a\u679d(Model Pruning)](#Pruning)\n- [\u56fe\u50cf\u538b\u7f29(Image Compression)](#IC)\n- [\u4e09\u7ef4\u91cd\u5efa(3D Reconstruction)](#3D-Reconstruction)\n- [\u6df1\u5ea6\u4f30\u8ba1(Depth Estimation)](#Depth-Estimation)\n- [\u8f68\u8ff9\u9884\u6d4b(Trajectory Prediction)](#TP)\n- [\u8f66\u9053\u7ebf\u68c0\u6d4b(Lane Detection)](#Lane-Detection)\n- [\u56fe\u50cf\u63cf\u8ff0(Image Captioning)](#Image-Captioning)\n- [\u89c6\u89c9\u95ee\u7b54(Visual Question Answering)](#VQA)\n- [\u624b\u8bed\u8bc6\u522b(Sign Language Recognition)](#SLR)\n- [\u89c6\u9891\u9884\u6d4b(Video Prediction)](#Video-Prediction)\n- [\u65b0\u89c6\u70b9\u5408\u6210(Novel View Synthesis)](#NVS)\n- [Zero-Shot Learning(\u96f6\u6837\u672c\u5b66\u4e60)](#ZSL)\n- [\u7acb\u4f53\u5339\u914d(Stereo Matching)](#Stereo-Matching)\n- [\u7279\u5f81\u5339\u914d(Feature Matching)](#Feature-Matching)\n- [\u573a\u666f\u56fe\u751f\u6210(Scene Graph Generation)](#SGG)\n- [\u9690\u5f0f\u795e\u7ecf\u8868\u793a(Implicit Neural Representations)](#INR)\n- [\u56fe\u50cf\u8d28\u91cf\u8bc4\u4ef7(Image Quality Assessment)](#IQA)\n- [\u89c6\u9891\u8d28\u91cf\u8bc4\u4ef7(Video Quality Assessment)](#Video-Quality-Assessment)\n- [\u6570\u636e\u96c6(Datasets)](#Datasets)\n- [\u65b0\u4efb\u52a1(New Tasks)](#New-Tasks)\n- [\u5176\u4ed6(Others)](#Others)\n\n<a name=\"3DGS\"></a>\n\n# 3DGS(Gaussian Splatting)\n\n**Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering**\n\n- Homepage: https://city-super.github.io/scaffold-gs/\n- Paper: https://arxiv.org/abs/2312.00109\n- Code: https://github.com/city-super/Scaffold-GS\n\n**GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis**\n\n- Homepage: https://shunyuanzheng.github.io/GPS-Gaussian \n- Paper: https://arxiv.org/abs/2312.02155\n- Code: https://github.com/ShunyuanZheng/GPS-Gaussian\n\n**GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians**\n\n- Paper: https://arxiv.org/abs/2312.02134\n- Code: https://github.com/huliangxiao/GaussianAvatar\n\n**GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting**\n\n- Paper: https://arxiv.org/abs/2311.14521\n- Code: https://github.com/buaacyw/GaussianEditor \n\n**Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction**\n\n- Homepage: https://ingra14m.github.io/Deformable-Gaussians/ \n- Paper: https://arxiv.org/abs/2309.13101\n- Code: https://github.com/ingra14m/Deformable-3D-Gaussians\n\n**SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes**\n\n- Homepage: https://yihua7.github.io/SC-GS-web/ \n- Paper: https://arxiv.org/abs/2312.14937\n- Code: https://github.com/yihua7/SC-GS\n\n**Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis**\n\n- Homepage: https://oppo-us-research.github.io/SpacetimeGaussians-website/ \n- Paper: https://arxiv.org/abs/2312.16812\n- Code: https://github.com/oppo-us-research/SpacetimeGaussians\n\n**DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization**\n\n- Homepage: https://fictionarry.github.io/DNGaussian/\n- Paper: https://arxiv.org/abs/2403.06912\n- Code: https://github.com/Fictionarry/DNGaussian\n\n**4D Gaussian Splatting for Real-Time Dynamic Scene Rendering**\n\n- Paper: https://arxiv.org/abs/2310.08528\n- Code: https://github.com/hustvl/4DGaussians\n\n**GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models**\n\n- Paper: https://arxiv.org/abs/2310.08529\n- Code: https://github.com/hustvl/GaussianDreamer\n\n<a name=\"Avatars\"></a>\n\n# Avatars\n\n**GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians**\n\n- Paper: https://arxiv.org/abs/2312.02134\n- Code: https://github.com/huliangxiao/GaussianAvatar\n\n**Real-Time Simulated Avatar from Head-Mounted Sensors**\n\n- Homepage: https://www.zhengyiluo.com/SimXR/\n- Paper: https://arxiv.org/abs/2403.06862\n\n<a name=\"Backbone\"></a>\n\n# Backbone\n\n**RepViT: Revisiting Mobile CNN From ViT Perspective**\n\n- Paper: https://arxiv.org/abs/2307.09283\n- Code: https://github.com/THU-MIG/RepViT\n\n**TransNeXt: Robust Foveal Visual Perception for Vision Transformers**\n\n- Paper: https://arxiv.org/abs/2311.17132\n- Code: https://github.com/DaiShiResearch/TransNeXt\n\n<a name=\"CLIP\"></a>\n\n# CLIP\n\n**Alpha-CLIP: A CLIP Model Focusing on Wherever You Want**\n\n- Paper: https://arxiv.org/abs/2312.03818\n- Code: https://github.com/SunzeY/AlphaCLIP\n\n**FairCLIP: Harnessing Fairness in Vision-Language Learning**\n\n- Paper: https://arxiv.org/abs/2403.19949\n- Code: https://github.com/Harvard-Ophthalmology-AI-Lab/FairCLIP\n\n<a name=\"MAE\"></a>\n\n# MAE\n\n<a name=\"Embodied-AI\"></a>\n\n# Embodied AI\n\n**EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI**\n\n- Homepage: https://tai-wang.github.io/embodiedscan/\n- Paper: https://arxiv.org/abs/2312.16170\n- Code: https://github.com/OpenRobotLab/EmbodiedScan\n\n**MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception**\n\n- Homepage: https://iranqin.github.io/MP5.github.io/ \n- Paper: https://arxiv.org/abs/2312.07472\n- Code: https://github.com/IranQin/MP5\n\n**LEMON: Learning 3D Human-Object Interaction Relation from 2D Images**\n\n- Paper: https://arxiv.org/abs/2312.08963\n- Code: https://github.com/yyvhang/lemon_3d \n\n<a name=\"GAN\"></a>\n\n# GAN\n\n<a name=\"OCR\"></a>\n\n# OCR\n\n**An Empirical Study of Scaling Law for OCR**\n\n- Paper: https://arxiv.org/abs/2401.00028\n- Code: https://github.com/large-ocr-model/large-ocr-model.github.io\n\n**ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting**\n\n- Paper: https://arxiv.org/abs/2403.00303\n- Code: https://github.com/PriNing/ODM \n\n<a name=\"NeRF\"></a>\n\n# NeRF\n\n**PIE-NeRF\ud83c\udf55: Physics-based Interactive Elastodynamics with NeRF**\n\n- Paper: https://arxiv.org/abs/2311.13099\n- Code: https://github.com/FYTalon/pienerf/ \n\n<a name=\"DETR\"></a>\n\n# DETR\n\n**DETRs Beat YOLOs on Real-time Object Detection**\n\n- Paper: https://arxiv.org/abs/2304.08069\n- Code: https://github.com/lyuwenyu/RT-DETR\n\n**Salience DETR: Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement**\n\n- Paper: https://arxiv.org/abs/2403.16131\n- Code: https://github.com/xiuqhou/Salience-DETR\n\n<a name=\"Prompt\"></a>\n\n# Prompt\n\n<a name=\"MLLM\"></a>\n\n# \u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLM)\n\n**mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration**\n\n- Paper: https://arxiv.org/abs/2311.04257\n- Code: https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2\n\n**Link-Context Learning for Multimodal LLMs**\n\n- Paper: https://arxiv.org/abs/2308.07891\n- Code: https://github.com/isekai-portal/Link-Context-Learning/tree/main \n\n**OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation**\n\n- Paper: https://arxiv.org/abs/2311.17911\n- Code: https://github.com/shikiw/OPERA\n\n**Making Large Multimodal Models Understand Arbitrary Visual Prompts**\n\n- Homepage: https://vip-llava.github.io/ \n- Paper: https://arxiv.org/abs/2312.00784\n\n**Pink: Unveiling the power of referential comprehension for multi-modal llms**\n\n- Paper: https://arxiv.org/abs/2310.00582\n- Code: https://github.com/SY-Xuan/Pink\n\n**Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding**\n\n- Paper: https://arxiv.org/abs/2311.08046\n- Code: https://github.com/PKU-YuanGroup/Chat-UniVi\n\n**OneLLM: One Framework to Align All Modalities with Language**\n\n- Paper: https://arxiv.org/abs/2312.03700\n- Code: https://github.com/csuhan/OneLLM\n\n<a name=\"LLM\"></a>\n\n# \u5927\u8bed\u8a00\u6a21\u578b(LLM)\n\n**VTimeLLM: Empower LLM to Grasp Video Moments**\n\n- Paper: https://arxiv.org/abs/2311.18445\n- Code: https://github.com/huangb23/VTimeLLM \n\n<a name=\"NAS\"></a>\n\n# NAS\n\n<a name=\"ReID\"></a>\n\n# ReID(\u91cd\u8bc6\u522b)\n\n**Magic Tokens: Select Diverse Tokens for Multi-modal Object Re-Identification**\n\n- Paper: https://arxiv.org/abs/2403.10254\n- Code: https://github.com/924973292/EDITOR \n\n**Noisy-Correspondence Learning for Text-to-Image Person Re-identification**\n\n- Paper: https://arxiv.org/abs/2308.09911\n\n- Code : https://github.com/QinYang79/RDE \n\n<a name=\"Diffusion\"></a>\n\n# \u6269\u6563\u6a21\u578b(Diffusion Models)\n\n**InstanceDiffusion: Instance-level Control for Image Generation**\n\n- Homepage: https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/\n\n- Paper: https://arxiv.org/abs/2402.03290\n- Code: https://github.com/frank-xwang/InstanceDiffusion\n\n**Residual Denoising Diffusion Models**\n\n- Paper: https://arxiv.org/abs/2308.13712\n- Code: https://github.com/nachifur/RDDM\n\n**DeepCache: Accelerating Diffusion Models for Free**\n\n- Paper: https://arxiv.org/abs/2312.00858\n- Code: https://github.com/horseee/DeepCache\n\n**DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations**\n\n- Homepage: https://tianhao-qi.github.io/DEADiff/ \n\n- Paper: https://arxiv.org/abs/2403.06951\n- Code: https://github.com/Tianhao-Qi/DEADiff_code\n\n**SVGDreamer: Text Guided SVG Generation with Diffusion Model**\n\n- Paper: https://arxiv.org/abs/2312.16476\n- Code: https://ximinng.github.io/SVGDreamer-project/\n\n**InteractDiffusion: Interaction-Control for Text-to-Image Diffusion Model**\n\n- Paper: https://arxiv.org/abs/2312.05849\n- Code: https://github.com/jiuntian/interactdiffusion\n\n**MMA-Diffusion: MultiModal Attack on Diffusion Models**\n\n- Paper: https://arxiv.org/abs/2311.17516\n- Code: https://github.com/yangyijune/MMA-Diffusion\n\n**VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models**\n\n- Homeoage: https://video-motion-customization.github.io/ \n- Paper: https://arxiv.org/abs/2312.00845\n- Code: https://github.com/HyeonHo99/Video-Motion-Customization\n\n<a name=\"Vision-Transformer\"></a>\n\n# Vision Transformer\n\n**TransNeXt: Robust Foveal Visual Perception for Vision Transformers**\n\n- Paper: https://arxiv.org/abs/2311.17132\n- Code: https://github.com/DaiShiResearch/TransNeXt\n\n**RepViT: Revisiting Mobile CNN From ViT Perspective**\n\n- Paper: https://arxiv.org/abs/2307.09283\n- Code: https://github.com/THU-MIG/RepViT\n\n**A General and Efficient Training for Transformer via Token Expansion**\n\n- Paper: https://arxiv.org/abs/2404.00672\n- Code: https://github.com/Osilly/TokenExpansion \n\n<a name=\"VL\"></a>\n\n# \u89c6\u89c9\u548c\u8bed\u8a00(Vision-Language)\n\n**PromptKD: Unsupervised Prompt Distillation for Vision-Language Models**\n\n- Paper: https://arxiv.org/abs/2403.02781\n- Code: https://github.com/zhengli97/PromptKD\n\n**FairCLIP: Harnessing Fairness in Vision-Language Learning**\n\n- Paper: https://arxiv.org/abs/2403.19949\n- Code: https://github.com/Harvard-Ophthalmology-AI-Lab/FairCLIP\n\n<a name=\"Object-Detection\"></a>\n\n# \u76ee\u6807\u68c0\u6d4b(Object Detection)\n\n**DETRs Beat YOLOs on Real-time Object Detection**\n\n- Paper: https://arxiv.org/abs/2304.08069\n- Code: https://github.com/lyuwenyu/RT-DETR\n\n**Boosting Object Detection with Zero-Shot Day-Night Domain Adaptation**\n\n- Paper: https://arxiv.org/abs/2312.01220\n- Code: https://github.com/ZPDu/Boosting-Object-Detection-with-Zero-Shot-Day-Night-Domain-Adaptation \n\n**YOLO-World: Real-Time Open-Vocabulary Object Detection**\n\n- Paper: https://arxiv.org/abs/2401.17270\n- Code: https://github.com/AILab-CVC/YOLO-World\n\n**Salience DETR: Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement**\n\n- Paper: https://arxiv.org/abs/2403.16131\n- Code: https://github.com/xiuqhou/Salience-DETR\n\n<a name=\"Anomaly-Detection\"></a>\n\n# \u5f02\u5e38\u68c0\u6d4b(Anomaly Detection)\n\n**Anomaly Heterogeneity Learning for Open-set Supervised Anomaly Detection**\n\n- Paper: https://arxiv.org/abs/2310.12790\n- Code: https://github.com/mala-lab/AHL\n\n<a name=\"VT\"></a>\n\n# \u76ee\u6807\u8ddf\u8e2a(Object Tracking)\n\n**Delving into the Trajectory Long-tail Distribution for Muti-object Tracking**\n\n- Paper: https://arxiv.org/abs/2403.04700\n- Code: https://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT \n\n<a name=\"Semantic-Segmentation\"></a>\n\n# \u8bed\u4e49\u5206\u5272(Semantic Segmentation)\n\n**Stronger, Fewer, & Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation**\n\n- Paper: https://arxiv.org/abs/2312.04265\n- Code: https://github.com/w1oves/Rein\n\n**SED: A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation**\n\n- Paper: https://arxiv.org/abs/2311.15537\n- Code: https://github.com/xb534/SED \n\n<a name=\"MI\"></a>\n\n# \u533b\u5b66\u56fe\u50cf(Medical Image)\n\n**Feature Re-Embedding: Towards Foundation Model-Level Performance in Computational Pathology**\n\n- Paper: https://arxiv.org/abs/2402.17228\n- Code: https://github.com/DearCaat/RRT-MIL\n\n**VoCo: A Simple-yet-Effective Volume Contrastive Learning Framework for 3D Medical Image Analysis**\n\n- Paper: https://arxiv.org/abs/2402.17300\n- Code: https://github.com/Luffy03/VoCo\n\n**ChAda-ViT : Channel Adaptive Attention for Joint Representation Learning of Heterogeneous Microscopy Images**\n\n- Paper: https://arxiv.org/abs/2311.15264\n- Code: https://github.com/nicoboou/chada_vit \n\n<a name=\"MIS\"></a>\n\n# \u533b\u5b66\u56fe\u50cf\u5206\u5272(Medical Image Segmentation)\n\n\n\n<a name=\"Autonomous-Driving\"></a>\n\n# \u81ea\u52a8\u9a7e\u9a76(Autonomous Driving)\n\n**UniPAD: A Universal Pre-training Paradigm for Autonomous Driving**\n\n- Paper: https://arxiv.org/abs/2310.08370\n- Code: https://github.com/Nightmare-n/UniPAD\n\n**Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications**\n\n- Paper: https://arxiv.org/abs/2311.17663\n- Code: https://github.com/haomo-ai/Cam4DOcc\n\n**Memory-based Adapters for Online 3D Scene Perception**\n\n- Paper: https://arxiv.org/abs/2403.06974\n- Code: https://github.com/xuxw98/Online3D\n\n**Symphonize 3D Semantic Scene Completion with Contextual Instance Queries**\n\n- Paper: https://arxiv.org/abs/2306.15670\n- Code: https://github.com/hustvl/Symphonies\n\n**A Real-world Large-scale Dataset for Roadside Cooperative Perception**\n\n- Paper: https://arxiv.org/abs/2403.10145\n- Code: https://github.com/AIR-THU/DAIR-RCooper\n\n**Adaptive Fusion of Single-View and Multi-View Depth for Autonomous Driving**\n\n- Paper: https://arxiv.org/abs/2403.07535\n- Code: https://github.com/Junda24/AFNet\n\n**Traffic Scene Parsing through the TSP6K Dataset**\n\n- Paper: https://arxiv.org/pdf/2303.02835.pdf\n- Code: https://github.com/PengtaoJiang/TSP6K \n\n<a name=\"3D-Point-Cloud\"></a>\n\n# 3D\u70b9\u4e91(3D-Point-Cloud)\n\n\n\n<a name=\"3DOD\"></a>\n\n# 3D\u76ee\u6807\u68c0\u6d4b(3D Object Detection)\n\n**PTT: Point-Trajectory Transformer for Efficient Temporal 3D Object Detection**\n\n- Paper: https://arxiv.org/abs/2312.08371\n- Code: https://github.com/kuanchihhuang/PTT\n\n**UniMODE: Unified Monocular 3D Object Detection**\n\n- Paper: https://arxiv.org/abs/2402.18573\n\n<a name=\"3DOD\"></a>\n\n# 3D\u8bed\u4e49\u5206\u5272(3D Semantic Segmentation)\n\n<a name=\"Image-Editing\"></a>\n\n# \u56fe\u50cf\u7f16\u8f91(Image Editing)\n\n**Edit One for All: Interactive Batch Image Editing**\n\n- Homepage: https://thaoshibe.github.io/edit-one-for-all \n- Paper: https://arxiv.org/abs/2401.10219\n- Code: https://github.com/thaoshibe/edit-one-for-all\n\n<a name=\"Video-Editing\"></a>\n\n# \u89c6\u9891\u7f16\u8f91(Video Editing)\n\n**MaskINT: Video Editing via Interpolative Non-autoregressive Masked Transformers**\n\n- Homepage:  [https://maskint.github.io](https://maskint.github.io/) \n\n- Paper: https://arxiv.org/abs/2312.12468\n\n<a name=\"LLV\"></a>\n\n# Low-level Vision\n\n**Residual Denoising Diffusion Models**\n\n- Paper: https://arxiv.org/abs/2308.13712\n- Code: https://github.com/nachifur/RDDM\n\n**Boosting Image Restoration via Priors from Pre-trained Models**\n\n- Paper: https://arxiv.org/abs/2403.06793\n\n<a name=\"SR\"></a>\n\n# \u8d85\u5206\u8fa8\u7387(Super-Resolution)\n\n**SeD: Semantic-Aware Discriminator for Image Super-Resolution**\n\n- Paper: https://arxiv.org/abs/2402.19387\n- Code: https://github.com/lbc12345/SeD\n\n**APISR: Anime Production Inspired Real-World Anime Super-Resolution**\n\n- Paper: https://arxiv.org/abs/2403.01598\n- Code: https://github.com/Kiteretsu77/APISR \n\n<a name=\"Denoising\"></a>\n\n# \u53bb\u566a(Denoising)\n\n## \u56fe\u50cf\u53bb\u566a(Image Denoising)\n\n<a name=\"3D-Human-Pose-Estimation\"></a>\n\n# 3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1(3D Human Pose Estimation)\n\n**Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation**\n\n- Paper: https://arxiv.org/abs/2311.12028\n- Code: https://github.com/NationalGAILab/HoT \n\n<a name=\"Image-Generation\"></a>\n\n# \u56fe\u50cf\u751f\u6210(Image Generation)\n\n**InstanceDiffusion: Instance-level Control for Image Generation**\n\n- Homepage: https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/\n\n- Paper: https://arxiv.org/abs/2402.03290\n- Code: https://github.com/frank-xwang/InstanceDiffusion\n\n**ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations**\n\n- Homepage: https://eclipse-t2i.vercel.app/\n- Paper: https://arxiv.org/abs/2312.04655\n\n- Code: https://github.com/eclipse-t2i/eclipse-inference\n\n**Instruct-Imagen: Image Generation with Multi-modal Instruction**\n\n- Paper: https://arxiv.org/abs/2401.01952\n\n**Residual Denoising Diffusion Models**\n\n- Paper: https://arxiv.org/abs/2308.13712\n- Code: https://github.com/nachifur/RDDM\n\n**UniGS: Unified Representation for Image Generation and Segmentation**\n\n- Paper: https://arxiv.org/abs/2312.01985\n\n**Multi-Instance Generation Controller for Text-to-Image Synthesis**\n\n- Paper: https://arxiv.org/abs/2402.05408\n- Code: https://github.com/limuloo/migc\n\n**SVGDreamer: Text Guided SVG Generation with Diffusion Model**\n\n- Paper: https://arxiv.org/abs/2312.16476\n- Code: https://ximinng.github.io/SVGDreamer-project/\n\n**InteractDiffusion: Interaction-Control for Text-to-Image Diffusion Model**\n\n- Paper: https://arxiv.org/abs/2312.05849\n- Code: https://github.com/jiuntian/interactdiffusion\n\n**Ranni: Taming Text-to-Image Diffusion for Accurate Prompt Following**\n\n- Paper: https://arxiv.org/abs/2311.17002\n- Code: https://github.com/ali-vilab/Ranni\n\n<a name=\"Video-Generation\"></a>\n\n# \u89c6\u9891\u751f\u6210(Video Generation)\n\n**Vlogger: Make Your Dream A Vlog**\n\n- Paper: https://arxiv.org/abs/2401.09414\n- Code: https://github.com/Vchitect/Vlogger\n\n**VBench: Comprehensive Benchmark Suite for Video Generative Models**\n\n- Homepage: https://vchitect.github.io/VBench-project/ \n- Paper: https://arxiv.org/abs/2311.17982\n- Code: https://github.com/Vchitect/VBench\n\n**VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models**\n\n- Homeoage: https://video-motion-customization.github.io/ \n- Paper: https://arxiv.org/abs/2312.00845\n- Code: https://github.com/HyeonHo99/Video-Motion-Customization\n\n<a name=\"3D-Generation\"></a>\n\n# 3D\u751f\u6210\n\n**CityDreamer: Compositional Generative Model of Unbounded 3D Cities**\n\n- Homepage: https://haozhexie.com/project/city-dreamer/ \n- Paper: https://arxiv.org/abs/2309.00610\n- Code: https://github.com/hzxie/city-dreamer\n\n**LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching**\n\n- Paper: https://arxiv.org/abs/2311.11284\n- Code: https://github.com/EnVision-Research/LucidDreamer \n\n<a name=\"Video-Understanding\"></a>\n\n# \u89c6\u9891\u7406\u89e3(Video Understanding)\n\n**MVBench: A Comprehensive Multi-modal Video Understanding Benchmark**\n\n- Paper: https://arxiv.org/abs/2311.17005\n- Code: https://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat2 \n\n<a name=\"KD\"></a>\n\n# \u77e5\u8bc6\u84b8\u998f(Knowledge Distillation)\n\n**Logit Standardization in Knowledge Distillation**\n\n- Paper: https://arxiv.org/abs/2403.01427\n- Code: https://github.com/sunshangquan/logit-standardization-KD\n\n**Efficient Dataset Distillation via Minimax Diffusion**\n\n- Paper: https://arxiv.org/abs/2311.15529\n- Code: https://github.com/vimar-gu/MinimaxDiffusion\n\n<a name=\"Stereo-Matching\"></a>\n\n# \u7acb\u4f53\u5339\u914d(Stereo Matching)\n\n**Neural Markov Random Field for Stereo Matching**\n\n- Paper: https://arxiv.org/abs/2403.11193\n- Code: https://github.com/aeolusguan/NMRF \n\n<a name=\"SGG\"></a>\n\n# \u573a\u666f\u56fe\u751f\u6210(Scene Graph Generation)\n\n**HiKER-SGG: Hierarchical Knowledge Enhanced Robust Scene Graph Generation**\n\n- Homepage: https://zhangce01.github.io/HiKER-SGG/ \n- Paper : https://arxiv.org/abs/2403.12033\n- Code: https://github.com/zhangce01/HiKER-SGG\n\n<a name=\"Video-Quality-Assessment\"></a>\n\n# \u89c6\u9891\u8d28\u91cf\u8bc4\u4ef7(Video Quality Assessment)\n\n**KVQ: Kaleidoscope Video Quality Assessment for Short-form Videos**\n\n- Homepage: https://lixinustc.github.io/projects/KVQ/ \n\n- Paper: https://arxiv.org/abs/2402.07220\n- Code: https://github.com/lixinustc/KVQ-Challenge-CVPR-NTIRE2024\n\n<a name=\"Datasets\"></a>\n\n# \u6570\u636e\u96c6(Datasets)\n\n**A Real-world Large-scale Dataset for Roadside Cooperative Perception**\n\n- Paper: https://arxiv.org/abs/2403.10145\n- Code: https://github.com/AIR-THU/DAIR-RCooper\n\n**Traffic Scene Parsing through the TSP6K Dataset**\n\n- Paper: https://arxiv.org/pdf/2303.02835.pdf\n- Code: https://github.com/PengtaoJiang/TSP6K \n\n<a name=\"Others\"></a>\n\n# \u5176\u4ed6(Others)\n\n**Object Recognition as Next Token Prediction**\n\n- Paper: https://arxiv.org/abs/2312.02142\n- Code: https://github.com/kaiyuyue/nxtp\n\n**ParameterNet: Parameters Are All You Need for Large-scale Visual Pretraining of Mobile Networks**\n\n- Paper: https://arxiv.org/abs/2306.14525\n- Code: https://parameternet.github.io/ \n\n**Seamless Human Motion Composition with Blended Positional Encodings**\n\n- Paper: https://arxiv.org/abs/2402.15509\n- Code: https://github.com/BarqueroGerman/FlowMDM \n\n**LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning**\n\n- Homepage:  https://ll3da.github.io/ \n\n- Paper: https://arxiv.org/abs/2311.18651\n- Code: https://github.com/Open3DA/LL3DA\n\n **CLOVA: A Closed-LOop Visual Assistant with Tool Usage and Update**\n\n- Homepage: https://clova-tool.github.io/ \n- Paper: https://arxiv.org/abs/2312.10908\n\n**MoMask: Generative Masked Modeling of 3D Human Motions**\n\n- Paper: https://arxiv.org/abs/2312.00063\n- Code: https://github.com/EricGuo5513/momask-codes\n\n **Amodal Ground Truth and Completion in the Wild**\n\n- Homepage: https://www.robots.ox.ac.uk/~vgg/research/amodal/ \n- Paper: https://arxiv.org/abs/2312.17247\n- Code: https://github.com/Championchess/Amodal-Completion-in-the-Wild\n\n**Improved Visual Grounding through Self-Consistent Explanations**\n\n- Paper: https://arxiv.org/abs/2312.04554\n- Code: https://github.com/uvavision/SelfEQ\n\n**ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object**\n\n- Homepage: https://chenshuang-zhang.github.io/imagenet_d/\n- Paper: https://arxiv.org/abs/2403.18775\n- Code: https://github.com/chenshuang-zhang/imagenet_d\n\n**Learning from Synthetic Human Group Activities**\n\n- Homepage: https://cjerry1243.github.io/M3Act/ \n- Paper  https://arxiv.org/abs/2306.16772\n- Code: https://github.com/cjerry1243/M3Act\n\n**A Cross-Subject Brain Decoding Framework**\n\n- Homepage: https://littlepure2333.github.io/MindBridge/\n- Paper: https://arxiv.org/abs/2404.07850\n- Code: https://github.com/littlepure2333/MindBridge\n\n**Multi-Task Dense Prediction via Mixture of Low-Rank Experts**\n\n- Paper : https://arxiv.org/abs/2403.17749\n- Code: https://github.com/YuqiYang213/MLoRE\n\n**Contrastive Mean-Shift Learning for Generalized Category Discovery**\n\n- Homepage: https://postech-cvlab.github.io/cms/ \n- Paper: https://arxiv.org/abs/2404.09451\n- Code: https://github.com/sua-choi/CMS\n  ",
        "releases": []
    }
}