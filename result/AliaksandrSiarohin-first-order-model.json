{
    "https://api.github.com/repos/AliaksandrSiarohin/first-order-model": {
        "forks": 3232,
        "watchers": 14667,
        "stars": 14667,
        "languages": {
            "Jupyter Notebook": 1194433,
            "Python": 117484,
            "Dockerfile": 400
        },
        "commits": [
            "2024-11-14T03:16:41Z",
            "2024-11-13T20:03:57Z",
            "2023-06-06T19:59:28Z",
            "2023-06-05T20:17:22Z",
            "2023-06-04T04:46:19Z",
            "2023-06-04T04:36:53Z",
            "2023-06-02T20:12:53Z",
            "2023-06-02T20:11:42Z",
            "2023-06-02T20:11:02Z",
            "2023-06-02T20:08:33Z",
            "2023-06-02T20:04:53Z",
            "2023-05-04T17:09:05Z",
            "2023-05-04T07:19:34Z",
            "2023-05-04T07:02:22Z",
            "2023-05-04T06:58:05Z",
            "2022-12-08T22:58:04Z",
            "2022-12-08T11:56:19Z",
            "2022-11-11T14:30:33Z",
            "2022-11-11T14:29:20Z",
            "2022-10-02T19:44:27Z",
            "2022-10-01T17:23:41Z",
            "2022-10-01T01:29:06Z",
            "2022-09-30T23:25:30Z",
            "2022-09-29T21:43:34Z",
            "2022-09-29T21:25:00Z",
            "2022-09-29T21:23:26Z",
            "2022-09-29T21:23:08Z",
            "2022-09-29T21:21:12Z",
            "2022-09-29T21:20:36Z",
            "2022-09-29T21:19:09Z"
        ],
        "creation_date": "2019-12-11T03:55:25Z",
        "contributors": 21,
        "topics": [
            "deep-learning",
            "generative-model",
            "image-animation",
            "motion-retargeting"
        ],
        "subscribers": 352,
        "readme": "<b>!!! Check out our new [paper](https://arxiv.org/pdf/2104.11280.pdf) and [framework](https://github.com/snap-research/articulated-animation) improved for articulated objects</b>\n\n# First Order Motion Model for Image Animation\n\nThis repository contains the source code for the paper [First Order Motion Model for Image Animation](https://papers.nips.cc/paper/8935-first-order-motion-model-for-image-animation) by Aliaksandr Siarohin, [St\u00e9phane Lathuili\u00e8re](http://stelat.eu), [Sergey Tulyakov](http://stulyakov.com), [Elisa Ricci](http://elisaricci.eu/) and [Nicu Sebe](http://disi.unitn.it/~sebe/). \n\n[Hugging Face Spaces](https://huggingface.co/spaces/abhishek/first-order-motion-model)\n\n## Example animations\n\nThe videos on the left show the driving videos. The first row on the right for each dataset shows the source videos. The bottom row contains the animated sequences with motion transferred from the driving video and object taken from the source image. We trained a separate network for each task.\n\n### VoxCeleb Dataset\n![Screenshot](sup-mat/vox-teaser.gif)\n### Fashion Dataset\n![Screenshot](sup-mat/fashion-teaser.gif)\n### MGIF Dataset\n![Screenshot](sup-mat/mgif-teaser.gif)\n\n\n### Installation\n\nWe support ```python3```. To install the dependencies run:\n```\npip install -r requirements.txt\n```\n\n### YAML configs\n\nThere are several configuration (```config/dataset_name.yaml```) files one for each `dataset`. See ```config/taichi-256.yaml``` to get description of each parameter.\n\n\n### Pre-trained checkpoint\nCheckpoints can be found under following link: [google-drive](https://drive.google.com/open?id=1PyQJmkdCsAkOYwUyaj_l-l0as-iLDgeH) or [yandex-disk](https://yadi.sk/d/lEw8uRm140L_eQ).\n\n### Animation Demo\nTo run a demo, download checkpoint and run the following command:\n```\npython demo.py  --config config/dataset_name.yaml --driving_video path/to/driving --source_image path/to/source --checkpoint path/to/checkpoint --relative --adapt_scale\n```\nThe result will be stored in ```result.mp4```.\n\nThe driving videos and source images should be cropped before it can be used in our method. To obtain some semi-automatic crop suggestions you can use ```python crop-video.py --inp some_youtube_video.mp4```. It will generate commands for crops using ffmpeg. In order to use the script, face-alligment library is needed:\n```\ngit clone https://github.com/1adrianb/face-alignment\ncd face-alignment\npip install -r requirements.txt\npython setup.py install\n```\n\n### Animation demo with Docker\n\nIf you are having trouble getting the demo to work because of library compatibility issues,\nand you're running Linux, you might try running it inside a Docker container, which would\ngive you better control over the execution environment.\n\nRequirements: Docker 19.03+ and [nvidia-docker](https://github.com/NVIDIA/nvidia-docker)\ninstalled and able to successfully run the `nvidia-docker` usage tests.\n\nWe'll first build the container.\n\n```\ndocker build -t first-order-model .\n```\n\nAnd now that we have the container available locally, we can use it to run the demo.\n\n```\ndocker run -it --rm --gpus all \\\n       -v $HOME/first-order-model:/app first-order-model \\\n       python3 demo.py --config config/vox-256.yaml \\\n           --driving_video driving.mp4 \\\n           --source_image source.png  \\ \n           --checkpoint vox-cpk.pth.tar \\ \n           --result_video result.mp4 \\\n           --relative --adapt_scale\n```\n\n### Colab Demo \n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb) [![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb)\n\n@graphemecluster prepared a GUI demo for the Google Colab. It also works in Kaggle. For the source code, see [```demo.ipynb```](https://github.com/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb).\n\nFor the old demo, see [```old_demo.ipynb```](https://github.com/AliaksandrSiarohin/first-order-model/blob/master/old_demo.ipynb).\n\n### Face-swap\nIt is possible to modify the method to perform face-swap using supervised segmentation masks.\n![Screenshot](sup-mat/face-swap.gif)\nFor both unsupervised and supervised video editing, such as face-swap, please refer to [Motion Co-Segmentation](https://github.com/AliaksandrSiarohin/motion-cosegmentation).\n\n\n### Training\n\nTo train a model on specific dataset run:\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3 python run.py --config config/dataset_name.yaml --device_ids 0,1,2,3\n```\nThe code will create a folder in the log directory (each run will create a time-stamped new directory).\nCheckpoints will be saved to this folder.\nTo check the loss values during training see ```log.txt```.\nYou can also check training data reconstructions in the ```train-vis``` subfolder.\nBy default the batch size is tuned to run on 2 or 4 Titan-X gpu (apart from speed it does not make much difference). You can change the batch size in the train_params in corresponding ```.yaml``` file.\n\n### Evaluation on video reconstruction\n\nTo evaluate the reconstruction performance run:\n```\nCUDA_VISIBLE_DEVICES=0 python run.py --config config/dataset_name.yaml --mode reconstruction --checkpoint path/to/checkpoint\n```\nYou will need to specify the path to the checkpoint,\nthe ```reconstruction``` subfolder will be created in the checkpoint folder.\nThe generated video will be stored to this folder, also generated videos will be stored in ```png``` subfolder in loss-less '.png' format for evaluation.\nInstructions for computing metrics from the paper can be found: https://github.com/AliaksandrSiarohin/pose-evaluation.\n\n### Image animation\n\nIn order to animate videos run:\n```\nCUDA_VISIBLE_DEVICES=0 python run.py --config config/dataset_name.yaml --mode animate --checkpoint path/to/checkpoint\n```\nYou will need to specify the path to the checkpoint,\nthe ```animation``` subfolder will be created in the same folder as the checkpoint.\nYou can find the generated video there and its loss-less version in the ```png``` subfolder.\nBy default video from test set will be randomly paired, but you can specify the \"source,driving\" pairs in the corresponding ```.csv``` files. The path to this file should be specified in corresponding ```.yaml``` file in pairs_list setting.\n\nThere are 2 different ways of performing animation:\nby using **absolute** keypoint locations or by using **relative** keypoint locations.\n\n1) <i>Animation using absolute coordinates:</i> the animation is performed using the absolute positions of the driving video and appearance of the source image.\nIn this way there are no specific requirements for the driving video and source appearance that is used.\nHowever this usually leads to poor performance since irrelevant details such as shape is transferred.\nCheck animate parameters in ```taichi-256.yaml``` to enable this mode.\n\n<img src=\"sup-mat/absolute-demo.gif\" width=\"512\"> \n\n2) <i>Animation using relative coordinates:</i> from the driving video we first estimate the relative movement of each keypoint,\nthen we add this movement to the absolute position of keypoints in the source image.\nThis keypoint along with source image is used for animation. This usually leads to better performance, however this requires\nthat the object in the first frame of the video and in the source image have the same pose\n\n<img src=\"sup-mat/relative-demo.gif\" width=\"512\"> \n\n\n### Datasets\n\n1) **Bair**. This dataset can be directly [downloaded](https://yadi.sk/d/Rr-fjn-PdmmqeA).\n\n2) **Mgif**. This dataset can be directly [downloaded](https://yadi.sk/d/5VdqLARizmnj3Q).\n\n3) **Fashion**. Follow the instruction on dataset downloading [from](https://vision.cs.ubc.ca/datasets/fashion/).\n\n4) **Taichi**. Follow the instructions in [data/taichi-loading](data/taichi-loading/README.md) or instructions from https://github.com/AliaksandrSiarohin/video-preprocessing. \n\n5) **Nemo**. Please follow the [instructions](https://www.uva-nemo.org/) on how to download the dataset. Then the dataset should be preprocessed using scripts from https://github.com/AliaksandrSiarohin/video-preprocessing.\n \n6) **VoxCeleb**. Please follow the instruction from https://github.com/AliaksandrSiarohin/video-preprocessing.\n\n\n### Training on your own dataset\n1) Resize all the videos to the same size e.g 256x256, the videos can be in '.gif', '.mp4' or folder with images.\nWe recommend the later, for each video make a separate folder with all the frames in '.png' format. This format is loss-less, and it has better i/o performance.\n\n2) Create a folder ```data/dataset_name``` with 2 subfolders ```train``` and ```test```, put training videos in the ```train``` and testing in the ```test```.\n\n3) Create a config ```config/dataset_name.yaml```, in dataset_params specify the root dir the ```root_dir:  data/dataset_name```. Also adjust the number of epoch in train_params.\n\n#### Additional notes\n\nCitation:\n\n```\n@InProceedings{Siarohin_2019_NeurIPS,\n  author={Siarohin, Aliaksandr and Lathuili\u00e8re, St\u00e9phane and Tulyakov, Sergey and Ricci, Elisa and Sebe, Nicu},\n  title={First Order Motion Model for Image Animation},\n  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},\n  month = {December},\n  year = {2019}\n}\n```\n",
        "releases": []
    }
}