{
    "https://api.github.com/repos/PeterL1n/RobustVideoMatting": {
        "forks": 1145,
        "watchers": 8697,
        "stars": 8697,
        "languages": {
            "Python": 128616
        },
        "commits": [
            "2023-03-13T23:53:45Z",
            "2023-03-10T18:14:09Z",
            "2022-01-14T06:36:37Z",
            "2021-12-10T05:47:37Z",
            "2021-12-08T17:20:03Z",
            "2021-11-24T08:24:34Z",
            "2021-11-10T17:54:49Z",
            "2021-11-09T07:20:04Z",
            "2021-11-04T08:18:17Z",
            "2021-10-20T18:08:00Z",
            "2021-10-20T15:48:55Z",
            "2021-10-20T15:47:05Z",
            "2021-10-19T00:27:37Z",
            "2021-10-19T00:09:26Z",
            "2021-10-18T14:37:02Z",
            "2021-10-14T04:55:02Z",
            "2021-10-12T17:15:56Z",
            "2021-10-12T15:30:01Z",
            "2021-09-30T07:44:11Z",
            "2021-09-29T08:04:01Z",
            "2021-09-28T19:46:50Z",
            "2021-09-28T18:24:54Z",
            "2021-09-27T22:03:17Z",
            "2021-09-27T21:56:41Z",
            "2021-09-27T21:42:30Z",
            "2021-09-27T21:41:20Z",
            "2021-09-27T20:36:15Z",
            "2021-09-26T17:56:00Z",
            "2021-09-20T19:10:12Z",
            "2021-09-20T17:32:09Z"
        ],
        "creation_date": "2021-08-30T20:57:44Z",
        "contributors": 5,
        "topics": [
            "ai",
            "computer-vision",
            "deep-learning",
            "machine-learning",
            "matting"
        ],
        "subscribers": 138,
        "readme": "# Robust Video Matting (RVM)\n\n![Teaser](/documentation/image/teaser.gif)\n\n<p align=\"center\">English | <a href=\"README_zh_Hans.md\">\u4e2d\u6587</a></p>\n\nOfficial repository for the paper [Robust High-Resolution Video Matting with Temporal Guidance](https://peterl1n.github.io/RobustVideoMatting/). RVM is specifically designed for robust human video matting. Unlike existing neural models that process frames as independent images, RVM uses a recurrent neural network to process videos with temporal memory. RVM can perform matting in real-time on any videos without additional inputs. It achieves **4K 76FPS** and **HD 104FPS** on an Nvidia GTX 1080 Ti GPU. The project was developed at [ByteDance Inc.](https://www.bytedance.com/)\n\n<br>\n\n## News\n\n* [Nov 03 2021] Fixed a bug in [train.py](https://github.com/PeterL1n/RobustVideoMatting/commit/48effc91576a9e0e7a8519f3da687c0d3522045f).\n* [Sep 16 2021] Code is re-released under GPL-3.0 license.\n* [Aug 25 2021] Source code and pretrained models are published.\n* [Jul 27 2021] Paper is accepted by WACV 2022.\n\n<br>\n\n## Showreel\nWatch the showreel video ([YouTube](https://youtu.be/Jvzltozpbpk), [Bilibili](https://www.bilibili.com/video/BV1Z3411B7g7/)) to see the model's performance. \n\n<p align=\"center\">\n    <a href=\"https://youtu.be/Jvzltozpbpk\">\n        <img src=\"documentation/image/showreel.gif\">\n    </a>\n</p>\n\nAll footage in the video are available in [Google Drive](https://drive.google.com/drive/folders/1VFnWwuu-YXDKG-N6vcjK_nL7YZMFapMU?usp=sharing).\n\n<br>\n\n\n## Demo\n* [Webcam Demo](https://peterl1n.github.io/RobustVideoMatting/#/demo): Run the model live in your browser. Visualize recurrent states.\n* [Colab Demo](https://colab.research.google.com/drive/10z-pNKRnVNsp0Lq9tH1J_XPZ7CBC_uHm?usp=sharing): Test our model on your own videos with free GPU. \n\n<br>\n\n## Download\n\nWe recommend MobileNetv3 models for most use cases. ResNet50 models are the larger variant with small performance improvements. Our model is available on various inference frameworks. See [inference documentation](documentation/inference.md) for more instructions.\n\n<table>\n    <thead>\n        <tr>\n            <td>Framework</td>\n            <td>Download</td>\n            <td>Notes</td>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>PyTorch</td>\n            <td>\n                <a  href=\"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3.pth\">rvm_mobilenetv3.pth</a><br>\n                <a  href=\"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50.pth\">rvm_resnet50.pth</a>\n            </td>\n            <td>\n                Official weights for PyTorch. <a href=\"documentation/inference.md#pytorch\">Doc</a>\n            </td>\n        </tr>\n        <tr>\n            <td>TorchHub</td>\n            <td>\n                Nothing to Download.\n            </td>\n            <td>\n                Easiest way to use our model in your PyTorch project. <a href=\"documentation/inference.md#torchhub\">Doc</a>\n            </td>\n        </tr>\n        <tr>\n            <td>TorchScript</td>\n            <td>\n                <a  href=\"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_fp32.torchscript\">rvm_mobilenetv3_fp32.torchscript</a><br>\n                <a  href=\"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_fp16.torchscript\">rvm_mobilenetv3_fp16.torchscript</a><br>\n                <a  href=\"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50_fp32.torchscript\">rvm_resnet50_fp32.torchscript</a><br>\n                <a  href=\"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50_fp16.torchscript\">rvm_resnet50_fp16.torchscript</a>\n            </td>\n            <td>\n                If inference on mobile, consider export int8 quantized models yourself. <a href=\"documentation/inference.md#torchscript\">Doc</a>\n            </td>\n        </tr>\n        <tr>\n            <td>ONNX</td>\n            <td>\n                <a  href=\"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_fp32.onnx\">rvm_mobilenetv3_fp32.onnx</a><br>\n                <a  href=\"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_fp16.onnx\">rvm_mobilenetv3_fp16.onnx</a><br>\n                <a  href=\"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50_fp32.onnx\">rvm_resnet50_fp32.onnx</a><br>\n                <a  href=\"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50_fp16.onnx\">rvm_resnet50_fp16.onnx</a>\n            </td>\n            <td>\n                Tested on ONNX Runtime with CPU and CUDA backends. Provided models use opset 12. <a href=\"documentation/inference.md#onnx\">Doc</a>, <a href=\"https://github.com/PeterL1n/RobustVideoMatting/tree/onnx\">Exporter</a>.\n            </td>\n        </tr>\n        <tr>\n            <td>TensorFlow</td>\n            <td>\n                <a  href=\"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_tf.zip\">rvm_mobilenetv3_tf.zip</a><br>\n                <a  href=\"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_resnet50_tf.zip\">rvm_resnet50_tf.zip</a>\n            </td>\n            <td>\n                TensorFlow 2 SavedModel. <a href=\"documentation/inference.md#tensorflow\">Doc</a>\n            </td>\n        </tr>\n        <tr>\n            <td>TensorFlow.js</td>\n            <td>\n                <a  href=\"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_tfjs_int8.zip\">rvm_mobilenetv3_tfjs_int8.zip</a><br>\n            </td>\n            <td>\n                Run the model on the web. <a href=\"https://peterl1n.github.io/RobustVideoMatting/#/demo\">Demo</a>, <a href=\"https://github.com/PeterL1n/RobustVideoMatting/tree/tfjs\">Starter Code</a>\n            </td>\n        </tr>\n        <tr>\n            <td>CoreML</td>\n            <td>\n                <a  href=\"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_1280x720_s0.375_fp16.mlmodel\">rvm_mobilenetv3_1280x720_s0.375_fp16.mlmodel</a><br>\n                <a  href=\"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_1280x720_s0.375_int8.mlmodel\">rvm_mobilenetv3_1280x720_s0.375_int8.mlmodel</a><br>\n                <a  href=\"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_1920x1080_s0.25_fp16.mlmodel\">rvm_mobilenetv3_1920x1080_s0.25_fp16.mlmodel</a><br>\n                <a  href=\"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3_1920x1080_s0.25_int8.mlmodel\">rvm_mobilenetv3_1920x1080_s0.25_int8.mlmodel</a><br>\n            </td>\n            <td>\n                CoreML does not support dynamic resolution. Other resolutions can be exported yourself. Models require iOS 13+. <code>s</code> denotes <code>downsample_ratio</code>. <a href=\"documentation/inference.md#coreml\">Doc</a>, <a href=\"https://github.com/PeterL1n/RobustVideoMatting/tree/coreml\">Exporter</a>\n            </td>\n        </tr>\n    </tbody>\n</table>\n\nAll models are available in [Google Drive](https://drive.google.com/drive/folders/1pBsG-SCTatv-95SnEuxmnvvlRx208VKj?usp=sharing) and [Baidu Pan](https://pan.baidu.com/s/1puPSxQqgBFOVpW4W7AolkA) (code: gym7).\n\n<br>\n\n## PyTorch Example\n\n1. Install dependencies:\n```sh\npip install -r requirements_inference.txt\n```\n\n2. Load the model:\n\n```python\nimport torch\nfrom model import MattingNetwork\n\nmodel = MattingNetwork('mobilenetv3').eval().cuda()  # or \"resnet50\"\nmodel.load_state_dict(torch.load('rvm_mobilenetv3.pth'))\n```\n\n3. To convert videos, we provide a simple conversion API:\n\n```python\nfrom inference import convert_video\n\nconvert_video(\n    model,                           # The model, can be on any device (cpu or cuda).\n    input_source='input.mp4',        # A video file or an image sequence directory.\n    output_type='video',             # Choose \"video\" or \"png_sequence\"\n    output_composition='com.mp4',    # File path if video; directory path if png sequence.\n    output_alpha=\"pha.mp4\",          # [Optional] Output the raw alpha prediction.\n    output_foreground=\"fgr.mp4\",     # [Optional] Output the raw foreground prediction.\n    output_video_mbps=4,             # Output video mbps. Not needed for png sequence.\n    downsample_ratio=None,           # A hyperparameter to adjust or use None for auto.\n    seq_chunk=12,                    # Process n frames at once for better parallelism.\n)\n```\n\n4. Or write your own inference code:\n```python\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import ToTensor\nfrom inference_utils import VideoReader, VideoWriter\n\nreader = VideoReader('input.mp4', transform=ToTensor())\nwriter = VideoWriter('output.mp4', frame_rate=30)\n\nbgr = torch.tensor([.47, 1, .6]).view(3, 1, 1).cuda()  # Green background.\nrec = [None] * 4                                       # Initial recurrent states.\ndownsample_ratio = 0.25                                # Adjust based on your video.\n\nwith torch.no_grad():\n    for src in DataLoader(reader):                     # RGB tensor normalized to 0 ~ 1.\n        fgr, pha, *rec = model(src.cuda(), *rec, downsample_ratio)  # Cycle the recurrent states.\n        com = fgr * pha + bgr * (1 - pha)              # Composite to green background. \n        writer.write(com)                              # Write frame.\n```\n\n5. The models and converter API are also available through TorchHub.\n\n```python\n# Load the model.\nmodel = torch.hub.load(\"PeterL1n/RobustVideoMatting\", \"mobilenetv3\") # or \"resnet50\"\n\n# Converter API.\nconvert_video = torch.hub.load(\"PeterL1n/RobustVideoMatting\", \"converter\")\n```\n\nPlease see [inference documentation](documentation/inference.md) for details on `downsample_ratio` hyperparameter, more converter arguments, and more advanced usage.\n\n<br>\n\n## Training and Evaluation\n\nPlease refer to the [training documentation](documentation/training.md) to train and evaluate your own model.\n\n<br>\n\n## Speed\n\nSpeed is measured with `inference_speed_test.py` for reference.\n\n| GPU            | dType | HD (1920x1080) | 4K (3840x2160) |\n| -------------- | ----- | -------------- |----------------|\n| RTX 3090       | FP16  | 172 FPS        | 154 FPS        |\n| RTX 2060 Super | FP16  | 134 FPS        | 108 FPS        |\n| GTX 1080 Ti    | FP32  | 104 FPS        | 74 FPS         |\n\n* Note 1: HD uses `downsample_ratio=0.25`, 4K uses `downsample_ratio=0.125`. All tests use batch size 1 and frame chunk 1.\n* Note 2: GPUs before Turing architecture does not support FP16 inference, so GTX 1080 Ti uses FP32.\n* Note 3: We only measure tensor throughput. The provided video conversion script in this repo is expected to be much slower, because it does not utilize hardware video encoding/decoding and does not have the tensor transfer done on parallel threads. If you are interested in implementing hardware video encoding/decoding in Python, please refer to [PyNvCodec](https://github.com/NVIDIA/VideoProcessingFramework).\n\n<br>  \n\n## Project Members\n* [Shanchuan Lin](https://www.linkedin.com/in/shanchuanlin/)\n* [Linjie Yang](https://sites.google.com/site/linjieyang89/)\n* [Imran Saleemi](https://www.linkedin.com/in/imran-saleemi/)\n* [Soumyadip Sengupta](https://homes.cs.washington.edu/~soumya91/)\n\n<br>\n\n## Third-Party Projects\n\n* [NCNN C++ Android](https://github.com/FeiGeChuanShu/ncnn_Android_RobustVideoMatting) ([@FeiGeChuanShu](https://github.com/FeiGeChuanShu))\n* [lite.ai.toolkit](https://github.com/DefTruth/RobustVideoMatting.lite.ai.toolkit) ([@DefTruth](https://github.com/DefTruth))\n* [Gradio Web Demo](https://huggingface.co/spaces/akhaliq/Robust-Video-Matting) ([@AK391](https://github.com/AK391))\n* [Unity Engine demo with NatML](https://hub.natml.ai/@natsuite/robust-video-matting) ([@natsuite](https://github.com/natsuite))  \n* [MNN C++ Demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/lite/mnn/cv/mnn_rvm.cpp) ([@DefTruth](https://github.com/DefTruth))\n* [TNN C++ Demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/lite/tnn/cv/tnn_rvm.cpp) ([@DefTruth](https://github.com/DefTruth))\n\n",
        "releases": [
            {
                "name": "Initial release",
                "date": "2021-09-17T07:31:29Z"
            }
        ]
    }
}