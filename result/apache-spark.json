{
    "https://api.github.com/repos/apache/spark": {
        "forks": 28458,
        "watchers": 40368,
        "stars": 40368,
        "languages": {
            "Scala": 56243361,
            "Python": 13994648,
            "Java": 5761284,
            "Jupyter Notebook": 4331372,
            "HiveQL": 1968675,
            "R": 1303815,
            "PLpgSQL": 352493,
            "Shell": 257781,
            "JavaScript": 242472,
            "ANTLR": 78027,
            "Dockerfile": 54633,
            "HTML": 43634,
            "Batchfile": 28201,
            "CSS": 27552,
            "Roff": 20703,
            "Makefile": 2388,
            "Thrift": 2016,
            "q": 1919,
            "PowerShell": 1772,
            "C": 1493,
            "ReScript": 240
        },
        "commits": [
            "2025-01-22T05:57:04Z",
            "2025-01-22T02:55:36Z",
            "2025-01-22T02:22:11Z",
            "2025-01-22T01:34:41Z",
            "2025-01-22T01:28:55Z",
            "2025-01-21T19:05:01Z",
            "2025-01-21T16:55:34Z",
            "2025-01-21T16:47:56Z",
            "2025-01-21T16:01:43Z",
            "2025-01-21T09:40:37Z",
            "2025-01-21T08:08:59Z",
            "2025-01-21T08:03:13Z",
            "2025-01-21T07:58:04Z",
            "2025-01-21T06:21:24Z",
            "2025-01-21T04:17:24Z",
            "2025-01-21T04:04:39Z",
            "2025-01-21T04:02:04Z",
            "2025-01-21T03:54:13Z",
            "2025-01-21T03:28:29Z",
            "2025-01-21T01:47:57Z",
            "2025-01-21T01:13:05Z",
            "2025-01-20T23:59:53Z",
            "2025-01-20T23:48:55Z",
            "2025-01-20T12:52:39Z",
            "2025-01-20T09:30:11Z",
            "2025-01-20T09:16:22Z",
            "2025-01-20T07:40:07Z",
            "2025-01-20T07:03:43Z",
            "2025-01-20T05:11:33Z",
            "2025-01-20T00:49:03Z"
        ],
        "creation_date": "2014-02-25T08:00:08Z",
        "contributors": 30,
        "topics": [
            "big-data",
            "java",
            "jdbc",
            "python",
            "r",
            "scala",
            "spark",
            "sql"
        ],
        "subscribers": 2018,
        "readme": "# Apache Spark\n\nSpark is a unified analytics engine for large-scale data processing. It provides\nhigh-level APIs in Scala, Java, Python, and R, and an optimized engine that\nsupports general computation graphs for data analysis. It also supports a\nrich set of higher-level tools including Spark SQL for SQL and DataFrames,\npandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing,\nand Structured Streaming for stream processing.\n\n- Official version: <https://spark.apache.org/>\n- Development version: <https://apache.github.io/spark/>\n\n[![GitHub Actions Build](https://github.com/apache/spark/actions/workflows/build_main.yml/badge.svg)](https://github.com/apache/spark/actions/workflows/build_main.yml)\n[![PySpark Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)\n[![PyPI Downloads](https://static.pepy.tech/personalized-badge/pyspark?period=month&units=international_system&left_color=black&right_color=orange&left_text=PyPI%20downloads)](https://pypi.org/project/pyspark/)\n\n\n## Online Documentation\n\nYou can find the latest Spark documentation, including a programming\nguide, on the [project web page](https://spark.apache.org/documentation.html).\nThis README file only contains basic setup instructions.\n\n## Building Spark\n\nSpark is built using [Apache Maven](https://maven.apache.org/).\nTo build Spark and its example programs, run:\n\n```bash\n./build/mvn -DskipTests clean package\n```\n\n(You do not need to do this if you downloaded a pre-built package.)\n\nMore detailed documentation is available from the project site, at\n[\"Building Spark\"](https://spark.apache.org/docs/latest/building-spark.html).\n\nFor general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](https://spark.apache.org/developer-tools.html).\n\n## Interactive Scala Shell\n\nThe easiest way to start using Spark is through the Scala shell:\n\n```bash\n./bin/spark-shell\n```\n\nTry the following command, which should return 1,000,000,000:\n\n```scala\nscala> spark.range(1000 * 1000 * 1000).count()\n```\n\n## Interactive Python Shell\n\nAlternatively, if you prefer Python, you can use the Python shell:\n\n```bash\n./bin/pyspark\n```\n\nAnd run the following command, which should also return 1,000,000,000:\n\n```python\n>>> spark.range(1000 * 1000 * 1000).count()\n```\n\n## Example Programs\n\nSpark also comes with several sample programs in the `examples` directory.\nTo run one of them, use `./bin/run-example <class> [params]`. For example:\n\n```bash\n./bin/run-example SparkPi\n```\n\nwill run the Pi example locally.\n\nYou can set the MASTER environment variable when running examples to submit\nexamples to a cluster. This can be spark:// URL,\n\"yarn\" to run on YARN, and \"local\" to run\nlocally with one thread, or \"local[N]\" to run locally with N threads. You\ncan also use an abbreviated class name if the class is in the `examples`\npackage. For instance:\n\n```bash\nMASTER=spark://host:7077 ./bin/run-example SparkPi\n```\n\nMany of the example programs print usage help if no params are given.\n\n## Running Tests\n\nTesting first requires [building Spark](#building-spark). Once Spark is built, tests\ncan be run using:\n\n```bash\n./dev/run-tests\n```\n\nPlease see the guidance on how to\n[run tests for a module, or individual tests](https://spark.apache.org/developer-tools.html#individual-tests).\n\nThere is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md\n\n## A Note About Hadoop Versions\n\nSpark uses the Hadoop core library to talk to HDFS and other Hadoop-supported\nstorage systems. Because the protocols have changed in different versions of\nHadoop, you must build Spark against the same version that your cluster runs.\n\nPlease refer to the build documentation at\n[\"Specifying the Hadoop Version and Enabling YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)\nfor detailed guidance on building for a particular distribution of Hadoop, including\nbuilding for particular Hive and Hive Thriftserver distributions.\n\n## Configuration\n\nPlease refer to the [Configuration Guide](https://spark.apache.org/docs/latest/configuration.html)\nin the online documentation for an overview on how to configure Spark.\n\n## Contributing\n\nPlease review the [Contribution to Spark guide](https://spark.apache.org/contributing.html)\nfor information on how to get started contributing to the project.\n",
        "releases": []
    }
}