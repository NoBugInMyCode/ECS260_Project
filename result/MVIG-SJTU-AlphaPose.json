{
    "https://api.github.com/repos/MVIG-SJTU/AlphaPose": {
        "forks": 1981,
        "watchers": 8139,
        "stars": 8139,
        "languages": {
            "Python": 1261797,
            "Cuda": 82488,
            "C++": 64243,
            "C": 4004,
            "Cython": 3980,
            "Shell": 1061
        },
        "commits": [
            "2023-01-12T07:14:47Z",
            "2023-01-12T07:11:25Z",
            "2023-01-12T07:11:12Z",
            "2022-12-27T14:13:44Z",
            "2022-11-26T13:19:25Z",
            "2022-11-24T08:44:23Z",
            "2022-11-23T09:30:11Z",
            "2022-11-08T05:34:20Z",
            "2022-11-08T04:51:36Z",
            "2022-11-08T04:35:58Z",
            "2022-09-26T07:11:36Z",
            "2022-08-05T04:45:02Z",
            "2022-08-03T15:28:16Z",
            "2022-08-03T15:23:11Z",
            "2022-08-03T15:22:47Z",
            "2022-08-03T15:19:41Z",
            "2022-08-03T08:20:50Z",
            "2022-08-01T04:25:41Z",
            "2022-07-31T14:43:56Z",
            "2022-07-31T14:43:22Z",
            "2022-07-26T16:02:47Z",
            "2022-07-26T10:06:06Z",
            "2022-07-25T13:06:09Z",
            "2022-07-01T09:10:06Z",
            "2022-06-28T06:08:05Z",
            "2022-06-26T06:47:44Z",
            "2022-06-26T06:32:16Z",
            "2022-06-25T06:22:34Z",
            "2022-06-22T13:21:58Z",
            "2022-06-22T11:22:44Z"
        ],
        "creation_date": "2018-01-17T02:33:17Z",
        "contributors": 17,
        "topics": [
            "accurate",
            "alpha-pose",
            "alphapose",
            "crowdpose",
            "full-body",
            "gpu",
            "human-computer-interaction",
            "human-joints",
            "human-pose-estimation",
            "human-pose-tracking",
            "human-tracking",
            "keypoints",
            "person-pose-estimation",
            "pose-estimation",
            "posetracking",
            "pytorch",
            "realtime",
            "skeleton",
            "tracking",
            "whole-body"
        ],
        "subscribers": 207,
        "readme": "\n<div align=\"center\">\n    <img src=\"docs/logo.jpg\", width=\"400\">\n</div>\n\n\n## News!\n- Nov 2022: [**AlphaPose paper**](http://arxiv.org/abs/2211.03375) is released! Checkout the paper for more details about this project.\n- Sep 2022: [**Jittor** version](https://github.com/tycoer/AlphaPose_jittor) of AlphaPose is released! It achieves 1.45x speed up with resnet50 backbone on the training stage.\n- July 2022: [**v0.6.0** version](https://github.com/MVIG-SJTU/AlphaPose) of AlphaPose is released! [HybrIK](https://github.com/Jeff-sjtu/HybrIK) for 3D pose and shape estimation is supported!\n- Jan 2022: [**v0.5.0** version](https://github.com/MVIG-SJTU/AlphaPose) of AlphaPose is released! Stronger whole body(face,hand,foot) keypoints! More models are availabel. Checkout [docs/MODEL_ZOO.md](docs/MODEL_ZOO.md)\n- Aug 2020: [**v0.4.0** version](https://github.com/MVIG-SJTU/AlphaPose) of AlphaPose is released! Stronger tracking! Include whole body(face,hand,foot) keypoints! [Colab](https://colab.research.google.com/drive/1c7xb_7U61HmeJp55xjXs24hf1GUtHmPs?usp=sharing) now available.\n- Dec 2019: [**v0.3.0** version](https://github.com/MVIG-SJTU/AlphaPose) of AlphaPose is released! Smaller model, higher accuracy!\n- Apr 2019: [**MXNet** version](https://github.com/MVIG-SJTU/AlphaPose/tree/mxnet) of AlphaPose is released! It runs at **23 fps** on COCO validation set.\n- Feb 2019: [CrowdPose](https://github.com/MVIG-SJTU/AlphaPose/docs/CrowdPose.md) is integrated into AlphaPose Now!\n- Dec 2018: [General version](https://github.com/MVIG-SJTU/AlphaPose/trackers/PoseFlow) of PoseFlow is released! 3X Faster and support pose tracking results visualization!\n- Sep 2018: [**v0.2.0** version](https://github.com/MVIG-SJTU/AlphaPose/tree/pytorch) of AlphaPose is released! It runs at **20 fps** on COCO validation set (4.6 people per image on average) and achieves 71 mAP!\n\n## AlphaPose\n[AlphaPose](http://www.mvig.org/research/alphapose.html) is an accurate multi-person pose estimator, which is the **first open-source system that achieves 70+ mAP (75 mAP) on COCO dataset and 80+ mAP (82.1 mAP) on MPII dataset.** \nTo match poses that correspond to the same person across frames, we also provide an efficient online pose tracker called Pose Flow. It is the **first open-source online pose tracker that achieves both 60+ mAP (66.5 mAP) and 50+ MOTA (58.3 MOTA) on PoseTrack Challenge dataset.**\n\nAlphaPose supports both Linux and **Windows!**\n\n<div align=\"center\">\n    <img src=\"docs/alphapose_17.gif\", width=\"400\" alt><br>\n    COCO 17 keypoints\n</div>\n<div align=\"center\">\n    <img src=\"docs/alphapose_26.gif\", width=\"400\" alt><br>\n    <b><a href=\"https://github.com/Fang-Haoshu/Halpe-FullBody\">Halpe 26 keypoints</a></b> + tracking\n</div>\n<div align=\"center\">\n    <img src=\"docs/alphapose_136.gif\", width=\"400\"alt><br>\n    <b><a href=\"https://github.com/Fang-Haoshu/Halpe-FullBody\">Halpe 136 keypoints</a></b> + tracking\n    <b><a href=\"https://youtu.be/uze6chg-YeU\">YouTube link</a></b><br>\n</div>\n<div align=\"center\">\n    <img src=\"docs/alphapose_hybrik_smpl.gif\", width=\"400\"alt><br>\n    <b><a href=\"https://github.com/Jeff-sjtu/HybrIK\">SMPL</a></b> + tracking\n</div>\n\n\n## Results\n### Pose Estimation\nResults on COCO test-dev 2015:\n<center>\n\n| Method | AP @0.5:0.95 | AP @0.5 | AP @0.75 | AP medium | AP large |\n|:-------|:-----:|:-------:|:-------:|:-------:|:-------:|\n| OpenPose (CMU-Pose) | 61.8 | 84.9 | 67.5 | 57.1 | 68.2 |\n| Detectron (Mask R-CNN) | 67.0 | 88.0 | 73.1 | 62.2 | 75.6 |\n| **AlphaPose** | **73.3** | **89.2** | **79.1** | **69.0** | **78.6** |\n\n</center>\n\nResults on MPII full test set:\n<center>\n\n| Method | Head | Shoulder | Elbow | Wrist | Hip | Knee | Ankle | Ave |\n|:-------|:-----:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|\n| OpenPose (CMU-Pose) | 91.2 | 87.6 | 77.7 | 66.8 | 75.4 | 68.9 | 61.7 | 75.6 |\n| Newell & Deng | **92.1** | 89.3 | 78.9 | 69.8 | 76.2 | 71.6 | 64.7 | 77.5 |\n| **AlphaPose** | 91.3 | **90.5** | **84.0** | **76.4** | **80.3** | **79.9** | **72.4** | **82.1** |\n\n</center>\n\nMore results and models are available in the [docs/MODEL_ZOO.md](docs/MODEL_ZOO.md).\n\n### Pose Tracking\n\n<p align='center'>\n    <img src=\"docs/posetrack.gif\", width=\"360\">\n    <img src=\"docs/posetrack2.gif\", width=\"344\">\n</p>\n\nPlease read [trackers/README.md](trackers/) for details.\n\n### CrowdPose\n<p align='center'>\n    <img src=\"docs/crowdpose.gif\", width=\"360\">\n</p>\n\nPlease read [docs/CrowdPose.md](docs/CrowdPose.md) for details.\n\n\n## Installation\nPlease check out [docs/INSTALL.md](docs/INSTALL.md)\n\n## Model Zoo\nPlease check out [docs/MODEL_ZOO.md](docs/MODEL_ZOO.md)\n\n## Quick Start\n- **Colab**: We provide a [colab example](https://colab.research.google.com/drive/1_3Wxi4H3QGVC28snL3rHIoeMAwI2otMR?usp=sharing) for your quick start.\n\n- **Inference**: Inference demo\n``` bash\n./scripts/inference.sh ${CONFIG} ${CHECKPOINT} ${VIDEO_NAME} # ${OUTPUT_DIR}, optional\n```\n\nInference SMPL (Download the SMPL model `basicModel_neutral_lbs_10_207_0_v1.0.0.pkl` from [here](https://smpl.is.tue.mpg.de/) and put it in `model_files/`).\n``` bash\n./scripts/inference_3d.sh ./configs/smpl/256x192_adam_lr1e-3-res34_smpl_24_3d_base_2x_mix.yaml ${CHECKPOINT} ${VIDEO_NAME} # ${OUTPUT_DIR}, optional\n```\nFor high level API, please refer to `./scripts/demo_api.py`. To enable tracking, please refer to [this page](./trackers).\n\n- **Training**: Train from scratch\n``` bash\n./scripts/train.sh ${CONFIG} ${EXP_ID}\n```\n\n- **Validation**: Validate your model on MSCOCO val2017\n``` bash\n./scripts/validate.sh ${CONFIG} ${CHECKPOINT}\n```\n\nExamples:\n\nDemo using `FastPose` model.\n``` bash\n./scripts/inference.sh configs/coco/resnet/256x192_res50_lr1e-3_1x.yaml pretrained_models/fast_res50_256x192.pth ${VIDEO_NAME}\n#or\npython scripts/demo_inference.py --cfg configs/coco/resnet/256x192_res50_lr1e-3_1x.yaml --checkpoint pretrained_models/fast_res50_256x192.pth --indir examples/demo/\n#or if you want to use yolox-x as the detector\npython scripts/demo_inference.py --detector yolox-x --cfg configs/coco/resnet/256x192_res50_lr1e-3_1x.yaml --checkpoint pretrained_models/fast_res50_256x192.pth --indir examples/demo/\n```\n\nTrain `FastPose` on mscoco dataset.\n``` bash\n./scripts/train.sh ./configs/coco/resnet/256x192_res50_lr1e-3_1x.yaml exp_fastpose\n```\n\nMore detailed inference options and examples, please refer to [GETTING_STARTED.md](docs/GETTING_STARTED.md)\n\n\n## Common issue & FAQ\nCheck out [faq.md](docs/faq.md) for faq. If it can not solve your problems or if you find any bugs, don't hesitate to comment on GitHub or make a pull request!\n\n## Contributors\nAlphaPose is based on RMPE(ICCV'17), authored by [Hao-Shu Fang](https://fang-haoshu.github.io/), Shuqin Xie, [Yu-Wing Tai](https://scholar.google.com/citations?user=nFhLmFkAAAAJ&hl=en) and [Cewu Lu](http://www.mvig.org/), [Cewu Lu](http://mvig.sjtu.edu.cn/) is the corresponding author. Currently, it is maintained by [Jiefeng Li\\*](http://jeff-leaf.site/), [Hao-shu Fang\\*](https://fang-haoshu.github.io/),  [Haoyi Zhu](https://github.com/HaoyiZhu), [Yuliang Xiu](http://xiuyuliang.cn/about/) and [Chao Xu](http://www.isdas.cn/). \n\nThe main contributors are listed in [doc/contributors.md](docs/contributors.md).\n\n## TODO\n- [x] Multi-GPU/CPU inference\n- [x] 3D pose\n- [x] add tracking flag\n- [ ] PyTorch C++ version\n- [x] Add model trained on mixture dataset (Check the model zoo)\n- [ ] dense support\n- [x] small box easy filter\n- [x] Crowdpose support\n- [ ] Speed up PoseFlow\n- [x] Add stronger/light detectors (yolox is now supported)\n- [x] High level API (check the scripts/demo_api.py)\n\nWe would really appreciate if you can offer any help and be the [contributor](docs/contributors.md) of AlphaPose.\n\n\n## Citation\nPlease cite these papers in your publications if it helps your research:\n\n    @article{alphapose,\n      author = {Fang, Hao-Shu and Li, Jiefeng and Tang, Hongyang and Xu, Chao and Zhu, Haoyi and Xiu, Yuliang and Li, Yong-Lu and Lu, Cewu},\n      journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n      title = {AlphaPose: Whole-Body Regional Multi-Person Pose Estimation and Tracking in Real-Time},\n      year = {2022}\n    }\n    \n    @inproceedings{fang2017rmpe,\n      title={{RMPE}: Regional Multi-person Pose Estimation},\n      author={Fang, Hao-Shu and Xie, Shuqin and Tai, Yu-Wing and Lu, Cewu},\n      booktitle={ICCV},\n      year={2017}\n    }\n\n    @inproceedings{li2019crowdpose,\n        title={Crowdpose: Efficient crowded scenes pose estimation and a new benchmark},\n        author={Li, Jiefeng and Wang, Can and Zhu, Hao and Mao, Yihuan and Fang, Hao-Shu and Lu, Cewu},\n        booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},\n        pages={10863--10872},\n        year={2019}\n    }\n\nIf you used the 3D mesh reconstruction module, please also cite:\n\n    @inproceedings{li2021hybrik,\n        title={Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation},\n        author={Li, Jiefeng and Xu, Chao and Chen, Zhicun and Bian, Siyuan and Yang, Lixin and Lu, Cewu},\n        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n        pages={3383--3393},\n        year={2021}\n    }\n\nIf you used the PoseFlow tracking module, please also cite:\n\n    @inproceedings{xiu2018poseflow,\n      author = {Xiu, Yuliang and Li, Jiefeng and Wang, Haoyu and Fang, Yinghong and Lu, Cewu},\n      title = {{Pose Flow}: Efficient Online Pose Tracking},\n      booktitle={BMVC},\n      year = {2018}\n    }\n\n\n\n\n\n## License\nAlphaPose is freely available for free non-commercial use, and may be redistributed under these conditions. For commercial queries, please drop an e-mail at mvig.alphapose[at]gmail[dot]com and cc lucewu[[at]sjtu[dot]edu[dot]cn. We will send the detail agreement to you.\n",
        "releases": []
    }
}